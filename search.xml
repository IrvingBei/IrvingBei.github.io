<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>读《浮生六记》</title>
      <link href="/2020/04/19/%E8%AF%BB%E3%80%8A%E6%B5%AE%E7%94%9F%E5%85%AD%E8%AE%B0%E3%80%8B/"/>
      <url>/2020/04/19/%E8%AF%BB%E3%80%8A%E6%B5%AE%E7%94%9F%E5%85%AD%E8%AE%B0%E3%80%8B/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;是爱情也是生活，是回忆也是经历，有美好也有无奈，有羡慕也有感概。生活，本就是这样。—-闫小鸿</p><a id="more"></a><p>&emsp;&emsp;最近把这本书读完了，在看书友评论的时，觉得上述描述实为贴切。<br>&emsp;&emsp;读前两章时，觉得这个人挺会过日子的，能把平淡的生活过成花，丰富多彩。这样的慢情趣着实让生活在这个快节奏的我心生羡慕。仔细想想，这样的小事也可发生在我们的日常生活中，只是我们没有足够的耐心，静下来，慢慢的想，细细的品。<br>&emsp;&emsp;读后两章时，更多的是思考，开始思考为啥会有如此的境地，在当时的情况下，如果是我，我会如何规划自己的未来，才能在一定程度上缓解这些问题；在现在的情况下（研三毕业季），我就是我，我该如何规划自己的未来（工作、从政 or 继续深造）。<br>&emsp;&emsp;前几天翻到了高中时记录的文字。非常有趣的事，在那紧张的学习氛围中，竟然可以挖掘出生活中的趣事，这与目前悠闲自得的生活形成了鲜明的对比，我想我已经找到了原因。</p><p><hr></p><p><center><p>那花，有时带着希望</p></center></p><p style="text-indent:2em">上学期布置教室时，买来了两盆盆栽，我们甚是喜欢。可能是假期里没人照料的缘故吧，这学期来的时候，看见的只有那干枯的骨架了，略显骨干美。</p><p style="text-indent:2em">第一次归宿假回来后，一个同学说她带来了一些向日葵的种子，说是要将它们种在这空空如也的花盆上。我们都以为她只是说着玩的，没想到他真的把它们种了下去。</p><p style="text-indent:2em">种子落入了土里，就意味着一个新生命的诞生，希望的开始。他们在这片贫瘠的土地里，吸收水分，膨胀生长。有的在这个过程中中途夭折，成为了同伴的养料。剩下的就算是同环境抗争后的胜利者，享受着胜利的果实。但它们仍艰难的生长着。因为这是一场持久战，如化蛹成茧，再破茧成蝶般漫长。漫长得时针走过一圈又一圈，漫长得太阳升了落下再升起，漫长得让我们怀疑这两盆装满泥土的花盆里是否真的有生命的迹象。</p><p style="text-indent:2em">就在我们几乎把它们从记忆里给摸掉的时候，一些绿色如雨后春笋奇迹般的出现在花盆里。点缀着花盆。我们都被这微小的生命震惊了。因为从种下到现在已经有两周的时间了。我们都以为那些种子都腐烂了。但是事实就在眼前，点缀花盆的是绿色。且它们正以极快的速度向上生长着。</p><p style="text-indent:2em">花苗一天天的生长着，每当它们顺利的熬过一天，就意味着面临着更多的困难。环境的变化无常，虫子的频繁来袭，人类的玩弄。然而这些因素都是它们必须面对的。在自然面前，它们显得是那么的渺小无助，况且一个小小的花盆给予它们的东西是有限的。</p><p style="text-indent:2em">花苗艰难的生长着，他们的活着是我们最大的期望。期望看到它开花，看到它因追随阳光而时刻改变自己的方向。到现在，花苗已有一尺高了。在那幼嫩的枝头，，已孕育出花苞来。那些点缀在绿叶间的黄绿色的小花苞，像是所有积蓄起来的希冀，等待在一场雨后阳光的沐浴下渐次盛放。</p><p style="text-indent:2em">人总是需要通过希望与梦想这些微妙的东西衬托，才会有生活下去的勇气，否则，生命将只能在黯淡里作茫然而无望的泅渡。而那花，有时带着希望。</p><p style="text-indent:2em">2012-5-19 于四川泸县第二中学</p>]]></content>
      
      
      <categories>
          
          <category> 记书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 记书 </tag>
            
            <tag> 《浮生六记》 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>免费临时短信临时邮箱接收验证码</title>
      <link href="/2020/04/15/%E4%B8%B4%E6%97%B6%E7%9F%AD%E4%BF%A1%E4%B8%B4%E6%97%B6%E9%82%AE%E7%AE%B1%E6%8E%A5%E6%94%B6%E9%AA%8C%E8%AF%81%E7%A0%81/"/>
      <url>/2020/04/15/%E4%B8%B4%E6%97%B6%E7%9F%AD%E4%BF%A1%E4%B8%B4%E6%97%B6%E9%82%AE%E7%AE%B1%E6%8E%A5%E6%94%B6%E9%AA%8C%E8%AF%81%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<p>现在大多数软件都需要注册验证，不注册就不给用。对于经常使用的软件来说，注册后可以保存一些信息，还是挺好的，但是对于那些只用一次就不用的软件来说，还必须要注册才能用，还疯狂推送广告，这简直就是耍流氓，比如某剪辑软件。<br>这种情况下可以考虑使用临时手机或者临时邮箱来进行注册验证。<br><a id="more"></a></p><h3 id="1-临时短信验证"><a href="#1-临时短信验证" class="headerlink" title="1 临时短信验证"></a>1 临时短信验证</h3><h4 id="1-1-网站列表"><a href="#1-1-网站列表" class="headerlink" title="1.1 网站列表"></a>1.1 网站列表</h4><ul><li><a href="https://yunduanxin.net/" target="_blank" rel="noopener">https://yunduanxin.net/</a></li><li><a href="https://www.pdflibr.com/" target="_blank" rel="noopener">https://www.pdflibr.com/</a></li><li><a href="https://smsreceivefree.com/" target="_blank" rel="noopener">https://smsreceivefree.com/</a></li><li><a href="https://www.receive-sms-online.info/" target="_blank" rel="noopener">https://www.receive-sms-online.info/</a></li><li><a href="https://receive-a-sms.com/" target="_blank" rel="noopener">https://receive-a-sms.com/</a></li><li><a href="https://smsnumbersonline.com/" target="_blank" rel="noopener">https://smsnumbersonline.com/</a></li><li><a href="https://sms-online.co/receive-free-sms" target="_blank" rel="noopener">https://sms-online.co/receive-free-sms</a></li><li><a href="https://receive-sms.com/" target="_blank" rel="noopener">https://receive-sms.com/</a></li><li><a href="http://receivefreesms.com/" target="_blank" rel="noopener">http://receivefreesms.com/</a></li><li><a href="https://www.receivesmsonline.net/" target="_blank" rel="noopener">https://www.receivesmsonline.net/</a></li><li><a href="https://www.freeonlinephone.org/" target="_blank" rel="noopener">https://www.freeonlinephone.org/</a></li><li><a href="http://receive-sms-online.com/" target="_blank" rel="noopener">http://receive-sms-online.com/</a></li><li><a href="https://www.textnow.com/" target="_blank" rel="noopener">https://www.textnow.com/</a></li><li><a href="https://www.pinger.com/text-free/" target="_blank" rel="noopener">https://www.pinger.com/text-free/</a></li><li><a href="http://sms.sellaite.com/" target="_blank" rel="noopener">http://sms.sellaite.com/</a></li><li><a href="https://www.twilio.com/" target="_blank" rel="noopener">https://www.twilio.com/</a></li><li><a href="https://www.pdflibr.com/" target="_blank" rel="noopener">https://www.pdflibr.com/</a></li></ul><h4 id="1-2-实验"><a href="#1-2-实验" class="headerlink" title="1.2 实验"></a>1.2 实验</h4><p>（1）注册某软件，发送验证码<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/59.png" alt="某软件的注册页面"></p><p>（2）查看验证码<br><a href="https://yunduanxin.net/" target="_blank" rel="noopener">https://yunduanxin.net/</a> 平台效果<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/10442.png" alt="https://yunduanxin.net/"></p><h3 id="2-临时邮箱验证"><a href="#2-临时邮箱验证" class="headerlink" title="2 临时邮箱验证"></a>2 临时邮箱验证</h3><h4 id="2-1-网站列表"><a href="#2-1-网站列表" class="headerlink" title="2.1 网站列表"></a>2.1 网站列表</h4><ul><li><a href="https://linshiyou.com/" target="_blank" rel="noopener">https://linshiyou.com/</a> 这个平台不仅提供临时邮箱，还提供免费发送短信等更多的服务。</li><li><a href="http://www.yopmail.com/zh/email-generator.php" target="_blank" rel="noopener">http://www.yopmail.com/zh/email-generator.php</a></li><li><a href="http://24mail.chacuo.net/" target="_blank" rel="noopener">http://24mail.chacuo.net/</a></li></ul><h4 id="2-2-实验"><a href="#2-2-实验" class="headerlink" title="2.2 实验"></a>2.2 实验</h4><p>（1）注册某软件，发送验证码<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/1118.png" alt="发送邮箱验证码"></p><p>（2）查看验证码<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/11747.png" alt="https://linshiyou.com/"></p><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>这里只对每种验证方式的第一个做了测试，速度还可以。<br>另外，请合法使用这些工具。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>临时手机接收验证码及临时邮箱：<a href="https://www.cnblogs.com/hookjoy/p/10606677.html" target="_blank" rel="noopener">https://www.cnblogs.com/hookjoy/p/10606677.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 资料整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具推荐 </tag>
            
            <tag> 软件推荐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用bat批处理命令实现Hexo文章的自动创建和部署</title>
      <link href="/2020/04/12/%E5%88%A9%E7%94%A8bat%E6%89%B9%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BA%E5%92%8C%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2/"/>
      <url>/2020/04/12/%E5%88%A9%E7%94%A8bat%E6%89%B9%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BA%E5%92%8C%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>之前每次写文章时都是手动的使用命令创建文件，写好后又通过相应的命令将其部署到github上，然后一直就在想，能不能通过bat命令来实现这些机械性的操作，本文就详细介绍这个过程。<br><a id="more"></a></p><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h3><p>想要利用bat批处理命令实现每次写文章时那些机械性的重复敲命令的过程，然后在网上搜了下，还真有一些大佬用了这种方法来实现，然后就参照他们的代码进行了初步实验。发现真的可行。<br>下面就给出相应的脚步。</p><h3 id="2-具体实验过程"><a href="#2-具体实验过程" class="headerlink" title="2 具体实验过程"></a>2 具体实验过程</h3><p>（1）自动创建文章<br>这一部分主要是想实现自动创建md文件，这样每次就不用手动输入文件中的头部信息了。在创建这个文件后并用指定的编辑器打开这个文件。<br><strong>create,bat文件</strong></p><pre class=" language-lang-Bash"><code class="language-lang-Bash">@echo offset /p name=input name:echo name:%name%echo please waithexo new post %name% && call openIDE.bat %name%pause</code></pre><p>这个文件是自动创建文章的主文件，在成功创建文件后调用打开IDE的脚本。<br><strong>openIDE.bat</strong></p><pre class=" language-lang-Bash"><code class="language-lang-Bash">start /min /w mshta vbscript:setTimeout("window.close()",1000)echo the input is %1set name=%1echo %name%start /d "D:\Program Files (x86)\Notepad++" notepad++.exe "F:\研究生\hexo\blog\source\_posts\%name%.md"pause</code></pre><p>（2）自动部署<br>这个主要是把部署的三条命令写进文件里。<br><strong>deploy.bat</strong></p><pre class=" language-lang-Bash"><code class="language-lang-Bash">start clean.batstart /min /w mshta vbscript:setTimeout("window.close()",2000)start gd.batexit</code></pre><p>这个文件是部署的主文件，先是调用清理缓存的脚本，然后再调用生成和部署脚本。<br><strong>clean.bat</strong></p><pre class=" language-lang-Bash"><code class="language-lang-Bash">@echo offecho cleanecho please waithexo cleanexit</code></pre><p><strong>gd.bat</strong></p><pre class=" language-lang-Bash"><code class="language-lang-Bash">start /min /w mshta vbscript:setTimeout("window.close()",2000)hexo g -dexit</code></pre><p>基本上就是这样了，需要注意的有以下几点：</p><ul><li>bat文件的编码方式请采用ANSI编码，这样才能正常显示中文</li><li>这些bat文件需要放在博客目录下</li><li>记得把相关文件路径替换成自己的文件路径</li></ul><h3 id="3-后记"><a href="#3-后记" class="headerlink" title="3 后记"></a>3 后记</h3><p>后面想到可以将这些脚本适当的扩展成通用性更强的客户端：</p><ul><li>设置配置文件，把涉及到的文件路径写入配置文件，在初次运行客户端时提示用户指定这些路径，每次运行时只需读取配置文件即可；</li><li>设置文件监听，把自动创建和自动部署合并，在自动创建文件后对文件进行监听，等文件修改完成后再调用自动部署脚本；</li><li>设置IDE扩展，把自启动编辑器设置成可供用户选择的方式，用户想用那种IDE就用那种。</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>（1）<a href="https://www.jianshu.com/p/c27fee631efd" target="_blank" rel="noopener">【Hexo】bat自动处理hexo命令</a><br>（2）<a href="http://www.manongjc.com/article/55860.html" target="_blank" rel="noopener">Hexo博客使用bat脚本自动部署和免密码部署</a><br>（3）<a href="https://www.jianshu.com/p/c27fee631efd" target="_blank" rel="noopener">Hexo，使用 bat 脚本部署文章</a></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
            <tag> bat批处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python36+cuda9+cudnn7+pytorch1_1环境配置</title>
      <link href="/2019/12/04/pytorch/"/>
      <url>/2019/12/04/pytorch/</url>
      
        <content type="html"><![CDATA[<p>之前配置过环境，然后过段时间就忘了，最近又在折腾机器，重新配置环境，跟着网上的教程来，发现走了很多坑，所以记录下。</p><h2 id="我的账号是带sudo权限的非root用户，不带sudo权限应该也是可以的。"><a href="#我的账号是带sudo权限的非root用户，不带sudo权限应该也是可以的。" class="headerlink" title="我的账号是带sudo权限的非root用户，不带sudo权限应该也是可以的。"></a>我的账号是带sudo权限的非root用户，不带sudo权限应该也是可以的。</h2><h3 id="1、准备"><a href="#1、准备" class="headerlink" title="1、准备"></a>1、准备</h3><p>前期准备主要是版本对应上：</p><h5 id="cuda和驱动程序"><a href="#cuda和驱动程序" class="headerlink" title="cuda和驱动程序"></a>cuda和驱动程序</h5><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191204214403.png" alt="cuda&amp;driver"></p><h5 id="cuda和cudnn"><a href="#cuda和cudnn" class="headerlink" title="cuda和cudnn"></a>cuda和cudnn</h5><p>具体对应关系参照：<a href="https://blog.csdn.net/omodao1/article/details/83241074" target="_blank" rel="noopener">https://blog.csdn.net/omodao1/article/details/83241074</a><br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191204215027.png" alt="cuda&amp;cudnn"></p><h5 id="cuda和pytorch"><a href="#cuda和pytorch" class="headerlink" title="cuda和pytorch"></a>cuda和pytorch</h5><p>前往pytorch官网查看版本的对应关系。<a href="https://pytorch.org" target="_blank" rel="noopener">https://pytorch.org</a><br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191204214639.png" alt="cuda&amp;pytorch"></p><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><h4 id="2-1-安装驱动程序"><a href="#2-1-安装驱动程序" class="headerlink" title="2.1 安装驱动程序"></a>2.1 安装驱动程序</h4><p>这个驱动程序其实可以在装cuda的时候装，也可以单独安装。由于原来的驱动程序和现在的cuda版本对应不上，所以我是把原来的驱动程序卸载了，在安装cuda时重新安装的，单独安装方法可以在网上搜下，但是一定要记住版本对应上（驱动程序和显卡型号也要对应上）。</p><h4 id="2-2-安装cuda"><a href="#2-2-安装cuda" class="headerlink" title="2.2 安装cuda"></a>2.2 安装cuda</h4><p>去官网下载安装包，我的系统是Ubuntu，所以下载的是cuda_9.0.176_384.81_linux.run。<br>然后在同级目录下执行：</p><pre class=" language-lang-Python"><code class="language-lang-Python">bash cuda_9.0.176_384.81_linux.run</code></pre><p>先后会遇到各种配置问题：</p><ul><li>是否accept？ accept</li><li>driver？ 这个根据自己的情况来，如果是已经安装了则no，如果已经安装了，想重新安装，需要将之前的卸载了再安装，此时yes</li><li>openGL? 好像是如果安装driver时才有的选项，yes</li><li>nvidia-xconfig? 这里我选择的no</li><li>cuda tookit? yes</li><li>cuda安装路径：这个根据自己的需要来看是否修改</li><li>cuda samples ? yes</li><li>samples 路径：根据情况修改</li></ul><p>这个步骤没出问题的话，基本上就是可以了的，网上有些教程里修改了cuda的环境变量，但是我没有修改，却可以使用，不知道是不是版本的原因？<br>测试是否安装成功：<br>(1)看nvcc版本nvcc —version<br>(2)看驱动能不能用nvidia-smi<br>(3)进入到samples文件夹在用户目录下或者/usr/local/cuda/samples下都有，进入1_Utilities/deviceQuery中，make 然后运行./deviceQuery，会输出一堆信息，最后是PASS，之前都对。</p><h4 id="2-3-安装cudnn"><a href="#2-3-安装cudnn" class="headerlink" title="2.3 安装cudnn"></a>2.3 安装cudnn</h4><p>去官网下载，要注册的。<a href="https://developer.nvidia.com/rdp/cudnn-download。选择cuDNN" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download。选择cuDNN</a> v5.1 Library for linux下载之后解压，下载下来之后是.tgz后缀，然后用tar -xvf解压即可。然后把lib和include拷贝到cuda对应的目录下：</p><pre class=" language-lang-Python"><code class="language-lang-Python">    sudo cp cuda/include/cudnn.h /usr/local/cuda/include    sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</code></pre><p>网上的教程说这里还要修改文件权限和建立链接，我没有弄，结果还是正确的。</p><h4 id="2-4-安装python、pytorch"><a href="#2-4-安装python、pytorch" class="headerlink" title="2.4 安装python、pytorch"></a>2.4 安装python、pytorch</h4><p>这部分就相对来说简单很多了，可以直接参考官网教程：<a href="https://pytorch.org" target="_blank" rel="noopener">https://pytorch.org</a></p><h3 id="3-测试pytorch的gpu环境是否安装好"><a href="#3-测试pytorch的gpu环境是否安装好" class="headerlink" title="3 测试pytorch的gpu环境是否安装好"></a>3 测试pytorch的gpu环境是否安装好</h3><pre class=" language-lang-Python"><code class="language-lang-Python"># CUDA TESTimport torchx = torch.Tensor([1.0])xx = x.cuda()print(xx)# cuDNN testfrom torch.backends import cudnnprint(cudnn.is_acceptable(xx))print(torch.cuda.is_available())</code></pre><p>我的输出结果：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191204221125.png" alt="测试输出结果"></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小记BertTokenizer中英文分词</title>
      <link href="/2019/11/28/%E5%B0%8F%E8%AE%B0BertTokenizer%E4%B8%AD%E8%8B%B1%E6%96%87%E5%88%86%E8%AF%8D/"/>
      <url>/2019/11/28/%E5%B0%8F%E8%AE%B0BertTokenizer%E4%B8%AD%E8%8B%B1%E6%96%87%E5%88%86%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<p>使用Hugging Face提供的pytorch接口来使用bert，这篇主要记录下BertTokenizer中英文分词情况。<a id="more"></a></p><hr><h3 id="1、准备"><a href="#1、准备" class="headerlink" title="1、准备"></a>1、准备</h3><pre class=" language-lang-Python"><code class="language-lang-Python">pip install pytorch-pretrained-bert</code></pre><h3 id="2、分词思路"><a href="#2、分词思路" class="headerlink" title="2、分词思路"></a>2、分词思路</h3><p>BertTokenizer 是用WordPiece模型创建的。这个模型使用贪心法创建了一个固定大小的词汇表，其中包含单个字符、子单词和最适合我们的语言数据的单词。由于我们的BERT tokenizer模型的词汇量限制大小为30,000，因此，用WordPiece模型生成一个包含所有英语字符的词汇表，再加上该模型所训练的英语语料库中发现的~30,000个最常见的单词和子单词。这个词汇表包含以下一些东西：</p><ul><li>整个单词</li><li>出现在单词前面或单独出现的子单词</li><li>不在单词前面的子单词，在前面加上“##”来表示这种情况</li><li>单个字符</li></ul><p>要在此模型下对单词进行记号化，tokenizer首先检查整个单词是否在词汇表中。如果没有，则尝试将单词分解为词汇表中包含的尽可能大的子单词，最后将单词分解为单个字符。注意，由于这个原因，我们总是可以将一个单词表示为至少是它的单个字符的集合。<br>因此，不是将词汇表中的单词分配给诸如“OOV”或“UNK”之类的全集令牌，而是将词汇表中没有的单词分解为子单词和字符令牌，然后我们可以为它们生成嵌入。</p><p>例如，对于词汇表中没有的单词embedding，我们没有将“embeddings”和词汇表之外的每个单词分配给一个重载的未知词汇表标记，而是将其拆分为子单词标记[‘ em ‘、’ ##bed ‘、’ ##ding ‘、’ ##s ‘]，这些标记将保留原单词的一些上下文含义。我们甚至可以平均这些子单词的嵌入向量来为原始单词生成一个近似的向量。</p><h3 id="3、简述分词源代码"><a href="#3、简述分词源代码" class="headerlink" title="3、简述分词源代码"></a>3、简述分词源代码</h3><p>BERT的分词是Python的代码，打开tokenization.py就能看到这个文件里的代码还是很容易理解的。主要有两类，一类是功能性方法，另一类是不同的分词方法类。</p><h4 id="3-1-功能性方法"><a href="#3-1-功能性方法" class="headerlink" title="3.1  功能性方法"></a>3.1  功能性方法</h4><p>主要包含了5个功能方法：<br><strong>load_vocab(vocab_file)</strong> ：主要是加载词汇表，构建一个有序字典，并建立词汇和id之间的映射关系<br><strong>whitespace_tokenize(text)</strong>：将给定文本按照空格进行分词。<br><strong>_is_punctuation(char)</strong>：判断给定字符是否是标点符号；<br><strong>_is_control(char)</strong>：判断给定的字符是否是转义字符；<br><strong>_is_whitespace(char)</strong>：判断给定的支付是否是空白符；</p><h4 id="3-2-三种分词方法"><a href="#3-2-三种分词方法" class="headerlink" title="3.2  三种分词方法"></a>3.2  三种分词方法</h4><p>（1）最常用的分词方法<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191128205055.png" alt="常用分词方法"><br>构造函数需要传入参数词典vocab_file和do_lower_case，函数首先调用load_vocab加载词典，建立词到id的映射关系。接下来是构造BasicTokenizer和WordpieceTokenizer。前者是根据空格等进行普通的分词，而后者会把前者的结果再细粒度的切分为WordPiece。<br>tokenize函数实现分词，它先调用BasicTokenizer进行分词，接着调用WordpieceTokenizer把前者的结果再做细粒度切分。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191128205606.png" alt="tokenize函数实现分词"><br>（2）BasicTokenizer<br>（3）WordpieceTokenizer<br>后面这两种分词方法不做详细介绍，需要深入了解的可以直接查看源代码。</p><h3 id="4、简单用法demo"><a href="#4、简单用法demo" class="headerlink" title="4、简单用法demo"></a>4、简单用法demo</h3><p>首先要下载预训练好的模型，然后放在指定目录下：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/0618/20191203153325.png" alt="下载的支持中文的预训练模型"></p><pre class=" language-lang-Python"><code class="language-lang-Python">from pytorch-pretrained-bert.tokenization import BertTokenizer# 加载bert分词器tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese', do_lower_case=True)print(tokenizer.tokenize("我要去看北京的天安门"))print(tokenizer.tokenize("I will go to BeiJing"))</code></pre><p>分词结果（中文是将文本分成单个字，而不是词语）：</p><pre class=" language-lang-Python"><code class="language-lang-Python">['我', '要', '去', '看', '北', '京', '的', '天', '安', '门']['i', 'will', 'go', 'to', 'be', '##i', '##jing']</code></pre><h3 id="5、相关链接"><a href="#5、相关链接" class="headerlink" title="5、相关链接"></a>5、相关链接</h3><p>【1】<a href="https://blog.csdn.net/u011984148/article/details/99921480" target="_blank" rel="noopener">BERT中的词向量指南，非常的全面，非常的干货</a><br>【2】<a href="https://blog.csdn.net/jiaowoshouzi/article/details/89388794" target="_blank" rel="noopener">一本读懂BERT(实践篇)</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 记录 </tag>
            
            <tag> 分词 </tag>
            
            <tag> Bert </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>300行python代码从零开始构建基于知识图谱的电影问答系统2-系统业务逻辑介绍</title>
      <link href="/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F2-%E7%B3%BB%E7%BB%9F%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F2-%E7%B3%BB%E7%BB%9F%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>昨天把这个教程的目录给理出来了，然后今天就想趁着这满腔热情（<del>无心搬砖</del>）把剩下的教程也写了。 对于整个系统，不管具体细节是怎么实现的，是逻辑一定要理清楚，于是这一节主要介绍各个模块的逻辑，为什么要这么做。</p></blockquote><p>首先我简单的画了一个示意图（这叫啥图我也还给软件工程老师了，肯定不规范，帮老师画了一上午的图，心都累了，将就着看吧）<br><img src="https://img-blog.csdnimg.cn/20190416143958243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="这是那个啥示意图"><br>从第一部分的<a href="https://blog.csdn.net/xyz1584172808/article/details/89319129" target="_blank" rel="noopener">目录</a>和上面的示意图我们我们都可以了解到，实际上要处理的就是那几个蓝色实线框，在实现那几个蓝色实线框之前还需要做一些准备，也就是蓝色圆角虚线框，下面我将对各个部分简单的啰嗦下。</p><ul><li>问题预处理</li></ul><p>问题预处理要做些什么事呢，这就要看看我们想要从问题中获取点什么东西了，首先我们想获取问题中的询问对象是什么，也就是主语，是人还是电影，这就涉及到自然语言处理中的命名实体识别；接着我们还想从问题中了解到用户想问什么，也就是用户的意图，要了解用户意图，这就涉及到文本表示问题，最最基本的文本表示方法是one-hot形式，在试验中使用的是sklearn中的tfidf工具。</p><ul><li>抽取关键信息</li></ul><p>对于关键信息抽取中的命名实体识别，在实验过程中，在这一部分我们其实用的词性标注，因为我发现，词性标注工具可以把人民标注出来，如下：</p><pre><code>nr    人名    名词代码 n和“人(ren)”的声母并在一起。</code></pre><p>所以，只要我们把问题词性标注后，找到对应的nr对应的单词，那就是人名啦，电影名称的识别和这个差不多，在下一篇实验准备的博文中将具体介绍怎么提高识别率。</p><ul><li><p>训练问题分类器</p><p>我们要想搞清楚用户到底想问啥，是问某某演了那些电影嘛，还是某某电影什么时候上映呢？想想我们在思考这个疑问的过程中是不是把问题在脑海里面进行的分类处理，其实在自然语言处理中很多问题都可以抽象成分类问题。好，我们现在把用户想问啥抽象成了分类问题，那用户问题主要涉及到哪些方面，可以分成多少类呢？以及我们怎么识别这个用户问题属于那一类呢？一种最直接的方法就是把用户会问到的各个方面都列出来，然后分门别类，这样就解决了前两个问题。对于怎么识别用户的问题属于那一类呢？当然选择分类器啦，但是等等，训练分类器需要数据咋办。解决办法就是前面不是把用户问题分成了很多类嘛，对于每一类问题，想想用户会以什么样的方式来提问，然后我们把这些提问方式记录下来，这样得到了该类别问题所对应的训练数据。是不是感觉很麻烦，我也觉得有点麻烦，这么麻烦的事其实没必要重复去造轮子（亲自去构建这些数据也不是不可以），这里需要想搞明白的这个思路是怎么来的（该教程针对的像我一样的小白，大佬请自动忽略，囧~~），这部分的数据在下一篇博文中会给出链接，<a href="https://github.com/IrvingBei/simple_movie_qa_with_KG" target="_blank" rel="noopener">github</a>目录下也有。（感觉有点啰嗦了），部分数据样例如下：</p><pre><code>nnt演了什么电影nnt出演了什么电影nnt演过什么电影nnt演过哪些电影nnt过去演过哪些电影nnt以前演过哪些电影nnt演过的电影有什么nnt有哪些电影nnt演过那些电影作品nnt的电影作品有哪些</code></pre></li><li><p>问题模板</p></li></ul><p>在前面训练问题分类器中，我们把用户的问题归纳成了很多类，在这个模块下，我们要做的事就是对各个类别进行抽象，比如对于用户询问某某演过哪些电影等一系列问题，我们可以把它抽象成：</p><pre><code>nr 电影作品</code></pre><p>$nr$前面介绍过了，代表人名，在模板中我们使用nnt来表示的。于是我们得到了用户各种问题的模板（$nm$代表电影名称，$ng$代表电影类型）：</p><pre><code>0:nm 评分1:nm 上映时间2:nm 类型3:nm 简介4:nm 演员列表5:nnt 介绍6:nnt ng 电影作品7:nnt 电影作品8:nnt 参演评分 大于 x9:nnt 参演评分 小于 x10:nnt 电影类型11:nnt nnr 合作 电影列表12:nnt 电影数量13:nnt 出生日期</code></pre><ul><li>查询答案</li></ul><p>在知道用户想问啥的时候，我们就可以根据用户的要求来查询，得到答案返回，这部分主要是如何对图数据库进行操作。</p><p>好了，到了现在，你对主要的模块都了解了，然后最后我来串一哈，首先得到用户的问题，从用户的问题中得到问题的关键信息，主要是人名、电影名称等信息，接着是对问题进行分类，看问题想问什么，预测出问题模板，在得到问题模板后，使用人名或电影名等具体信息来替换模板中的抽象信息，得到一个新的问题，下面是一个实际的例子：</p><pre><code>刘德华演过哪些电影呀？</code></pre><p>获取关键信息：$刘德华$<br>问题分类得到问题模板：$7:nnt 电影作品$<br>进行替换得到新的问题：</p><pre><code>刘德华 电影作品</code></pre><p>然后就根据这个新的问题来查询，获得答案返回。</p><p>这个系统逻辑介绍就啰嗦到这里吧，废话挺多的，至于每个模块怎么实现的，那就接着看后续文章吧</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 问答系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>300行python代码从零开始构建基于知识图谱的电影问答系统3-实验环境和实验数据准备</title>
      <link href="/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F3-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E5%92%8C%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/"/>
      <url>/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F3-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E5%92%8C%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>貌似很久没有写了，这段时间一直在忙着准备复试，就有点耽误了，好吧，今天继续写。<del>你们的魔鬼又来啦</del>  （什么鬼 </p></blockquote><p>在上一篇中，我对整个系统的业务逻辑啰里啰唆的梳理了一遍，如果你被我绕晕了，那也没关系，因为不用看上面那篇也能继续往下走，当你自己理清楚他的逻辑的时候，你就会有一种踏破铁鞋无觅处，柳暗花明又一村的感觉，好吧下面言归正传。</p><p>这一篇主要介绍实验的准备工作，也就是为后续工作铺平道路，主要包含实验环境和实验数据准备两部分，那么接下来就依次介绍这两个。</p><h3 id="1、实验环境的搭建"><a href="#1、实验环境的搭建" class="headerlink" title="1、实验环境的搭建"></a>1、实验环境的搭建</h3><h5 id="（1）python环境的安装"><a href="#（1）python环境的安装" class="headerlink" title="（1）python环境的安装"></a>（1）python环境的安装</h5><p>本教程是基于python实现的，所以最基本的python环境要有，我的python版本是3.6，建议直接下载对应版本的anaconda来安装python，python环境搭建的具体细节这里就不做过多的介绍了，网上有很多，也可以直接参考anaconda官网。<br>接下来是安装依赖库，我把本项目涉及到的<a href="https://github.com/IrvingBei/simple_movie_qa_with_KG/blob/master/requirements.txt" target="_blank" rel="noopener">依赖库</a>打包好了，可以直接使用下面的命令来安装：</p><pre><code>pip install -r requirements.txt</code></pre><p>哦，对了，这个项目基于webpy库来搭建的，上面的库里面已经包含了，但是这里安装可以会出错，那就等其他库装好了再来装他。</p><blockquote><p>2019年5月10日09:03:18更新：webpy安装方式：<br><img src="https://img-blog.csdnimg.cn/20190510090408838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="webpy安装方式"></p></blockquote><p>通过上述步骤，那么基本的环境就搭建好了，这好比修一座房子，四面墙砌好了。</p><h5 id="（2）知识图谱基本介绍"><a href="#（2）知识图谱基本介绍" class="headerlink" title="（2）知识图谱基本介绍"></a>（2）知识图谱基本介绍</h5><p>关于知识图谱，我自己也没研究得多深，所以请大家移步机器之心科普性的文章→ <a href="https://www.jiqizhixin.com/articles/2018-06-20-4" target="_blank" rel="noopener">这是一份通俗易懂的知识图谱技术与应用指南</a>，读了这篇文章，相信你对知识图谱有了一个初步的印象，其实质就是利用三元组来表示实体的一些信息，而关于这些信息的存储，一种是基于RDF的存储；另一种是基于图数据库的存储。而本项目采用的是图数据库存储，主要是图数据库较简单，还直观，要是你搞了半天，一个像样一点的东西都没搞出来，你还有继续搞下去的信心吗？于是我就选择了图数据库neo4j，关于这个数据库的安装，请参照<a href="https://blog.csdn.net/qq_30843221/article/details/53306540" target="_blank" rel="noopener">关于ubuntu下neo4j的安装与使用</a>不管你是在win下安装，还是linux下安装都是可以的，因为这些坑我都踩过了，另外，我这里给出来的链接都是我之前实验时，成功了的教程，如果我再重复写一遍，貌似也没有多少意思，倒不如直接把他们的链接给出来，同时也我对这些文章的感谢。好了，到了现在，python的开发环境有了，知识图谱的存储环境有了，就差数据了，所谓万事俱备只欠东风。<br><img src="https://img-blog.csdnimg.cn/20190506201635126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="这是安装好的数据库界面"></p><h3 id="2、实验数据"><a href="#2、实验数据" class="headerlink" title="2、实验数据"></a>2、实验数据</h3><p>数据来源于IMDB数据库，这是一个关于电影演员、电影、电视节目、电视明星和电影制作的在线数据库。我们需要哪些数据呢？想想我们的目标是啥，我们要查询某部电影的信息，比如电影的评分，上映时间、大体讲了啥内容等，也可能查询某位演员的个人信息，演了那些电影等，好了，这里就出现了两个实体，$电影$，$人物$，然后人物和电影之间有着直接的关系：<strong>[act]</strong>，即某人出演了某部电影，于是可以用这个关系连链接演员和电影，此外，图数据库的好处在这里就凸显出来了，除了这种实体与实体的关系外，图数据库还可以链接实体和他的属性，所以，为了说明这种情况，单独构造了电影的类型，他是电影的属性，电影和他之间的关系是<strong>is</strong>。那么理下关系，主要有电影和人物这两个实体，他们之间的关系是出演，比如李连杰出演了《卧虎藏龙》，除此之外，实体还有自己的属性，特别的，对电影的类型这一属性单独处理。数据的格式如下：<br>全部数据文件：<br><img src="https://img-blog.csdnimg.cn/20190506204004599.png" alt="全部数据文件"><br>人物实体数据文件：<br><img src="https://img-blog.csdnimg.cn/20190506204109978.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="人物实体数据文件"><br>人物和电影之间的关系链接：<br><img src="https://img-blog.csdnimg.cn/2019050620420781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这里的pid指的是personid，mid指的是movieid，是不是和传统的关系型数据库很像。</p><p>前面对数据的情况进行了介绍，那么怎么获取这样的数据呢？一种方法是爬去imdb上的数据，按照上面的说明来处理，另一种当然是直接下载我这处理好的数据啦，然后把数据csv文件放入neo4j安装目录下的import目录下，链接：<a href="https://pan.baidu.com/s/1HgjZFQ7q4V_8EzzjNmMwwQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1HgjZFQ7q4V_8EzzjNmMwwQ</a><br>提取码：7qv1<br>复制这段内容后打开百度网盘手机App，操作更方便哦我的路径是：</p><pre><code>E:\neo4j-community-3.5.3-windows\neo4j-community-3.5.3\import</code></pre><p>然后你们参照着这个来找吧。</p><p>上一篇中提到了问题分类的训练集怎么构造的，也提到了问题模板，<a href="https://github.com/IrvingBei/simple_movie_qa_with_KG/tree/master/movie_QA_with_KQ/data/question" target="_blank" rel="noopener">github</a>上也有相关数据，可以直接下载，其中A是问题分类的训练数据，B是问题模板。<br><img src="https://img-blog.csdnimg.cn/20190506205317373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="问题分类相关数据集"><br>好吧，今天就写到这里了，我感觉又是一大堆废话，才写没多久，哎，大家先讲究着看，如果实在看不下去了就去找其他教程吧。希望以后空闲时间多一点的时候，多一些干活，少一些废话，保质保量。</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 问答系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>300行python代码从零开始构建基于知识图谱的电影问答系统4-用户问题预处理</title>
      <link href="/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F4-%E7%94%A8%E6%88%B7%E9%97%AE%E9%A2%98%E9%A2%84%E5%A4%84%E7%90%86/"/>
      <url>/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F4-%E7%94%A8%E6%88%B7%E9%97%AE%E9%A2%98%E9%A2%84%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>今天冷得，我在五月份穿了三件衣服，你敢相信。。。</p></blockquote><p>这篇主要介绍对用户问题的处理，也就是从获取用户问题到明白用户意图这个过程，主要涉及到命名实体识别（这个任务简单，我就用词性标注来代替了），问题分类，以及填充问题模板这几个部分。介绍的时候，可能会用一些代码来说明，但是下面列出来的代码并不完整，完整的代码请参照github。这些代码只是辅助理解整个过程，这样去看代码的时候才容易理清函数之间的来龙去脉。再说明下，这个系统适合那些刚入门没多久，通过实现一个小东西来练练手的同学，而这篇教程，可以看作是代码的说明书，帮助理解代码，主要还是梳理逻辑。</p><h5 id="关键信息抽取"><a href="#关键信息抽取" class="headerlink" title="关键信息抽取"></a>关键信息抽取</h5><p>这部分就是通过命名实体识别来获取用户问题中的人名，电影名等信息，而在实验过程中，在这一部分我们其实用的词性标注，因为我发现，词性标注工具可以把人名标注出来，如下使用jieba进行词性标注时的含义：</p><pre><code>nr    人名    名词代码 n和“人(ren)”的声母并在一起。</code></pre><p>所以，只要我们把问题词性标注后，找到对应的nr对应的单词，那就是人名啦，电影名称的识别和这个差不多。这就是这部分的主要思路，接下来就看下代码是怎么实现的。</p><ul><li>用户问题词性标注</li></ul><p>功能的实现就是下面这个函数：</p><pre><code>    def question_posseg(self):        jieba.load_userdict(&quot;./data/userdict3.txt&quot;)        clean_question = re.sub(&quot;[\s+\.\!\/_,$%^*(+\&quot;\&#39;)]+|[+——()?【】“”！，。？、~@#￥%……&amp;*（）]+&quot;,&quot;&quot;,self.raw_question)        self.clean_question=clean_question        question_seged=jieba.posseg.cut(str(clean_question))        result=[]        question_word, question_flag = [], []        for w in question_seged:            temp_word=f&quot;{w.word}/{w.flag}&quot;            result.append(temp_word)            # 预处理问题            word, flag = w.word,w.flag            question_word.append(str(word).strip())            question_flag.append(str(flag).strip())        assert len(question_flag) == len(question_word)        self.question_word = question_word        self.question_flag = question_flag        print(result)        return result</code></pre><p>jieba的词性标注和分词是同步进行的，所以如果分词不准确的话，那么词性标注往往也会出错，比如说电影《卧虎藏龙》，被jieba分词分为：</p><pre><code>卧虎  藏龙</code></pre><p>这样肯定就不能识别出卧虎藏龙这个实体了啊，人名也是同样的道理，如果把一个演员的姓名分开了，也就不能标注出这样一个人名了，于是我就加了一个自定义字典，把所涉及到的所有电影，所有人名都加到了这个字典中，这样以来，jieba分词的时候就会参考字典里面的词来进行分词和词性标注了，自定义词典的格式如下：</p><pre><code>太极气功 15 nm功夫小子 15 nm大师 15 nm...陈雅伦 15 nr李修贤 15 nr黄锦燊 15 nr潘恒生 15 nr林熙蕾 15 nr锺丽缇 15 nr...恐怖 15 ng动作 15 ng喜剧 15 ng历史 15 ng</code></pre><p>这样涉及到的电影名，人名以及电影类型就不会被jieba分词分开了，此外还需要去掉问题中的特殊字符，这些操作搞完了后就可以进行分词和词性标注了，处理后把结果返回即可。</p><ul><li>问题分类和模板填充</li></ul><p>接下来是对用户的问题进行分类，获取对应问题模板，从而明白用户的意图，首先要使用前面介绍的根据用户习惯构造的各种各样的问题来训练一个分类器，我这里使用的sklearn里面的贝叶斯分类器。<br>首先是组织训练数据：</p><pre><code># 获取训练数据    def read_train_data(self):        train_x=[]        train_y=[]        file_list=getfilelist(&quot;./data/question/&quot;)        # 遍历所有文件        for one_file in file_list:            # 获取文件名中的数字            num = re.sub(r&#39;\D&#39;, &quot;&quot;, one_file)            # 如果该文件名有数字，则读取该文件            if str(num).strip()!=&quot;&quot;:                # 设置当前文件下的数据标签                label_num=int(num)                # 读取文件内容                with(open(one_file,&quot;r&quot;,encoding=&quot;utf-8&quot;)) as fr:                    data_list=fr.readlines()                    for one_line in data_list:                        word_list=list(jieba.cut(str(one_line).strip()))                        # 将这一行加入结果集                        train_x.append(&quot; &quot;.join(word_list))                        train_y.append(label_num)        return train_x,train_y</code></pre><p>接着是训练多分类贝叶斯分类器模型，并返回：</p><pre><code># 训练并测试模型-NB    def train_model_NB(self):        X_train, y_train = self.train_x, self.train_y        self.tv = TfidfVectorizer()        train_data = self.tv.fit_transform(X_train).toarray()        clf = MultinomialNB(alpha=0.01)        clf.fit(train_data, y_train)        return clf</code></pre><p>利用训练好的模型来对新问题进行分类：</p><pre><code># 预测    def predict(self,question):        question=[&quot; &quot;.join(list(jieba.cut(question)))]        test_data=self.tv.transform(question).toarray()        y_predict = self.model.predict(test_data)[0]        # print(&quot;question type:&quot;,y_predict)        return y_predict</code></pre><p>返回用户问题所属的类别编号，这个编号也就对应一个问题模板：</p><pre><code>0:nm 评分1:nm 上映时间2:nm 类型3:nm 简介4:nm 演员列表5:nnt 介绍6:nnt ng 电影作品7:nnt 电影作品8:nnt 参演评分 大于 x9:nnt 参演评分 小于 x10:nnt 电影类型11:nnt nnr 合作 电影列表12:nnt 电影数量13:nnt 出生日期</code></pre><p>数字和模板的对应关系可以提前存到一个字典中，当预测出编号后，直接通过这个编号作为字典的key值，这样就查询出问题模板了。比如预测的结果是2，则对应的问题模板 $nm 类型$，表示询问某部电影的类型，再结合前一阶段获取的电影名字，则可以组成一个新的问题，比如：</p><pre><code>卧虎藏龙 类型</code></pre><p>上面对涉及到的重点部分进行了介绍，这里小结一下，当遇到一个新问题时：</p><pre><code>1、得到原始问题；2、抽取问题中的关键信息，比如从问题“张学友的个人介绍”中抽取出张学友这个人名；3、使用分类模型来预测问题类别，预测出模板编号，查询字典得到问题模板；如dict[5]:nnt 介绍；4、替换模板中的抽象内容，得到：张学友 介绍。</code></pre><p>大体思路是这样。</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 问答系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>300行python代码从零开始构建基于知识图谱的电影问答系统5-答案获取</title>
      <link href="/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F5-%E7%AD%94%E6%A1%88%E8%8E%B7%E5%8F%96/"/>
      <url>/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F5-%E7%AD%94%E6%A1%88%E8%8E%B7%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>啦啦啦，终于快写完了，虽然我也知道并没有写什么实质性的东西，至少我坚持下来啦，后面再慢慢多拧拧，少一些水分。</p></blockquote><p>在上一篇中，主要介绍了如何从接收到的用户问题中抽取关键信息，以及如何识别用户的意图，那么接下来就将介绍在得到了这些信息后，如何在知识图谱中查询答案。我在处理这个问题时，想得很直接，简单来说，每个问题模板就对应了一个用户意图，那么就按照每个意图来写查询语句，这是一种简单粗暴的方法，优点就是就只有简单了，缺点当然很多了，比如不利于维护以及扩展等。现在要有维护扩展的意识，但是具体实现是，如果想快速实现一个demo的话，可以使用这种简单粗暴方法。下面就对获取答案进行介绍。</p><p>首先，我定义了一个问题模板的方法字典，每一个key对应模板的编号，value就是根据该模板来查询答案的方法，结构如下：</p><pre><code>self.q_template_dict={            0:self.get_movie_rating,            1:self.get_movie_releasedate,            2:self.get_movie_type,            3:self.get_movie_introduction,            4:self.get_movie_actor_list,            5:self.get_actor_info,            6:self.get_actor_act_type_movie,            7:self.get_actor_act_movie_list,            8:self.get_movie_rating_bigger,            9:self.get_movie_rating_smaller,            10:self.get_actor_movie_type,            11:self.get_cooperation_movie_list,            12:self.get_actor_movie_num,            13:self.get_actor_birthday        }</code></pre><p>当我们得到用户问题，以及该问题的模板后需要做那些工作呢？<br>首先时利用上一篇介绍的方法来获取问题的关键信息，然后就根据模板编号来调用相应的方法，整体框架的代码如下：</p><pre><code>    def get_question_answer(self,question,template):        # 如果问题模板的格式不正确则结束        assert len(str(template).strip().split(&quot;\t&quot;))==2        template_id,template_str=int(str(template).strip().split(&quot;\t&quot;)[0]),str(template).strip().split(&quot;\t&quot;)[1]        self.template_id=template_id        self.template_str2list=str(template_str).split()        # 预处理问题        question_word,question_flag=[],[]        for one in question:            word, flag = one.split(&quot;/&quot;)            question_word.append(str(word).strip())            question_flag.append(str(flag).strip())        assert len(question_flag)==len(question_word)        self.question_word=question_word        self.question_flag=question_flag        self.raw_question=question        # 根据问题模板来做对应的处理，获取答案        answer=self.q_template_dict[template_id]()        return answer</code></pre><p>进入到对应的方法中，利用Cypher语言来构建查询语句，其基本形式是：</p><pre><code>match(n)-[r] -(b)</code></pre><p>Cypher语言不太懂没关系，也可以直接使用python的库py2neo来操作图数据库neo4j。这里只涉及到查询操作，所以我直接构造了Cypher查询语句，然后使用py2neo库的run方法来查询，数据库的链接和查询我单独的写了一个类，在需要的时候调用即可：</p><pre><code>from py2neo import Graph,Node,Relationship,NodeMatcherclass Query():    def __init__(self):        self.graph=Graph(&quot;http://localhost:7474&quot;, username=&quot;neo4j&quot;,password=&quot;123456&quot;)    # 问题类型0，查询电影得分    def run(self,cql):        # find_rela  = test_graph.run(&quot;match (n:Person{name:&#39;张学友&#39;})-[actedin]-(m:Movie) return m.title&quot;)        result=[]        find_rela = self.graph.run(cql)        for i in find_rela:            result.append(i.items()[0][1])        return result</code></pre><p>比如这里以查询某部电影的演员列表为例，查询答案的代码为：</p><pre><code># 4:nm 演员列表    def get_movie_actor_list(self):        movie_name=self.get_movie_name()        cql = f&quot;match(n:Person)-[r:actedin]-&gt;(m:Movie) where m.title=&#39;{movie_name}&#39; return n.name&quot;        print(cql)        answer = self.graph.run(cql)        answer_set = set(answer)        answer_list = list(answer_set)        answer = &quot;、&quot;.join(answer_list)        final_answer = movie_name + &quot;由&quot; + str(answer) + &quot;等演员主演！&quot;        return final_answer</code></pre><p>主要是构造Cypher语句，然后调用py2neo的run方法，对得到的答案进行处理这几步，得到答案返回即可。</p><p>这个系列这里也就差不多了吧，除了目录，也只有4篇，所以这并不是一个很详细的教程，主要是想帮助大家对代码的理解，理清思路或者扰乱思路。在做试验的时候，我就是先理清思路再写的，这或许是一个不错的方法，由于这个教程不是很详细，大家有被我搅晕的地方，可以给我发邮件（irvingbei@qq.com ）或者直接评论，也可以去网上找找资料。最后，这也是一个python版本，很好上手，大家不妨试一试。</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 问答系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的搬砖工具（科研工具）推荐-附下载链接</title>
      <link href="/2019/04/16/%E6%88%91%E7%9A%84%E6%90%AC%E7%A0%96%E5%B7%A5%E5%85%B7%EF%BC%88%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7%EF%BC%89%E6%8E%A8%E8%8D%90-%E9%99%84%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5/"/>
      <url>/2019/04/16/%E6%88%91%E7%9A%84%E6%90%AC%E7%A0%96%E5%B7%A5%E5%85%B7%EF%BC%88%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7%EF%BC%89%E6%8E%A8%E8%8D%90-%E9%99%84%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5/</url>
      
        <content type="html"><![CDATA[<h3 id="论文搜索："><a href="#论文搜索：" class="headerlink" title="论文搜索："></a><strong>论文搜索：</strong></h3><ul><li><a href="https://www.semanticscholar.org/" target="_blank" rel="noopener">semantic scholar</a> or 谷歌学术  or   谷歌学术搜索按钮插件  </li><li><p>NLP论文list:  <a href="https://aclanthology.coli.uni-saarland.de/" target="_blank" rel="noopener">ACL Anthology</a></p><p>第一个国内都可以访问，后面的需要访问国外，如果学校图书馆可以上谷歌学术的话，可以直接使用后两个。做自然语言处理的可以在ACL网站上找最新的论文。</p></li></ul><h3 id="文献管理工具："><a href="#文献管理工具：" class="headerlink" title="文献管理工具："></a>文献管理工具：</h3><p><a href="https://www.mendeley.com/?interaction_required=true" target="_blank" rel="noopener"><strong>Mendeley</strong></a></p><p>之前不习惯用文献管理工具，就把论文分门别类的放在指定的文件夹下，但是找起来非常不方便，后面找到了mendeley，也慢慢的习惯了，后面还给整个小组安利这款文献管理工具，刚开始使用可能不太熟悉，网上有相应的教程。对了，他还会根据你的文献，给你推荐论文。</p><h3 id="pdf阅读器："><a href="#pdf阅读器：" class="headerlink" title="pdf阅读器："></a>pdf阅读器：</h3><p> <a href="https://blog.csdn.net/xintingandzhouyang/article/details/82558235" target="_blank" rel="noopener"><strong>Adobe Acrobat 2017</strong></a></p><p>虽然mendeley里面也可以阅读pdf，里面的highlight功能和notes功能也特别好用，但是因为大部分用有道词典划词翻译的时候可能会出现多余的空格，以至于不能准确的翻译。然后也因为另一个工作不得不使用acorbat，所以干脆就都使用这个阅读器了（是缘分让我们相遇）。这个软件是集阅读和编辑于一身，所以我把福析阅读器都给卸载了。</p><h3 id="思维导图工具："><a href="#思维导图工具：" class="headerlink" title="思维导图工具："></a>思维导图工具：</h3><p><a href="https://blog.csdn.net/qq_16093323/article/details/80967867" target="_blank" rel="noopener"><strong>xmind 8 pro 破解版</strong></a></p><p>pro版有各种模板，也可以直接将思维导图转化为word，ppt等格式。有人推荐幕布，这个没用过，大家也可以试试。</p><h6 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h6><p>幕布使用感受，同步功能挺方便的，有这个需求的可以尝试。</p><h3 id="根据论文找代码"><a href="#根据论文找代码" class="headerlink" title="根据论文找代码"></a>根据论文找代码</h3><p><a href="https://paperswithcode.com/" target="_blank" rel="noopener"><strong>paperswithcode</strong></a><br><strong>github</strong><br>这个网站上，输入论文名字，就会匹配出对应的代码，也会给出当前任务的SOTA的分数和代码等信息。不过有时候在这个网站上搜不到，这个时候可以直接在github上搜索.</p><h3 id="论文翻译工具"><a href="#论文翻译工具" class="headerlink" title="论文翻译工具"></a>论文翻译工具</h3><p><a href="http://tongtianta.site/" target="_blank" rel="noopener">通天塔</a></p><p>免费的论文翻译喔</p><h3 id="将word格式转化为markdown格式"><a href="#将word格式转化为markdown格式" class="headerlink" title="将word格式转化为markdown格式"></a>将word格式转化为markdown格式</h3><p><a href="https://word2md.com/" target="_blank" rel="noopener"><strong>word2md</strong></a><br>只需要散步就可以将将word格式转化markdown格式，前提是你word的格式很标准，我是这么用的，首先使用上面的思维导图对论文做笔记，然后转化为word，如果要发博客的话，就通过这个工具将word格式转为md格式，手动调整的地方很少。</p><h3 id="将markdown文件排版为微信公众号文章"><a href="#将markdown文件排版为微信公众号文章" class="headerlink" title="将markdown文件排版为微信公众号文章"></a>将markdown文件排版为微信公众号文章</h3><p><a href="http://blog.didispace.com/tools/online-markdown/" target="_blank" rel="noopener"><strong>markdown转化工具</strong></a><br>如果你不需要啥花里胡哨的东西，这个基本上就够用了，专业的微信排版工具可以使用135编辑器，他那个封面图的做的还可以。</p><h3 id="论文书写工具"><a href="#论文书写工具" class="headerlink" title="论文书写工具"></a>论文书写工具</h3><ul><li><a href="https://www.overleaf.com/" target="_blank" rel="noopener"><strong>overleaf</strong></a></li><li><a href="http://www.tablesgenerator.com/" target="_blank" rel="noopener"><strong>tablegenerator</strong></a></li></ul><p>这个也是我给同学强烈安利的一个文具，在线latex编辑器，还可以多人合作，还有各种会议期刊的模板，简直不要太好，后面一个是快速生成表格代码的工具。论文中的矢量图的处理过程为：visio画图=》保存成pdf=》使用上面推荐的pdf编辑器对pdf进行裁剪=》另存为eps格式。</p><h3 id="人工智能领域国际会议投稿deadline"><a href="#人工智能领域国际会议投稿deadline" class="headerlink" title="人工智能领域国际会议投稿deadline"></a>人工智能领域国际会议投稿deadline</h3><ul><li><a href="https://aideadlin.es/?sub=NLP,RO,SP,ML,CV" target="_blank" rel="noopener"><strong>AI Conference Deadlines</strong></a></li></ul><h3 id="英语写作相关"><a href="#英语写作相关" class="headerlink" title="英语写作相关"></a>英语写作相关</h3><ul><li>语法检查工具： Grammaly、1Checker、ginger</li><li>词语搭配工具：Writefull、Linggle</li></ul><p>上面那三个软件建议都安装，反正也不大，可以是单独的安装，也可以安装成office的插件，也可以在添加到谷歌浏览器的插件，这样在office里面写英语，网页上写英语，语法啥的都有提示，慢慢的你就有写英语的信心了。</p><h3 id="效率工具"><a href="#效率工具" class="headerlink" title="效率工具"></a>效率工具</h3><ul><li>文件搜索工具：listry</li></ul><h3 id="图床上传工具"><a href="#图床上传工具" class="headerlink" title="图床上传工具"></a>图床上传工具</h3><ul><li>PicGo+github</li></ul><p>之前写博客一直在找图床，后面试过各种方法，还是这个最给力</p><h3 id="文献分析工具"><a href="#文献分析工具" class="headerlink" title="文献分析工具"></a>文献分析工具</h3><ul><li>Histcite pro</li></ul><h3 id="百度云盘搜索"><a href="#百度云盘搜索" class="headerlink" title="百度云盘搜索"></a>百度云盘搜索</h3><ul><li>网址：wowenda.com</li><li>软件：Pandownload</li></ul><p>强烈安利pandownload这个软件，下载不限速，分享链接自动识别，还可以通过这个来搜索资源</p><h3 id="markdown书写软件"><a href="#markdown书写软件" class="headerlink" title="markdown书写软件"></a>markdown书写软件</h3><ul><li>MarkdownPad 2</li><li>notepad++</li></ul><p>这两个都可以来写md文件，第二个可以边写边看，配合picgo+github，写作飞快。</p><h3 id="数学公式工具"><a href="#数学公式工具" class="headerlink" title="数学公式工具"></a>数学公式工具</h3><ul><li>Axmath</li><li>Mathpix Snipping Tool</li></ul><p>第一个主要配合ppt和word来编辑公式，比office自带的方便很多，第二个可以用于ocr数学公式并生成latex公式代码。</p><h3 id="下载工具"><a href="#下载工具" class="headerlink" title="下载工具"></a>下载工具</h3><ul><li><a href="https://download.csdn.net/download/xyz1584172808/9923413" target="_blank" rel="noopener">迅雷极速版</a></li></ul><p>不限速，没广告，软件只找到之前上传的资源链接，如果积分不够，给我发邮件，我帮忙下载。</p><h3 id="搜软件资源"><a href="#搜软件资源" class="headerlink" title="搜软件资源"></a>搜软件资源</h3><ul><li><a href="https://www.baidu.com/link?url=fPjktH9sUnUK68LdYeFOAoSWTT0hPRQKuPc-Ds6riU_&amp;wd=&amp;eqid=aa5db877000074c1000000035dfa1b29" target="_blank" rel="noopener">macno1</a></li></ul><h3 id="录屏软件"><a href="#录屏软件" class="headerlink" title="录屏软件"></a>录屏软件</h3><ul><li>ScreenToGif</li></ul><p>小巧，可编辑，还可以转化为各种格式。</p><h2 id="更多工具更新中。。。。"><a href="#更多工具更新中。。。。" class="headerlink" title="更多工具更新中。。。。"></a>更多工具更新中。。。。</h2>]]></content>
      
      
      <categories>
          
          <category> 资料整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件推荐 </tag>
            
            <tag> 科研工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>300行python代码从零开始构建基于知识图谱的电影问答系统-目录</title>
      <link href="/2019/04/15/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F-%E7%9B%AE%E5%BD%95/"/>
      <url>/2019/04/15/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F-%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<blockquote><p>平时除了看论文还是看论文，感觉有点无聊，于是嘛就想着搞点东西来玩玩，然后就搞了一个非常简单的基于知识图谱的电影问答系统。系统是用python实现的，大概只花了1天吧，代码也仅有300多行，可以说是很容易上手了。然后在这里也简单的记录下整个搭建过程，梳理思路，给那些像我一样想玩玩的童鞋一些参考，大佬请自动跳过。</p></blockquote><p>首先给来看看我们要实现的是什么东西，效果图如下：<br><img src="https://img-blog.csdnimg.cn/20190415192606960.gif" alt="demo"><br>完成这个系统主要涉及到以下一些知识点，其实知识点说不上，只需要对这些概念有印象即可，这个系统本来就是做着玩的，还不需要用那些复杂的东西。</p><ul><li>网络爬虫</li><li>自然语言处理</li><li>知识图谱</li><li>图数据库</li><li><p>机器学习</p><p>看着这些点感觉内容挺多的，其实真正用到的只是一丢丢啦，比如说对于机器学习，其实我们就是用其中的一个分类器就可以了。这几部分我都写成了对应的处理类，代码也上传<a href="https://github.com/IrvingBei/simple_movie_qa_with_KG" target="_blank" rel="noopener">github</a>了，想提前玩玩的可以去下载来试试<del>（要是能给我个star就更好了（我怎么也这么无耻的来要star了。。。。</del> </p></li></ul><blockquote><p>本系列教程除了目录只有4篇，或许这并不是一个很详细的教程，主要是想帮助大家对代码的理解，理清思路或者扰乱思路。在做试验的时候，我就是先理清思路再写的，这是一个不错的方法，由于这个教程不是很详细，大家有被我搅晕的地方，可以给我发邮件（irvingbei@qq.com<br>或者xiongzy@std.uestc.edu.cn），也可以去网上找找资料。</p></blockquote><p>接下来根据业务逻辑从以下几个问题入手来介绍整个构建过程：</p><ul><li><a href="http://bei.dreamcykj.com/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F2-%E7%B3%BB%E7%BB%9F%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91%E4%BB%8B%E7%BB%8D/">系统的整体逻辑是怎么样的？（系统业务逻辑介绍）</a></li><li><a href="http://bei.dreamcykj.com/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F3-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E5%92%8C%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/">需要做那些准备工作？（实验环境和实验数据准备）</a></li><li><a href="http://bei.dreamcykj.com/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F4-%E7%94%A8%E6%88%B7%E9%97%AE%E9%A2%98%E9%A2%84%E5%A4%84%E7%90%86/">接收到用户的问题后需要怎么处理用户问题？（用户问题预处理）</a></li><li><a href="http://bei.dreamcykj.com/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F4-%E7%94%A8%E6%88%B7%E9%97%AE%E9%A2%98%E9%A2%84%E5%A4%84%E7%90%86/">怎么识别用户到底问的啥？（用户问题分类）</a></li><li><p><a href="http://bei.dreamcykj.com/2019/04/16/300%E8%A1%8Cpython%E4%BB%A3%E7%A0%81%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%94%B5%E5%BD%B1%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F5-%E7%AD%94%E6%A1%88%E8%8E%B7%E5%8F%96/">如果根据用户问题来查找答案？（答案获取）</a></p><p>对于这个系统的前端页面，是以前搞得一个基于信息检索的问答系统的界面，源代码见<a href="https://github.com/IrvingBei/chatbot_with_IR" target="_blank" rel="noopener">github</a></p></li></ul><p>好啦，目录就这样吧，等后面把对应的部分写了就链接上。</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 问答系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beam search 算法的通俗理解</title>
      <link href="/2019/04/11/Beam%20search%20%E7%AE%97%E6%B3%95%E7%9A%84%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3/"/>
      <url>/2019/04/11/Beam%20search%20%E7%AE%97%E6%B3%95%E7%9A%84%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>Beam search 算法在文本生成中用得比较多，用于选择较优的结果（可能并不是最优的）。接下来将以seq2seq机器翻译为例来说明这个Beam search的算法思想。<br>在机器翻译中，beam search算法在测试的时候用的，因为在训练过程中，每一个decoder的输出是有与之对应的正确答案做参照，也就不需要beam search去加大输出的准确率。<br>有如下从中文到英语的翻译：<br>中文：</p><pre><code>我 爱 学习，学习 使 我 快乐</code></pre><p>英语：</p><pre><code>I love learning, learning makes me happy</code></pre><p>在这个测试中，中文的词汇表是{我，爱，学习，使，快乐}，长度为5。英语的词汇表是{I, love, learning, make, me, happy}（全部转化为小写），长度为6。那么首先使用seq2seq中的编码器对中文序列（记这个中文序列为$X$）进行编码，得到语义向量$C$。<br><img src="https://img-blog.csdnimg.cn/20190411204526407.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="seq2seq"><br>得到语义向量$C$后，进入解码阶段，依次翻译成目标语言。在正式解码之前，有一个参数需要设置，那就是beam search中的beam size，这个参数就相当于top-k中的k，选择前k个最有可能的结果。在本例中，我们选择beam size=3。</p><p>来看解码器的第一个输出$y_1$，在给定语义向量$C$的情况下，首先选择英语词汇表中最有可能k个单词，也就是依次选择条件概率$P(y_1|C)$前3大对应的单词，比如这里概率最大的前三个单词依次是$I$，$learning$，$happy$。</p><p>接着生成第二个输出$y_2$，在这个时候我们得到了那些东西呢，首先我们得到了编码阶段的语义向量$C$，还有第一个输出$y_1$。此时有个问题，$y_1$有三个，怎么作为这一时刻的输入呢（解码阶段需要将前一时刻的输出作为当前时刻的输入），答案就是都试下，具体做法是：</p><ul><li>确定$I$为第一时刻的输出，将其作为第二时刻的输入，得到在已知$(C, I)$的条件下，各个单词作为该时刻输出的条件概率$P(y_2|C,I)$，有6个组合，每个组合的概率为$P(I|C)P(y_2|C, I)$。</li><li>确定$learning$为第一时刻的输出，将其作为第二时刻的输入，得到该条件下，词汇表中各个单词作为该时刻输出的条件概率$P(y_2|C, learning)$，这里同样有6种组合；</li><li>确定$happy$为第一时刻的输出，将其作为第二时刻的输入，得到该条件下各个单词作为输出的条件概率$P(y_2|C, happy)$，得到6种组合，概率的计算方式和前面一样。</li></ul><p>这样就得到了18个组合，每一种组合对应一个概率值$P(y_1|C)P(y_2|C, y_1)$，接着在这18个组合中选择概率值top3的那三种组合，假设得到$I love$，$I happy$，$learning make$。<br>接下来要做的重复这个过程，逐步生成单词，直到遇到结束标识符停止。最后得到概率最大的那个生成序列。其概率为：</p><script type="math/tex; mode=display">P(Y|C)=P(y_1|C)P(y_2|C,y_1),...,P(y_6|C,y_1,y_2,y_3,y_4,y_5)</script><p>以上就是Beam search算法的思想，当beam size=1时，就变成了贪心算法。</p><p>Beam search算法也有许多改进的地方，根据最后的概率公式可知，该算法倾向于选择最短的句子，因为在这个连乘操作中，每个因子都是小于1的数，因子越多，最后的概率就越小。解决这个问题的方式，最后的概率值除以这个生成序列的单词数（记生成序列的单词数为$N$），这样比较的就是每个单词的平均概率大小。<br>此外，连乘因子较多时，可能会超过浮点数的最小值，可以考虑取对数来缓解这个问题。</p><p>参考文献：<br>吴恩达-《序列模型》课程</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 常见算法 </tag>
            
            <tag> 搜索算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记14】Gated-Attention Readers for Text Comprehension</title>
      <link href="/2019/04/03/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B014%E3%80%91Gated-Attention%20Readers%20for%20Text%20Comprehension/"/>
      <url>/2019/04/03/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B014%E3%80%91Gated-Attention%20Readers%20for%20Text%20Comprehension/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍GA reader模型的基本思想，由论文《Gated-Attention Readers for Text Comprehension》提出，<a href="https://aclweb.org/anthology/P17-1168" target="_blank" rel="noopener">论文直通车</a></p><h3 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1 论文概述"></a>1 论文概述</h3><p>本篇论文的思想其实是很简单的，可以把其看成是AS reader模型的扩展，尽管模型简单，但是也取得了不错的成绩，也证明了乘法操作机制带来的效果显著。值得一提的是，论文中的相关工作部分，对之前的模型进行了分门别类的概述，这一点要比其他论文的相应部分丰富得多。</p><h3 id="2-模型细节"><a href="#2-模型细节" class="headerlink" title="2 模型细节"></a>2 模型细节</h3><p>GA reader模型的整体架构如下图所示，看起来很多层有些复杂，其实单独拿粗来看就不复杂了。<br><img src="https://i.imgur.com/YJ7tEYL.png" alt="模型结构图"><br>总的来说可以分为四个模块：</p><ul><li>文本表示模块</li><li>多层结构</li><li>GA 模块</li><li>答案预测模块</li></ul><p>下面就依次来介绍这些模块，不过都是介绍最基本的模型，不包括后面追加的特征。</p><h4 id="（1）文本表示模块"><a href="#（1）文本表示模块" class="headerlink" title="（1）文本表示模块"></a>（1）文本表示模块</h4><p>文本表示是基础，这篇论文中对问题和文章的表示均采用文本表示法的模型一，即对文本中的单词序列逐一表示，而不是对问题或文章的整体语义信息进行表示。使用这种方法的模型较多，因为没有将所有信息都压缩在一个向量中，所以这样对问题或文章的语义信息保留得相对完整。将输入序列$X=[x_1 , x_2 , … , x_T ]$送入GRU，得到输出$H=[h_1 , h_2 , … , h_T]$，在这个模块中，本篇论文使用的是BiGRU来对文章和问题进行处理，所以对输出的前后向隐层状态进行拼接：</p><script type="math/tex; mode=display">\overleftrightarrow{GRU}(X)=[h_1^f||h_T^b , ... , h_T^f|| h_1^b]</script><p>最后，使用$X^{(0)}=[x_1^{(0)},x_2^{(0)}, … , x_{|D|}^{(0)}]$表示文章序列，使用$Y^{(0)}=[y_1,y_2, … , y_{|Q|}]$表示问题序列。<br>文章表示序列中有个上标$(0)$，因为在这个模型中有很多层，添加上标为了区别于后面每层的文章表示。下面就介绍，不同层之间文章表示是怎么形成的。</p><h4 id="（2）多层结构"><a href="#（2）多层结构" class="headerlink" title="（2）多层结构"></a>（2）多层结构</h4><p>在上一个模块中得到了文章序列和问题序列表示，在这一个模块中，使用两个不同的BiGRU分别对文章和问题进行处理：</p><script type="math/tex; mode=display">D^{(k)}=\overleftrightarrow{GRU}_D^{(k)}(X^{(k-1)})</script><script type="math/tex; mode=display">Q^{(k)}=\overleftrightarrow{GRU}_Q^{(k)}(X)</script><p>这里的上标也是表示这是第几层的，接下来使用Gated-Attention来对这两个信息进行组合，作为下一层的输入：</p><script type="math/tex; mode=display">X^{(k)}=GA(D^{(k)},Q^{(k)})</script><p>其中GA就是接下来要介绍的GA模块。</p><h4 id="（3）GA模块"><a href="#（3）GA模块" class="headerlink" title="（3）GA模块"></a>（3）GA模块</h4><p>这个模块的思想也是很容易理解的，对于文章中的每一个单词$d_i$，后续操作可以分解成三个操作：</p><ul><li>计算问题中每个单词与这个给定单词之间的相关性：$\alpha _i=softmax(Q^T d_i)$</li><li>对问题进行加权求和，得到特定于文章单词$d_i$的问题表示：$\widehat{q_i}=Qa_i$</li><li>将特定的问题表示和这个文章单词做乘法操作：$x_i=d_i \odot \widehat{q_i}$</li></ul><h4 id="（4）答案预测模块"><a href="#（4）答案预测模块" class="headerlink" title="（4）答案预测模块"></a>（4）答案预测模块</h4><p>答案预测模块和AS reader模块的一样，主要有以下两个部分：</p><script type="math/tex; mode=display">Pr(c|d,q) \propto \sum_{i \in I_(c,d)}(s_i)</script><p>其中，$I(c,d)$是单词c在文章d中出现位置的集合，最后概率最大的作为答案：</p><script type="math/tex; mode=display">a^{\star}=argmax_{c \in C}Pr(c|d,q)</script>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记13】Attention-over-Attention Neural Networks for Reading Comprehension</title>
      <link href="/2019/04/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B013%E3%80%91Attention-over-Attention%20Neural%20Networks%20for%20Reading%20Comprehension/"/>
      <url>/2019/04/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B013%E3%80%91Attention-over-Attention%20Neural%20Networks%20for%20Reading%20Comprehension/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍AoA reader 的模型结构，由论文《Attention-over-Attention Neural Networks for Reading Comprehension》提出，<a href="https://aclweb.org/anthology/P17-1055" title="论文链接" target="_blank" rel="noopener">论文直通车</a></p><h3 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1 论文概述"></a>1 论文概述</h3><p>该论文针对机器阅读理解任务，提出了AoA reader模型，在CNN/Daily Mail 和CBT数据集上取得了不错的效果。</p><h3 id="2-模型细节"><a href="#2-模型细节" class="headerlink" title="2 模型细节"></a>2 模型细节</h3><p><img src="https://i.imgur.com/A1eLkuS.png" alt="模型结构"></p><p>模型的结构图如上图所示，主要包含了以下几个模块：</p><ul><li>上下文编码模块（Contextual Embedding）</li><li>两两匹配模块（Pair-wise Matching Score）</li><li>独立注意力模块（Individual Attentions）</li><li>AoA模块（Attention-over-Attention）</li><li>最终预测模块（Final Predictions）</li></ul><p>接下来就逐一介绍这些组成部分。</p><h4 id="（1）上下文编码模块"><a href="#（1）上下文编码模块" class="headerlink" title="（1）上下文编码模块"></a>（1）上下文编码模块</h4><p>对于问题和文章的表示，整体来说使用的文本表示法中的模型1的方法（即不对文章或者问题的整体语义信息建模，而是对文本中的单词序列逐一表示）。本文中的具体做法是，首先先将文章和问题分别表示成one-hot形式，然后使用词向量矩阵将问题和文章中的one-hot形式转化为向量形式（注意，在具体实现的过程中，使用的随机初始化词向量矩阵而没有使用预训练），最后将这些向量序列送入BiRNN，最后每个单词的表示是由该单词的前向和后向隐层状态拼接得到的，这三个步骤可以用下面的几个公式来表示：</p><script type="math/tex; mode=display">e(x)=W_e \cdot x, where \ x \in D,Q</script><script type="math/tex; mode=display">\overrightarrow{h_s(x)}=\overrightarrow{GRU}(e(x))</script><script type="math/tex; mode=display">\overleftarrow{h_s(x)}=\overleftarrow{GRU}(e(x))</script><script type="math/tex; mode=display">h_s(x)=[\overrightarrow{h_s(x)};\overleftarrow{h_s(x)}</script><p>然后使用$h_{doc} \in R^{|D|<em>2d}$和$h_{query} \in R^{|Q|</em>2d}$分别表示文章和问题的上下文表示。</p><h4 id="（2）两两匹配模块（Pair-wise-Matching-Score）"><a href="#（2）两两匹配模块（Pair-wise-Matching-Score）" class="headerlink" title="（2）两两匹配模块（Pair-wise Matching Score）"></a>（2）两两匹配模块（Pair-wise Matching Score）</h4><p>这部分的操作很简单，利用点乘操作来得到文章中的一个词和问题中一个词的匹配程度。具体来说，给定文章中第i个单词以及问题中的第j个单词，将这两个向量进行点乘得到一个标量，这个标量就表示这两个向量之间的匹配程度。公式如下：</p><script type="math/tex; mode=display">M(i,j)=h_{doc}(i)^T \cdot h_{query}(j)</script><p>其实在这个公式里面，点乘是对应元素相乘后相加，个人感觉公式是有些问题的，公式中使用了转置，是矩阵乘法，那么就不应该再使用$\ \cdot \ $了，而应该使用$ <em> $，我也不清楚是不是我理解错了，反正最后得到的是一个标量。<br>将文章的单词和问题中的单词两两配对，得到矩阵$M(i,j) \in R^{|D|</em>|Q|}$，也就是下面这个：</p><p><img src="https://i.imgur.com/H82sT6s.png" alt="两量匹配"></p><h4 id="3-独立注意力模块"><a href="#3-独立注意力模块" class="headerlink" title="(3)独立注意力模块"></a>(3)独立注意力模块</h4><p>在得到文章和问题的单词两两匹配矩阵后，对每一列使用一个softmax函数，这样做的意义是，对于问题中$t$时刻的单词，得到一个独立的文档级的注意力（通俗点理解就是对于问题中的某一个单词，文章中的每个单词与该单词的相关程度）。操作的示意图如下：</p><p><img src="https://i.imgur.com/HzN88jU.png" alt="column-wise softmax"></p><p>对应的公式也很简单,首先是得到每一列处理后的结果</p><script type="math/tex; mode=display">a(t)=softmax(M(1,t),...,M(|D|,t)</script><p>将处理后的每一列进行拼接：<br>$a=[a(1),a(2),…,a(|Q|)]$</p><h4 id="4-AoA模块"><a href="#4-AoA模块" class="headerlink" title="(4)AoA模块"></a>(4)AoA模块</h4><p>这个模块很有意思，也是这篇论文的亮点，利用注意力来对注意力权重进行加权求和。它首先计算一个相反的注意力，具体来说，在前面那个M矩阵的行上使用softmax函数，表示的含义就变成了给定一个文章中的单词，问题中的那些单词对其的重要性，这样的操作的话，就会得到$|D|$个行向量，然后对这些向量求一个平均。<br><img src="https://i.imgur.com/yWw3DSi.png" alt=""></p><p>公式为：</p><script type="math/tex; mode=display">\beta (t)=softmax(M(t,1),...,M(t,|Q|))</script><script type="math/tex; mode=display">\beta = \frac{1}{n}\sum_{t=1}^{|D|} \beta (t)</script><p>这样就得到了上图右边的那个小向量。<br>接着就是重点了，$a$一个注意力权重矩阵，$ \beta $是一个权重向量，将$a$和$ \beta $做点乘，这样就得到了<strong>“attended document-level attention”</strong>，这个操作就是<strong>the attention-over-attention mechanism</strong>,AoA实质上就是利用注意力来对另一个层次的注意力加权求和。</p><p><img src="https://i.imgur.com/zgVqoWq.png" alt=""></p><p>表达式为$s=a^T \beta $,这里的公式表示我也觉得有点问题。</p><h4 id="（5）预测模块"><a href="#（5）预测模块" class="headerlink" title="（5）预测模块"></a>（5）预测模块</h4><p>预测模块和AS reader 的预测模块很相似，将某个单词在全文中出现地方的权重信息都加起来：</p><script type="math/tex; mode=display">P(w|D,Q)=\sum_{i \ in I(w,D)} s_i, \  w \in V</script><p>其中$I(w,D)$表示单词w在文章D中出现的位置集合。<br>最后目标函数如下：</p><script type="math/tex; mode=display">L=\sum_{i}log(p(x)), \ x \in A</script><p>本文也使用了“N-best Re-ranking Strategy”来选择最终的答案，这里就不做介绍了。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记12】Iterative Alternating Neural Attention for Machine Reading</title>
      <link href="/2019/03/30/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B012%E3%80%91Iterative%20Alternating%20Neural%20Attention%20for%20Machine%20Reading/"/>
      <url>/2019/03/30/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B012%E3%80%91Iterative%20Alternating%20Neural%20Attention%20for%20Machine%20Reading/</url>
      
        <content type="html"><![CDATA[<h4 id="（1）论文创新点"><a href="#（1）论文创新点" class="headerlink" title="（1）论文创新点"></a>（1）论文创新点</h4><ul><li>这个模型的文章和问题在真正使用时，都是根据前一时刻的隐层状态，动态的表示成一个向量；</li><li>在更新状态时，使用了gated机制(虽然看起来有点复杂)。</li></ul><h4 id="（2）论文模型"><a href="#（2）论文模型" class="headerlink" title="（2）论文模型"></a>（2）论文模型</h4><p>模型的整体结构图如下所示：<br><img src="https://i.imgur.com/ClYNuHO.png" alt="模型结构图"><br>接下来将对该模型上的组成部分做简单的介绍。<br><strong>bidirectional encoddings</strong><br>对于问题和文章的基本表示，主要是使用BiGRU来对文本序列进行编码。具体来说，对文章，不对文章的语义信息进行整体表示，而是对文章中的每个单词序列逐一表示（即文章表示法的模型1），每个单词的表示由双向GRU的前后隐层状态拼接后得到：</p><script type="math/tex; mode=display">d_i=[\vec{h_i},\overleftarrow{h_i}]</script><p>对于问题来说做同样的操作，对序列中每个单词进行表示：</p><script type="math/tex; mode=display">q_i=[\vec{h_i},\overleftarrow{h_i}]</script><blockquote><p>在对推理状态进行更新的时候，每次都会从问题和文章中抓取相关信息，于是构造了下面几个组成部分。</p></blockquote><p><strong>query attention</strong></p><p>这个我们可以把它形容成获取在当前时刻$t$问题的重点，这个重点是通过一个注意力机制得到的，通过公式：</p><script type="math/tex; mode=display">q_{i,t}=softmax[\widehat{q_i^T}(A_{q}S_{t-1}+a_{q})]</script><p>得到该时刻每个单词的权重（其中$\widehat{q_i^T}$是问题表示），于是这个时刻问题的重点由下面的公式表示：</p><script type="math/tex; mode=display">q_t=\sum_{i}q_{i,t}\widehat{q_i}</script><p><strong>document attention</strong><br>首先计算文章中每个单词的权重，结合问题的重点，然后得到文章这个时刻的重点：</p><script type="math/tex; mode=display">d_{i,t}=softmax[\widehat{d_i^T}(A_d[S_{t_1},q_t]+a_d)]</script><p><strong>search gates</strong><br>这个部分是由FFNN+sigmold来组成的，具体的计算公式如下：</p><script type="math/tex; mode=display">r=g([S_{t-1},q_t,d_t,q_t \cdot d_t])</script><p><strong>更新推理状态</strong><br>通过上述几个公式，得到问题和文章该时刻的重点，然后通过gates对这些重点进行过滤，结合前一时刻的推理状态来得到当前时刻的推理状态：</p><script type="math/tex; mode=display">S_t=f([r_q \cdot q_t , r_d \cdot d_t],S_{t-1})</script><h4 id="（3）小结"><a href="#（3）小结" class="headerlink" title="（3）小结"></a>（3）小结</h4><p>这个模型动态的构建问题，文章，以及推理状态之间的相关性，也就是动态的注意力机制。一个可以扩展的点就是动态的决定推理步骤，此外，gate的计算方式或许也可以优化下。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 动态注意力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch中LSTM输出问题</title>
      <link href="/2019/01/09/pytorch%E4%B8%ADLSTM%E8%BE%93%E5%87%BA%E9%97%AE%E9%A2%98/"/>
      <url>/2019/01/09/pytorch%E4%B8%ADLSTM%E8%BE%93%E5%87%BA%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>给出结论便于查询，具体分析过程见<a href="https://bigquant.com/community/t/topic/125904" target="_blank" rel="noopener">聊一聊PyTorch中LSTM的输出格式</a></p><ul><li>output保存了最后一层，每个time step的输出h，如果是双向LSTM，每个time step的输出h = [h正向, h逆向]<br>(同一个time step的正向和逆向的h连接起来)。 </li><li>h_n保存了每一层，最后一个time step的输出h，如果是双向LSTM，单独保存前向和后向的最后一个time step的输出h。</li><li>c_n与h_n一致，只是它保存的是c的值。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 实验笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Must-read papers on GNN</title>
      <link href="/2019/01/09/Must-read%20papers%20on%20graph%20neural%20networks%20(GNN)/"/>
      <url>/2019/01/09/Must-read%20papers%20on%20graph%20neural%20networks%20(GNN)/</url>
      
        <content type="html"><![CDATA[<p>为了保存资料，于是上传至博客。<br><a id="more"></a></p><h2 id="Must-read-papers-on-GNN"><a href="#Must-read-papers-on-GNN" class="headerlink" title="Must-read papers on GNN"></a>Must-read papers on GNN</h2><p>GNN: graph neural network</p><p>Contributed by Jie Zhou, Ganqu Cui and Zhengyan Zhang.</p><h3 id="Survey-papers"><a href="#Survey-papers" class="headerlink" title="Survey papers"></a>Survey papers</h3><ol><li><strong>Graph Neural Networks: A Review of Methods and Applications.</strong> <em>Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Maosong Sun.</em> 2018. <a href="https://arxiv.org/pdf/1812.08434.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>A Comprehensive Survey on Graph Neural Networks.</strong> <em>Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu.</em> 2019. <a href="https://arxiv.org/pdf/1901.00596.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Deep Learning on Graphs: A Survey.</strong> <em>Ziwei Zhang, Peng Cui, Wenwu Zhu.</em> 2018. <a href="https://arxiv.org/pdf/1812.04202.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Relational Inductive Biases, Deep Learning, and Graph Networks.</strong> <em>Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others.</em> 2018. <a href="https://arxiv.org/pdf/1806.01261.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Geometric Deep Learning: Going beyond Euclidean data.</strong> <em>Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre.</em> IEEE SPM 2017. <a href="https://arxiv.org/pdf/1611.08097.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Computational Capabilities of Graph Neural Networks.</strong> <em>Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele.</em> IEEE TNN 2009. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4703190" target="_blank" rel="noopener">paper</a></li><li><strong>Neural Message Passing for Quantum Chemistry.</strong> <em>Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E.</em> 2017. <a href="https://arxiv.org/pdf/1704.01212.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Non-local Neural Networks.</strong> <em>Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming.</em> CVPR 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>The Graph Neural Network Model.</strong> <em>Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele.</em> IEEE TNN 2009. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4700287" target="_blank" rel="noopener">paper</a></li></ol><h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><ol><li><strong>A new model for learning in graph domains.</strong> <em>Marco Gori, Gabriele Monfardini, Franco Scarselli.</em> IJCNN 2005. <a href="https://www.researchgate.net/profile/Franco_Scarselli/publication/4202380_A_new_model_for_earning_in_raph_domains/links/0c9605188cd580504f000000.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Neural Networks for Ranking Web Pages.</strong> <em>Franco Scarselli, Sweah Liang Yong, Marco Gori, Markus Hagenbuchner, Ah Chung Tsoi, Marco Maggini.</em> WI 2005. <a href="https://www.researchgate.net/profile/Franco_Scarselli/publication/221158677_Graph_Neural_Networks_for_Ranking_Web_Pages/links/0c9605188cd5090ede000000/Graph-Neural-Networks-for-Ranking-Web-Pages.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Gated Graph Sequence Neural Networks.</strong> <em>Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel.</em> ICLR 2016. <a href="https://arxiv.org/pdf/1511.05493.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Geometric deep learning on graphs and manifolds using mixture model cnns.</strong> <em>Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Jan Svoboda, Michael M. Bronstein.</em> CVPR 2017. <a href="https://arxiv.org/pdf/1611.08402.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Spectral Networks and Locally Connected Networks on Graphs.</strong> <em>Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun.</em> ICLR 2014. <a href="https://arxiv.org/pdf/1312.6203.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Deep Convolutional Networks on Graph-Structured Data.</strong> <em>Mikael Henaff, Joan Bruna, Yann LeCun.</em> 2015. <a href="https://arxiv.org/pdf/1506.05163.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.</strong> <em>Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst.</em> NIPS 2016. <a href="http://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Convolutional Neural Networks for Graphs.</strong> <em>Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov.</em> ICML 2016. <a href="http://proceedings.mlr.press/v48/niepert16.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Semi-Supervised Classification with Graph Convolutional Networks.</strong> <em>Thomas N. Kipf, Max Welling.</em> ICLR 2017. <a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Attention Networks.</strong> <em>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio.</em> ICLR 2018. <a href="https://mila.quebec/wp-content/uploads/2018/07/d1ac95b60310f43bb5a0b8024522fbe08fb2a482.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Deep Sets.</strong> <em>Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, Alexander Smola.</em>NIPS 2017. <a href="https://arxiv.org/pdf/1703.06114.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Partition Neural Networks for Semi-Supervised Classification.</strong> <em>Renjie Liao, Marc Brockschmidt, Daniel Tarlow, Alexander L. Gaunt, Raquel Urtasun, Richard Zemel.</em> 2018. <a href="https://arxiv.org/pdf/1803.06272.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Covariant Compositional Networks For Learning Graphs.</strong> <em>Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, Shubhendu Trivedi.</em> 2018. <a href="https://arxiv.org/pdf/1801.02144.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Modeling Relational Data with Graph Convolutional Networks.</strong> <em>Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling.</em> ESWC 2018. <a href="https://arxiv.org/pdf/1703.06103.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Stochastic Training of Graph Convolutional Networks with Variance Reduction.</strong> <em>Jianfei Chen, Jun Zhu, Le Song.</em> ICML 2018. <a href="http://www.scipaper.net/uploadfile/2018/0716/20180716100330880.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Steady-States of Iterative Algorithms over Graphs.</strong> <em>Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alex Smola, Le Song.</em>ICML 2018. <a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Deriving Neural Architectures from Sequence and Graph Kernels.</strong> <em>Tao Lei, Wengong Jin, Regina Barzilay, Tommi Jaakkola.</em>ICML 2017. <a href="https://arxiv.org/pdf/1705.09037.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Adaptive Graph Convolutional Neural Networks.</strong> <em>Ruoyu Li, Sheng Wang, Feiyun Zhu, Junzhou Huang.</em> AAAI 2018. <a href="https://arxiv.org/pdf/1801.03226.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph-to-Sequence Learning using Gated Graph Neural Networks.</strong> <em>Daniel Beck, Gholamreza Haffari, Trevor Cohn.</em> ACL 2018. <a href="https://arxiv.org/pdf/1806.09835.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning.</strong> <em>Qimai Li, Zhichao Han, Xiao-Ming Wu.</em> AAAI 2018. <a href="https://arxiv.org/pdf/1801.07606.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graphical-Based Learning Environments for Pattern Recognition.</strong> <em>Franco Scarselli, Ah Chung Tsoi, Marco Gori, Markus Hagenbuchner.</em> SSPR/SPR 2004. <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-540-27868-9_4.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>A Comparison between Recursive Neural Networks and Graph Neural Networks.</strong> <em>Vincenzo Di Massa, Gabriele Monfardini, Lorenzo Sarti, Franco Scarselli, Marco Maggini, Marco Gori.</em> IJCNN 2006. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1716174" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Neural Networks for Object Localization.</strong> <em>Gabriele Monfardini, Vincenzo Di Massa, Franco Scarselli, Marco Gori.</em> ECAI 2006. <a href="http://ebooks.iospress.nl/volumearticle/2775" target="_blank" rel="noopener">paper</a></li><li><strong>Knowledge-Guided Recurrent Neural Network Learning for Task-Oriented Action Prediction.</strong> <em>Liang Lin, Lili Huang, Tianshui Chen, Yukang Gan, Hui Cheng.</em> ICME 2017. <a href="https://arxiv.org/pdf/1707.04677.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Semantic Object Parsing with Graph LSTM.</strong> <em>Xiaodan LiangXiaohui ShenJiashi FengLiang Lin, Shuicheng Yan.</em> ECCV 2016. <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-46448-0_8.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>CelebrityNet: A Social Network Constructed from Large-Scale Online Celebrity Images.</strong> <em>Li-Jia Li, David A. Shamma, Xiangnan Kong, Sina Jafarpour, Roelof Van Zwol, Xuanhui Wang.</em> TOMM 2015. <a href="https://dl.acm.org/ft_gateway.cfm?id=2801125&amp;ftid=1615097&amp;dwn=1&amp;CFID=38275959&amp;CFTOKEN=6938a464cf972252-DF065FDC-9FED-EB68-3528017EA04F0D29" target="_blank" rel="noopener">paper</a></li><li><strong>Inductive Representation Learning on Large Graphs.</strong> <em>William L. Hamilton, Rex Ying, Jure Leskovec.</em> NIPS 2017. <a href="https://arxiv.org/pdf/1706.02216.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Classification using Structural Attention.</strong> <em>John Boaz Lee, Ryan Rossi, Xiangnan Kong.</em> KDD 18. <a href="https://dl.acm.org/ft_gateway.cfm?id=3219980&amp;ftid=1988883&amp;dwn=1&amp;CFID=38275959&amp;CFTOKEN=6938a464cf972252-DF065FDC-9FED-EB68-3528017EA04F0D29" target="_blank" rel="noopener">paper</a></li><li><strong>Adversarial Attacks on Neural Networks for Graph Data.</strong> <em>Daniel Zügner, Amir Akbarnejad, Stephan Günnemann.</em> KDD 18. <a href="http://delivery.acm.org/10.1145/3230000/3220078/p2847-zugner.pdf?ip=101.5.139.169&amp;id=3220078&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E587F3204F5B62A59%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1545706391_e7484be677293ffb5f18b39ce84a0df9" target="_blank" rel="noopener">paper</a></li><li><strong>Large-Scale Learnable Graph Convolutional Networks.</strong> <em>Hongyang Gao, Zhengyang Wang, Shuiwang Ji.</em> KDD 18. <a href="http://delivery.acm.org/10.1145/3220000/3219947/p1416-gao.pdf?ip=101.5.139.169&amp;id=3219947&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E587F3204F5B62A59%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1545706457_bb20316c7ce038aefb97afcf4ef9668b" target="_blank" rel="noopener">paper</a></li><li><strong>Contextual Graph Markov Model: A Deep and Generative Approach to Graph Processing.</strong> <em>Davide Bacciu, Federico Errica, Alessio Micheli.</em> ICML 2018. <a href="https://arxiv.org/pdf/1805.10636.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Diffusion-Convolutional Neural Networks.</strong> <em>James Atwood, Don Towsley.</em> NIPS 2016. <a href="https://arxiv.org/pdf/1511.02136.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Neural networks for relational learning: an experimental comparison.</strong> <em>Werner Uwents, Gabriele Monfardini, Hendrik Blockeel, Marco Gori, Franco Scarselli.</em> Machine Learning 2011. <a href="https://link.springer.com/content/pdf/10.1007%2Fs10994-010-5196-5.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.</strong> <em>Jie Chen, Tengfei Ma, Cao Xiao.</em>ICLR 2018. <a href="https://arxiv.org/pdf/1801.10247.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Adaptive Sampling Towards Fast Graph Representation Learning.</strong> <em>Wenbing Huang, Tong Zhang, Yu Rong, Junzhou Huang.</em>NeurIPS 2018. <a href="https://arxiv.org/pdf/1809.05343.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Structure-Aware Convolutional Neural Networks.</strong> <em>Jianlong Chang, Jie Gu, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, Chunhong Pan.</em> NeurIPS 2018. <a href="http://papers.nips.cc/paper/7287-structure-aware-convolutional-neural-networks.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Bayesian Semi-supervised Learning with Graph Gaussian Processes.</strong> <em>Yin Cheng Ng, Nicolò Colombo, Ricardo Silva.</em>NeurIPS 2018. <a href="https://arxiv.org/pdf/1809.04379" target="_blank" rel="noopener">paper</a></li><li><strong>Mean-field theory of graph neural networks in graph partitioning.</strong> <em>Tatsuro Kawamoto, Masashi Tsubaki, Tomoyuki Obuchi.</em> NeurIPS 2018. <a href="http://papers.nips.cc/paper/7689-mean-field-theory-of-graph-neural-networks-in-graph-partitioning.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Hierarchical Graph Representation Learning with Differentiable Pooling.</strong> <em>Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec.</em> NeurIPS 2018. <a href="https://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>How Powerful are Graph Neural Networks?</strong> <em>Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka.</em> ICLR 2019. <a href="https://openreview.net/pdf?id=ryGs6iA5Km" target="_blank" rel="noopener">paper</a></li></ol><h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ol><li><strong>Discovering objects and their relations from entangled scene representations.</strong> <em>David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, Peter Battaglia.</em> ICLR Workshop 2017. <a href="https://arxiv.org/pdf/1702.05068.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>A simple neural network module for relational reasoning.</strong> <em>Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap.</em> NIPS 2017. <a href="https://arxiv.org/pdf/1706.01427.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Attend, Infer, Repeat: Fast Scene Understanding with Generative Models.</strong> <em>S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, Geoffrey E. Hinton.</em> NIPS 2016. <a href="https://arxiv.org/pdf/1603.08575.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships.</strong> <em>Tomasz Malisiewicz, Alyosha Efros.</em> NIPS 2009. <a href="http://papers.nips.cc/paper/3647-beyond-categories-the-visual-memex-model-for-reasoning-about-object-relationships.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Understanding Kin Relationships in a Photo.</strong> <em>Siyu Xia, Ming Shao, Jiebo Luo, Yun Fu.</em> TMM 2012. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6151163" target="_blank" rel="noopener">paper</a></li><li><strong>Graph-Structured Representations for Visual Question Answering.</strong> <em>Damien Teney, Lingqiao Liu, Anton van den Hengel.</em>CVPR 2017. <a href="https://arxiv.org/pdf/1609.05600.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition.</strong> <em>Sijie Yan, Yuanjun Xiong, Dahua Lin.</em> AAAI 2018. <a href="https://arxiv.org/pdf/1801.07455.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Few-Shot Learning with Graph Neural Networks.</strong> <em>Victor Garcia, Joan Bruna.</em> ICLR 2018. <a href="https://arxiv.org/pdf/1711.04043.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>The More You Know: Using Knowledge Graphs for Image Classification.</strong> <em>Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta.</em> CVPR 2017. <a href="https://arxiv.org/pdf/1612.04844.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs.</strong> <em>Xiaolong Wang, Yufei Ye, Abhinav Gupta.</em> CVPR 2018. <a href="https://arxiv.org/pdf/1803.08035.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Rethinking Knowledge Graph Propagation for Zero-Shot Learning.</strong> <em>Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, Eric P. Xing.</em> 2018. <a href="https://arxiv.org/pdf/1805.11724.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Interaction Networks for Learning about Objects, Relations and Physics.</strong> <em>Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray Kavukcuoglu.</em> NIPS 2016. <a href="https://arxiv.org/pdf/1612.00222.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>A Compositional Object-Based Approach to Learning Physical Dynamics.</strong> <em>Michael B. Chang, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum.</em> ICLR 2017. <a href="https://arxiv.org/pdf/1612.00341.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Visual Interaction Networks: Learning a Physics Simulator from Vide.o</strong> <em>Nicholas Watters, Andrea Tacchetti, Théophane Weber, Razvan Pascanu, Peter Battaglia, Daniel Zoran.</em> NIPS 2017. <a href="http://papers.nips.cc/paper/7040-visual-interaction-networks-learning-a-physics-simulator-from-video.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Relational neural expectation maximization: Unsupervised discovery of objects and their interactions.</strong> <em>Sjoerd van Steenkiste, Michael Chang, Klaus Greff, Jürgen Schmidhuber.</em> ICLR 2018. <a href="https://arxiv.org/pdf/1802.10353.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph networks as learnable physics engines for inference and control.</strong> <em>Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia.</em> ICML 2018. <a href="https://arxiv.org/pdf/1806.01242.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Multiagent Communication with Backpropagation.</strong> <em>Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus.</em> NIPS 2016. <a href="https://arxiv.org/pdf/1605.07736.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>VAIN: Attentional Multi-agent Predictive Modeling.</strong> <em>Yedid Hoshen.</em> NIPS 2017 <a href="https://arxiv.org/pdf/1706.06122.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Neural Relational Inference for Interacting Systems.</strong> <em>Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, Richard Zemel.</em> ICML 2018. <a href="https://arxiv.org/pdf/1802.04687.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Translating Embeddings for Modeling Multi-relational Data.</strong> <em>Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko.</em> NIPS 2013. <a href="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Representation learning for visual-relational knowledge graphs.</strong> <em>Daniel Oñoro-Rubio, Mathias Niepert, Alberto García-Durán, Roberto González, Roberto J. López-Sastre.</em> 2017. <a href="https://arxiv.org/pdf/1709.02314.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Knowledge Transfer for Out-of-Knowledge-Base Entities : A Graph Neural Network Approach.</strong> <em>Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, Yuji Matsumoto.</em> IJCAI 2017. <a href="https://arxiv.org/pdf/1706.05674.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Representation Learning on Graphs with Jumping Knowledge Networks.</strong> <em>Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka.</em> ICML 2018. <a href="https://arxiv.org/pdf/1806.03536.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Multi-Label Zero-Shot Learning with Structured Knowledge Graphs.</strong> <em>Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, Yu-Chiang Frank Wang.</em> CVPR 2018. <a href="https://arxiv.org/pdf/1711.06526.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams.</strong> <em>Daesik Kim, Youngjoon Yoo, Jeesoo Kim, Sangkuk Lee, Nojun Kwak.</em> CVPR 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Dynamic_Graph_Generation_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Deep Reasoning with Knowledge Graph for Social Relationship Understanding.</strong> <em>Zhouxia Wang, Tianshui Chen, Jimmy Ren, Weihao Yu, Hui Cheng, Liang Lin.</em> IJCAI 2018. <a href="https://arxiv.org/pdf/1807.00504.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Constructing Narrative Event Evolutionary Graph for Script Event Prediction.</strong> <em>Zhongyang Li, Xiao Ding, Ting Liu.</em> IJCAI 2018. <a href="https://arxiv.org/pdf/1805.05081.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering.</strong> <em>Daniil Sorokin, Iryna Gurevych.</em> COLING 2018. <a href="https://arxiv.org/pdf/1808.04126.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Convolutional networks on graphs for learning molecular fingerprints.</strong> <em>David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timothy Hirzel, Alán Aspuru-Guzik, Ryan P. Adams.</em> NIPS 2015. <a href="https://arxiv.org/pdf/1509.09292.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Molecular Graph Convolutions: Moving Beyond Fingerprints.</strong> <em>Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick Riley.</em> Journal of computer-aided molecular design 2016. <a href="https://arxiv.org/pdf/1603.00856.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Protein Interface Prediction using Graph Convolutional Networks.</strong> <em>Alex Fout, Jonathon Byrd, Basir Shariat, Asa Ben-Hur.</em>NIPS 2017. <a href="http://papers.nips.cc/paper/7231-protein-interface-prediction-using-graph-convolutional-networks.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting.</strong> <em>Zhiyong Cui, Kristian Henrickson, Ruimin Ke, Yinhai Wang.</em> 2018. <a href="https://arxiv.org/pdf/1802.07007.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting.</strong> <em>Bing Yu, Haoteng Yin, Zhanxing Zhu.</em> IJCAI 2018. <a href="https://arxiv.org/pdf/1709.04875.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Semi-supervised User Geolocation via Graph Convolutional Networks.</strong> <em>Afshin Rahimi, Trevor Cohn, Timothy Baldwin.</em> ACL 2018. <a href="https://arxiv.org/pdf/1804.08049.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Dynamic Graph CNN for Learning on Point Clouds.</strong> <em>Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon.</em> CVPR 2018. <a href="https://arxiv.org/pdf/1801.07829.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.</strong> <em>Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas.</em> CVPR 2018. <a href="https://arxiv.org/pdf/1612.00593.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>3D Graph Neural Networks for RGBD Semantic Segmentation.</strong> <em>Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, Raquel Urtasun.</em> CVPR 2017. <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Qi_3D_Graph_Neural_ICCV_2017_paper.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Iterative Visual Reasoning Beyond Convolutions.</strong> <em>Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta.</em> CVPR 2018. <a href="https://arxiv.org/pdf/1803.11189" target="_blank" rel="noopener">paper</a></li><li><strong>Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs.</strong> <em>Martin Simonovsky, Nikos Komodakis.</em>CVPR 2017. <a href="https://arxiv.org/pdf/1704.02901" target="_blank" rel="noopener">paper</a></li><li><strong>Situation Recognition with Graph Neural Networks.</strong> <em>Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, Sanja Fidler.</em> ICCV 2017. <a href="https://arxiv.org/pdf/1708.04320" target="_blank" rel="noopener">paper</a></li><li><strong>Conversation Modeling on Reddit using a Graph-Structured LSTM.</strong> <em>Vicky Zayats, Mari Ostendorf.</em> TACL 2018. <a href="https://arxiv.org/pdf/1704.02080" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolutional Networks for Text Classification.</strong> <em>Liang Yao, Chengsheng Mao, Yuan Luo.</em> AAAI 2019. <a href="https://arxiv.org/pdf/1809.05679.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Attention Is All You Need.</strong> <em>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.</em> NIPS 2017. <a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener">paper</a></li><li><strong>Self-Attention with Relative Position Representations.</strong> <em>Peter Shaw, Jakob Uszkoreit, Ashish Vaswani.</em> NAACL 2018. <a href="https://arxiv.org/pdf/1803.02155" target="_blank" rel="noopener">paper</a></li><li><strong>Hyperbolic Attention Networks.</strong> <em>Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, Nando de Freitas</em> 2018. <a href="https://arxiv.org/pdf/1805.09786" target="_blank" rel="noopener">paper</a></li><li><strong>Effective Approaches to Attention-based Neural Machine Translation.</strong> <em>Minh-Thang Luong, Hieu Pham, Christopher D. Manning.</em> EMNLP 2015. <a href="https://arxiv.org/pdf/1508.04025" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.</strong> <em>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima’an.</em> EMNLP 2017. <a href="https://arxiv.org/pdf/1704.04675" target="_blank" rel="noopener">paper</a></li><li><strong>NerveNet: Learning Structured Policy with Graph Neural Networks.</strong> <em>Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler.</em>ICLR 2018. <a href="https://openreview.net/pdf?id=S1sqHMZCb" target="_blank" rel="noopener">paper</a></li><li><strong>Metacontrol for Adaptive Imagination-Based Optimization.</strong> <em>Jessica B. Hamrick, Andrew J. Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, Peter W. Battaglia.</em> ICLR 2017. <a href="https://arxiv.org/pdf/1705.02670" target="_blank" rel="noopener">paper</a></li><li><strong>Learning model-based planning from scratch.</strong> <em>Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra, Peter Battaglia.</em> 2017. <a href="https://arxiv.org/pdf/1707.06170" target="_blank" rel="noopener">paper</a></li><li><strong>Structured Dialogue Policy with Graph Neural Networks.</strong> <em>Lu Chen, Bowen Tan, Sishan Long and Kai Yu.</em> ICCL 2018. <a href="http://www.aclweb.org/anthology/C18-1107" target="_blank" rel="noopener">paper</a></li><li><strong>Relational inductive bias for physical construction in humans and machines.</strong> <em>Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R. McKee, Joshua B. Tenenbaum, Peter W. Battaglia.</em> CogSci 2018. <a href="https://arxiv.org/abs/1806.01203" target="_blank" rel="noopener">paper</a></li><li><strong>Relational Deep Reinforcement Learning.</strong> <em>Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, Peter Battaglia.</em> 2018. <a href="https://arxiv.org/abs/1806.01830" target="_blank" rel="noopener">paper</a></li><li><strong>Action Schema Networks: Generalised Policies with Deep Learning.</strong> <em>Sam Toyer, Felipe Trevizan, Sylvie Thiébaux, Lexing Xie.</em> AAAI 2018. <a href="https://arxiv.org/abs/1709.04271" target="_blank" rel="noopener">paper</a></li><li><strong>Neural Combinatorial Optimization with Reinforcement Learning.</strong> <em>Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, Samy Bengio.</em> 2016. <a href="https://arxiv.org/abs/1611.09940" target="_blank" rel="noopener">paper</a></li><li><strong>A Note on Learning Algorithms for Quadratic Assignment with Graph Neural Networks.</strong> <em>Alex Nowak, Soledad Villar, Afonso S. Bandeira, Joan Bruna.</em> PADL 2017. <a href="https://www.padl.ws/papers/Paper%2017.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Combinatorial Optimization Algorithms over Graphs.</strong> <em>Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, Le Song.</em> NIPS 2017. <a href="https://arxiv.org/abs/1704.01665" target="_blank" rel="noopener">paper</a></li><li><strong>Attention Solves Your TSP, Approximately.</strong> <em>Wouter Kool, Herke van Hoof, Max Welling.</em> 2018. <a href="https://arxiv.org/abs/1803.08475" target="_blank" rel="noopener">paper</a></li><li><strong>Learning a SAT Solver from Single-Bit Supervision.</strong> <em>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, David L. Dill.</em> 2018. <a href="https://arxiv.org/abs/1802.03685" target="_blank" rel="noopener">paper</a></li><li><strong>Learning to Represent Programs with Graphs.</strong> <em>Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi.</em> ICLR 2018. <a href="https://arxiv.org/abs/1711.00740" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Graphical State Transitions.</strong> <em>Daniel D. Johnson.</em> ICLR 2017. <a href="https://openreview.net/forum?id=HJ0NvFzxl" target="_blank" rel="noopener">paper</a></li><li><strong>Inference in Probabilistic Graphical Models by Graph Neural Networks.</strong> <em>KiJung Yoon, Renjie Liao, Yuwen Xiong, Lisa Zhang, Ethan Fetaya, Raquel Urtasun, Richard Zemel, Xaq Pitkow.</em> ICLR Workshop 2018. <a href="https://arxiv.org/abs/1803.07710" target="_blank" rel="noopener">paper</a></li><li><strong>Learning deep generative models of graphs.</strong> <em>Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, Peter Battaglia.</em> ICLR Workshop 2018. <a href="https://arxiv.org/abs/1803.03324" target="_blank" rel="noopener">paper</a></li><li><strong>MolGAN: An implicit generative model for small molecular graphs.</strong> <em>Nicola De Cao, Thomas Kipf.</em> 2018. <a href="https://arxiv.org/abs/1805.11973" target="_blank" rel="noopener">paper</a></li><li><strong>GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models.</strong> <em>Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, Jure Leskovec.</em> ICML 2018. <a href="https://arxiv.org/abs/1802.08773" target="_blank" rel="noopener">paper</a></li><li><strong>NetGAN: Generating Graphs via Random Walks.</strong> <em>Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, Stephan Günnemann.</em> ICML 2018. <a href="https://arxiv.org/abs/1803.00816" target="_blank" rel="noopener">paper</a></li><li><strong>Adversarial Attack on Graph Structured Data.</strong> <em>Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song.</em> ICML 2018. <a href="https://arxiv.org/abs/1806.02371" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolutional Neural Networks for Web-Scale Recommender Systems.</strong> <em>Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec.</em> KDD 2018. <a href="https://arxiv.org/abs/1806.01973" target="_blank" rel="noopener">paper</a></li><li><strong>Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.</strong> <em>Kai Sheng Tai, Richard Socher, Christopher D. Manning.</em> ACL 2015. <a href="https://www.aclweb.org/anthology/P15-1150" target="_blank" rel="noopener">paper</a></li><li><strong>Neural Module Networks.</strong> <em>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein.</em> CVPR 2016. <a href="https://arxiv.org/pdf/1511.02799.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.</strong> <em>Diego Marcheggiani, Ivan Titov.</em>EMNLP 2017. <a href="https://arxiv.org/abs/1703.04826" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolutional Networks with Argument-Aware Pooling for Event Detection.</strong> <em>Thien Huu Nguyen, Ralph Grishman.</em>AAAI 2018. <a href="http://ix.cs.uoregon.edu/~thien/pubs/graphConv.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks.</strong> <em>Federico Monti, Michael M. Bronstein, Xavier Bresson.</em> NIPS 2017. <a href="https://arxiv.org/abs/1704.06803" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolutional Matrix Completion.</strong> <em>Rianne van den Berg, Thomas N. Kipf, Max Welling.</em> 2017. <a href="https://arxiv.org/abs/1706.02263" target="_blank" rel="noopener">paper</a></li><li><strong>Hybrid Approach of Relation Network and Localized Graph Convolutional Filtering for Breast Cancer Subtype Classification.</strong> <em>Sungmin Rhee, Seokjun Seo, Sun Kim.</em> IJCAI 2018. <a href="https://arxiv.org/abs/1711.05859" target="_blank" rel="noopener">paper</a></li><li><strong>Modeling polypharmacy side effects with graph convolutional networks.</strong> <em>Marinka Zitnik, Monica Agrawal, Jure Leskovec.</em>ISMB 2018. <a href="https://arxiv.org/abs/1802.00543" target="_blank" rel="noopener">paper</a></li><li><strong>DeepInf: Modeling influence locality in large social networks.</strong> <em>Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, Jie Tang.</em> KDD 2018. <a href="https://arxiv.org/pdf/1807.05560.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks.</strong> <em>Diego Marcheggiani, Joost Bastings, Ivan Titov.</em> NAACL 2018. <a href="http://www.aclweb.org/anthology/N18-2078" target="_blank" rel="noopener">paper</a></li><li><strong>Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks.</strong> <em>Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, Daniel Gildea.</em> 2018. <a href="https://arxiv.org/abs/1809.02040" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.</strong> <em>Yuhao Zhang, Peng Qi, Christopher D. Manning.</em> EMNLP 2018. <a href="https://arxiv.org/abs/1809.10185" target="_blank" rel="noopener">paper</a></li><li><strong>N-ary relation extraction using graph state LSTM.</strong> <em>Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea.</em> EMNLP 18. <a href="https://arxiv.org/abs/1808.09101" target="_blank" rel="noopener">paper</a></li><li><strong>A Graph-to-Sequence Model for AMR-to-Text Generation.</strong> <em>Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea.</em> ACL 2018. <a href="https://arxiv.org/abs/1805.02473" target="_blank" rel="noopener">paper</a></li><li><strong>Cross-Sentence N-ary Relation Extraction with Graph LSTMs.</strong> <em>Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, Wen-tau Yih.</em> TACL. <a href="https://arxiv.org/abs/1708.03743" target="_blank" rel="noopener">paper</a></li><li><strong>Sentence-State LSTM for Text Representation.</strong> <em>Yue Zhang, Qi Liu, Linfeng Song.</em> ACL 2018. <a href="https://arxiv.org/abs/1805.02474" target="_blank" rel="noopener">paper</a></li><li><strong>End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures.</strong> <em>Makoto Miwa, Mohit Bansal.</em> ACL 2016. <a href="https://arxiv.org/abs/1601.00770" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Human-Object Interactions by Graph Parsing Neural Networks.</strong> <em>Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, Song-Chun Zhu.</em> ECCV 2018. <a href="https://arxiv.org/pdf/1808.07962.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Multiple Events Extraction via Attention-based Graph Information Aggregation.</strong> <em>Xiao Liu, Zhunchen Luo, Heyan Huang.</em>EMNLP 2018. <a href="https://arxiv.org/pdf/1809.09078.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks.</strong> <em>Zhichun Wang, Qingsong Lv, Xiaohan Lan, Yu Zhang.</em> EMNLP 2018. <a href="http://www.aclweb.org/anthology/D18-1032" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.</strong> <em>Yuhao Zhang, Peng Qi, Christopher D. Manning.</em> EMNLP 2018. <a href="https://arxiv.org/pdf/1809.10185" target="_blank" rel="noopener">paper</a></li><li><strong>Recurrent Relational Networks.</strong> <em>Rasmus Palm, Ulrich Paquet, Ole Winther.</em> NeurIPS 2018. <a href="http://papers.nips.cc/paper/7597-recurrent-relational-networks.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation.</strong> <em>Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec.</em> NeurIPS 2018. <a href="https://arxiv.org/pdf/1806.02473" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Conditioned Graph Structures for Interpretable Visual Question Answering.</strong> <em>Will Norcliffe-Brown, Efstathios Vafeias, Sarah Parisot.</em> NeurIPS 2018. <a href="https://arxiv.org/pdf/1806.07243" target="_blank" rel="noopener">paper</a></li><li><strong>Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search.</strong> <em>Zhuwen Li, Qifeng Chen, Vladlen Koltun.</em> NeurIPS 2018. <a href="http://papers.nips.cc/paper/7335-combinatorial-optimization-with-graph-convolutional-networks-and-guided-tree-search.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Symbolic Graph Reasoning Meets Convolutions.</strong> <em>Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, Eric P. Xing.</em> NeurIPS 2018. <a href="http://papers.nips.cc/paper/7456-symbolic-graph-reasoning-meets-convolutions.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering.</strong> <em>Medhini Narasimhan, Svetlana Lazebnik, Alexander Schwing.</em> NeurIPS 2018. <a href="http://papers.nips.cc/paper/7531-out-of-the-box-reasoning-with-graph-convolution-nets-for-factual-visual-question-answering.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders.</strong> <em>Tengfei Ma, Jie Chen, Cao Xiao.</em> NeurIPS 2018. <a href="https://papers.nips.cc/paper/7942-constrained-generation-of-semantically-valid-graphs-via-regularizing-variational-autoencoders.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Structural-RNN: Deep Learning on Spatio-Temporal Graphs.</strong> <em>Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena.</em>CVPR 2016. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Jain_Structural-RNN_Deep_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Relation Networks for Object Detection.</strong> <em>Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei.</em> CVPR 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Hu_Relation_Networks_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a></li><li><strong>Learning Region features for Object Detection.</strong> <em>Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai.</em> ECCV 2018. <a href="https://arxiv.org/pdf/1803.07066" target="_blank" rel="noopener">paper</a></li><li><strong>Deep Graph Infomax.</strong> <em>Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm.</em> ICLR 2019. <a href="https://openreview.net/pdf?id=rklz9iAcKQ" target="_blank" rel="noopener">paper</a></li><li><strong>Combining Neural Networks with Personalized PageRank for Classification on Graphs.</strong> <em>Johannes Klicpera, Aleksandar Bojchevski, Stephan Günnemann.</em> ICLR 2019. <a href="https://arxiv.org/pdf/1810.05997.pdf" target="_blank" rel="noopener">paper</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 资料整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记11】TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS Documents</title>
      <link href="/2018/12/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B011%E3%80%91TRACKING%20THE%20WORLD%20STATE%20WITH%20RECURRENT%20ENTITY%20NETWORKS/"/>
      <url>/2018/12/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B011%E3%80%91TRACKING%20THE%20WORLD%20STATE%20WITH%20RECURRENT%20ENTITY%20NETWORKS/</url>
      
        <content type="html"><![CDATA[<p>这个模型也就是前面提到的动态记忆，这篇论文来自ICLR2017，论文比笔记还是参考了北邮的两位大佬的博客，后面给出了原博客地址。<br>论文提出了一种新的动态记忆网络，使用固定长度的记忆单元来存储世界上的实体，每个记忆单元对应一个实体，主要存储该实体相关的属性（如一个人拿了什么东西，在哪里，跟谁等等），并且该记忆会随着输入内容实时更新。多个记忆槽之间相互独立，由（key，value）组成。key用来标识实体，value用来存储实体相关的属性，也就是记忆。</p><h3 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1 模型结构"></a>1 模型结构</h3><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/2.png" alt="模型结构"><br>其架构如图所示，方块代表记忆单元，使用Gated RNN来实现该记忆单元的功能，一层代表一个RNN，也就是一个记忆槽，用来存储一个实体及其相关属性，共m层，且相互独立，但是每层内、各层间各个方块之间的参数共享，保持一致。key对应于w参数，每一层的w不一样，用来标识不同的实体。（w，h）就是记忆单元。<br>下面分别介绍模型的三个主要组件，输入模块、动态记忆模块、输出模块。<br><strong>输入模块</strong>：将句子中的所有单词进行加权得到一个句子的整体语义表示（也就是文章表示法的模型二），公式如下，其中，f是模型需要学习的变量，用来学习句子中各单词的位置信息，并且在时间步上是共享的。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/3.png" alt="输入模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/3-1.png" alt="输入模块"></p><p><strong>动态记忆模块</strong>：主要是四个计算过程。首先，对于每一个记忆槽而言，分别使用前一时刻的记忆 h 和该记忆单元的key w分别与输入s相乘，然后经过激活函数得到一个门控单元g，这里可以理解成计算s中与实体信息和实体属性的相关程度。接着，当输入s后，计算需要更新的内容，需要注意的是UVW这三个参数在所有的记忆单元当中都是共享的。然后，根据门控单元g和h来对记忆进行更新，将新的信息写入记忆之中：最后对新的记忆进行归一化，论文中提到该归一化可以达到遗忘的作用，个人理解为归一化后能存储的信息量减少了，所以有有遗忘的那种效果。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/4.png" alt="动态记忆模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/4-1.png" alt="动态记忆模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/4-2.png" alt="动态记忆模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/4-3.png" alt="动态记忆模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/4-4.png" alt="动态记忆模块"></p><p><strong>输出模块</strong>：经过动态记忆模块后，已经把输入转化为记忆存储在各个记忆槽里面，那么接下来的工作就是根据问题来产生答案。首先要将问题编码成一个向量，使用与输入模块相同的操作（也就是问题表示法模型二），然后将问题的向量表示与每个记忆槽相乘，再经过softmax，这就相当于是一层注意力，用于获得问题和每个记忆槽之间的相关性。接着，使用该注意力权重p作为每个记忆槽的权重进行加权求和，得到所有记忆中关于该问题的相关记忆，作为答案候选。最后经过一个简单的输出层将其编码为最终的答案即可。这个输出层与一个一层的端到端记忆网络的输出层相似，传入问题和记忆，再经过激活函数。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/5-1.png" alt="输出模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/5-2.png" alt="输出模块"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/5-3.png" alt="输出模块"></p><h3 id="2-EntNet模型实例"><a href="#2-EntNet模型实例" class="headerlink" title="2 EntNet模型实例"></a>2 EntNet模型实例</h3><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/6.png" alt="输入第一句话"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/7.png" alt="输入第二句话"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/entnet/8.png" alt="提问"></p><h3 id="3-参考文献"><a href="#3-参考文献" class="headerlink" title="3 参考文献"></a>3 参考文献</h3><p>[1]论文原文<br>[2]参考笔记：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/29817459" target="_blank" rel="noopener">记忆网络之Recurrent Entity Networks</a>（<a href="https://www.zhihu.com/people/liu-he-he-44/activities" target="_blank" rel="noopener">知乎-呜呜哈</a>）</li><li><a href="https://blog.csdn.net/Irving_zhang/article/details/79204426" target="_blank" rel="noopener">记忆网络系列之Recurrent Entity Network</a> （<a href="https://blog.csdn.net/Irving_zhang" target="_blank" rel="noopener">北邮张博</a>）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> 记忆网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记10】Key-Value Memory Networks for Directly Reading Documents</title>
      <link href="/2018/12/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B010%E3%80%91Key-Value%20Memory%20Networks%20for%20Directly%20Reading%20Documents/"/>
      <url>/2018/12/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B010%E3%80%91Key-Value%20Memory%20Networks%20for%20Directly%20Reading%20Documents/</url>
      
        <content type="html"><![CDATA[<p>上个月看了Facebook的记忆网络系列，前面的两篇论文的笔记看完就整理了，后面这几篇就耽误了，最近又看了一遍，于是及时整理，不然又忘了。这篇文章主要参考北邮的两位大佬（<a href="https://blog.csdn.net/Irving_zhang" target="_blank" rel="noopener">北邮张博</a>、<a href="https://www.zhihu.com/people/liu-he-he-44/activities" target="_blank" rel="noopener">知乎-呜呜哈</a>）的文章，这两个大佬是真的厉害Orz，他们的文章我在最后面贴出了链接。为了自己更好的理解，部分地方我进行了细化，也有些地方进行了省略。</p><h3 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1 模型结构"></a>1 模型结构</h3><p>这是来自ACL2016的论文，它修改基本的端到端结构，使其可以更好的存储QA所需的先验知识。其结构如图所示。从上图可以看出key embedding和value embedding两个模块跟端到端记忆网络里面的Input memory和Output memory两个模块是相同的，不过这里记忆是使用key和value来表示。而且每个hop之间有R矩阵对输入进行线性映射。此外，求每个问题时会进行一个key hashing的预处理，从知识源里选择出与之相关的记忆，然后再进行模型的训练。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/1.png" alt="网络结构"><br>这个模型看起来有点复杂，于是我将其进行了分解。</p><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/2.png" alt="Key hashing"><br><strong>Key hashing：</strong>首先根据输入的问题从知识源中检索出与问题相关的事实，检索的方法可以是至少包含一个相同的实体等多种方法。也就是图中下面的绿色部分，相当于一个记忆的子集。这部分工作可以在处理数据的时候进行。然后将其作为模型输入的一部分，训练的时候输入模型即可。而知识源可以是知识图谱，维基百科，或者通过搜索引擎得到的结果。</p><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/3.png" alt="Key addressing"><br><strong>Key addressing：</strong>寻址，也就是对记忆单元进行相关性评分。用key memory与输入的问题相乘之后做softmax得到一个概率分布。概率大小就表明了相关程度。ox和ok都是embedding模型，对问题和key进行编码得到其向量表示。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/3-1.png" alt="Key addressing"></p><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/4.png" alt="Value Reading"><br><strong>Value Reading：</strong>有了相关性评分，接下来就对value memory进行加权求和即可，得到一个输出向量o，计算公式如下。这样就完成了一个hop的操作。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/4-1.png" alt="Value Reading"></p><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/5.png" alt="循环"><br>接下来将输出向量o与输入问题的向量表示q相加，经过矩阵进行映射，再作为下一层的输入，重复循环这个过程即可。</p><p><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/1.png" alt="网络结构"><br>最后，使用一个答案的预测层即可，计算公式如下。yi可以理解为所有知识库中的实体或者候选的答案句子，使用交叉熵作为损失函数即可，然后就可以端到端的对模型进行反向传播训练。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/6.png" alt="输出计算公式"></p><h3 id="2-模型创新点"><a href="#2-模型创新点" class="headerlink" title="2 模型创新点"></a>2 模型创新点</h3><p>整个过程其实跟端到端的模型很相似，最关键的地方就在于Key-Value的记忆如何表示，这也是本模型的创新所在。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/kvmemnn/7.png" alt="KVmemNN和MemN2N记忆对比图"><br>KV记忆网络的好处在于可以很方便的对先验知识进行编码，这样就可以让每个领域的人都方便的将本领域内的一些背景知识编码进记忆中，从而训练自己的问答系统，相比于端到端模型将相同的输入经过不同的矩阵分别编码到input和output模块中，KV模型则选择对输入首先进行一个（key， value）形式的表示，然后再分别编码进入key和value两个记忆模块，于是就有了更多的变化和灵活性。我们可以按照自己的理解对数据进行key索引和value记忆，而不需要完全依赖于模型的embedding矩阵，这样也可以使模型更方便找到相关记亿并产生与答案最相近的输出。<br>论文根据不同形式的输入（也就是不同的知识源），提出了几种不同的编码方式：</p><ul><li>KB Triple：由于知识库是以三元组的形式构成，于是很方便就可以表示为（key， value）的形式。首先将知识库复制一份，并将其中一份进行反转，也就是将主谓宾反过来变成宾谓主，这样使得有些问题可以很方便的被回答。然后将每个三元组的主+谓作为key，宾作为value即可（可以想象，一般的问题都是主语干了什么事之类的，所以主+谓作为key可以更好的跟问题进行匹配，而宾语往往是答案，所以这样的组合很合理）</li><li>Sentence Level：以句子为单位进行存储的时候，与端到端的模型一样，都直接将句子的向量表示存入记忆槽即可，也就是key和value一样。</li><li>Window Level：主要以窗口长度W对原始文章进行切分（只取以实体作为窗口中心的样本），然后将整个窗口的向量表示作为key，该实体作为value。因为整个窗口的模式与问题模式更像，所以用它来对问题相关性评分可以找到最相关的记忆，而答案往往就是实体，所以将窗口中心的实体作为value对生成答案更方便。</li><li>Window + Title：该方式使用窗口的向量表示作为key，使用文章的标题title作为value（知识源是多篇文章的情况）。比如对于电影信息而言，文章标题往往就是电影名称，所以更贴近问题的答案。</li></ul><h3 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3 参考资料"></a>3 参考资料</h3><p>[1]论文原文<br>[2]参考笔记：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/29817459" target="_blank" rel="noopener">记忆网络之Key-Value Memory Networks</a>（<a href="https://www.zhihu.com/people/liu-he-he-44/activities" target="_blank" rel="noopener">知乎-呜呜哈</a>）</li><li><a href="https://zhuanlan.zhihu.com/p/29941178" target="_blank" rel="noopener">记忆网络之Key-Value Memory Networks tensorflow实现</a>（<a href="https://www.zhihu.com/people/liu-he-he-44/activities" target="_blank" rel="noopener">知乎-呜呜哈</a>）</li><li><a href="https://blog.csdn.net/irving_zhang/article/details/79174025" target="_blank" rel="noopener">记忆网络系列之Key Value Memory Network</a> （<a href="https://blog.csdn.net/Irving_zhang" target="_blank" rel="noopener">北邮张博</a>）</li></ul><p>[3]参考代码：<a href="https://github.com/lc222/key-value-MemNN" target="_blank" rel="noopener">lc222</a>、<a href="https://github.com/siyuanzhao/key-value-memory-networks" target="_blank" rel="noopener">siyuanzhao</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> 记忆网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记09】Teaching Machines to Read and Comprehend</title>
      <link href="/2018/11/19/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B009%E3%80%91Teaching%20Machines%20to%20Read%20and%20Comprehend/"/>
      <url>/2018/11/19/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B009%E3%80%91Teaching%20Machines%20to%20Read%20and%20Comprehend/</url>
      
        <content type="html"><![CDATA[<p>本文主要做出了两个大的贡献：</p><ul><li>给出了阅读理解数据集的构造方法，并开源了两个阅读理解数据集；</li><li>提出了三种神经网络模型作为baseline，以方便后面的研究者进行相关的研究。</li></ul><h3 id="1-数据构造"><a href="#1-数据构造" class="headerlink" title="1 数据构造"></a>1 数据构造</h3><p>主要是从新闻网站中抓取新闻作为文章，新闻的摘要去掉一个实体词成为query，被去掉的单词作为答案。为了防止模型对训练数据的过度依赖，于是将文章和问题中的实体进行匿名化和随机替换。<a href="https://github.com/deepmind/rc-data" target="_blank" rel="noopener">具体见官方教程</a></p><h3 id="2-三种神经网络模型"><a href="#2-三种神经网络模型" class="headerlink" title="2 三种神经网络模型"></a>2 三种神经网络模型</h3><p>（1） Deep LSTM<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/TMRC/1.jpg" alt="Deep LSTM"><br>从上图的结构可以看到，主要是将文章和问题进行拼接（doc|||query 或者 query||| doc）,实际上都是看成一个长文本，通过多层LSTM编码，得到最后的隐藏层状态，进而进行后面的任务。<br>（2）Attentive Reader<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/TMRC/2.jpg" alt="Attentive Reader"><br>这个模型将document和query分开表示，其中query部分采用了问题表示法的模型三，就是用了一个双向LSTM来encode，然后将两个方向上的last hidden state拼接作为query的表示；document这部分采用文章表示法的模型二，也就是通过单词语义向量表示文章整体语义，即用一个双向的LSTM来对文章进行编码，每个token的表示是用两个方向上的hidden state拼接而成，document的表示则是用document中所有token的加权平均来表示，这里的权重就是attention，权重越大表示回答query时对应的token的越重要。最后利用g函数来处理文章语义和问题语义。<br>（3）Impatient Reader<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/TMRC/3.jpg" alt="Impatient Reader"><br>这个模型在Attentive Reader模型的基础上更细了一步，即每个query token都与document tokens有关联，而不是像之前的模型将整个query考虑为整体。具体来说，文章还是采用的文章表示法模型二，而query中每处理一个单词后，都用问题表示法的模型三来对当前问题序列进行整体语义表示，这个过程就好像是你读query中的每个token都需要找到document中对应相关的token。</p><h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3 小结"></a>3 小结</h3><p>这篇论文作为阅读理解任务上的经典论文，对这个领域后来的发展推进作用挺大的。这两个数据集目前是这个领域常用的数据集；三种神经网络模型也是后面提出来的哪些模型的基础。另外，张俊林博士总结的常用文章和问题表示法真的非常清晰，已看网络结构就大概知道用的什么表示方法了。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>【1】<a href="https://blog.csdn.net/malefactor/article/details/52832134" target="_blank" rel="noopener">深度学习解决机器阅读理解任务的研究进展 from 张俊林</a><br>【2】<a href="https://zhuanlan.zhihu.com/p/21343662?refer=paperweekly" target="_blank" rel="noopener">Teaching Machines to Read and Comprehend from paperweekly</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记07】End-To-End Memory Networks</title>
      <link href="/2018/11/17/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B007%E3%80%91End-To-End%20Memory%20Networks/"/>
      <url>/2018/11/17/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B007%E3%80%91End-To-End%20Memory%20Networks/</url>
      
        <content type="html"><![CDATA[<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h3><p>（1）在记忆网络中，主要由4个模块组成：I、G、O、R，前面也提到I和G模块其实并没有进行多复杂的操作，只是将原始文本进行向量表示后直接存储在记忆槽中。而主要工作集中在O和R模块，O用来选择与问题相关的记忆，R用来回答，而这两部分都需要监督，也就是需要知道O模块中选择的记忆是否正确，R生成的答案是否正确，这种模型多处需要监督，而且不太容易使用常见的BP算法进行训练，这就限制了模型的推广。<br>（2）而这篇论文就是在前一篇Memory networks的基础上提出来的，针对其中出现的问题，采用了端到端的模型结构(End-to-End)，因此模型需要的监督更少，能够更好的训练模型。</p><h3 id="2-具体内容"><a href="#2-具体内容" class="headerlink" title="2 具体内容"></a>2 具体内容</h3><p>论文中提出了单层和多层两种结构，多层就是将单层网络进行堆叠起来的。<br>（1）单层结构<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/1.jpg" alt="单层结构"><br>如图所示，模型主要包含了A、B、C、W四个矩阵,其中，B是用来对问题进行向量编码，W是最终输出的权重矩阵，而A和C都是将输入的文本编码成词向量，然后分别存入Input和Output模块，这两个模块的值是对应的。<br>其具体内容如下：<br><strong>输入模块：</strong>输入模块主要是对文本进行编码，这里采用了文章表示法模型二的思想（具体见张俊林博士关于<a href="https://blog.csdn.net/malefactor/article/details/52832134" target="_blank" rel="noopener">深度学习解决机器阅读理解任务研究进展</a>的相关介绍），用一个向量来表示句子的整体语义信息（根据一句话中各单词的词向量，将其压缩成一个句向量）。论文中提出了两种编码方式，bow和位置编码。bow就是直接将一个句子中所有单词的词向量求和表示成一个向量的形式（表示法模型2中权重系数为1的情况），这种表示方法最明显的缺点就是不能捕获一句话中单词之间的位置信息。对于位置编码的方法，给不同位置的单词设置不同的权重（即模型二中的权重系数是由该单词所在位置决定的，而不全部是1），然后对各个单词的词向量按照不同的权重加权求和得到句子的整体表示，位置编码方法的公式如下：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/2.jpg" alt="位置编码方式"><br>此外为了引入时序信息，在上面mi的基础上又加上了每句话出现顺序的矩阵，所以最后每句话对应的记忆mi的表达式如下：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/3.jpg" alt="记忆单元Mi"><br><strong>记忆模块：</strong>主要由两部分组成，input和output模块，统称为记忆槽，矩阵A和C都是将输入的文本编码成词向量，然后分别存入Input和Output模块。<br><strong>输出模块：</strong>输入的文本信息通过编码成向量保存在记忆槽（input和outpu）中，input用来和question编码得到的向量相乘得到每句话和问题的相关性（这里可以看成是注意力权重），output模块则是通过相关性进行加权求和得到输出向量o。</p><ul><li><strong>step1:</strong>将问题经过输入模块编码成一个向量u，然后将其与每个mi点积得到两个向量之间的相似度，再通过一个softmax函数进行归一化处理：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/4.jpg" alt="相似度计算"></li><li><strong> step2:</strong>通过上述的相关性指标，对output中的各个记忆ci按照pi进行加权求和得到模型的输出向量o，公式如下：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/5.jpg" alt="输出向量o计算公式"></li></ul><p><strong>Response模块：</strong>该模块主要根据输入信息产生最终的答案，具体是其结合o和q两个向量的和与W相乘再经过一个softmax函数产生各个单词是答案的概率，使用交叉熵损失函数作为目标函数进行训练。<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/9.jpg" alt="预测答案"></p><p>（2）多层结构<br>多层结构其实就是将多个单层模型堆叠起来，其结构图如下：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/6.jpg" alt="多层结构"><br>整体来说还是比较好理解的，上面几层的问题输入就是下层o与u的和，公式为：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/7.jpg" alt="上层问题输入"><br>至于对文本语义表示中的矩阵Ai和Ci，为了减少参数量，论文中提出了两种方法：</p><ul><li>Adjacent：这种方法让相邻层之间的A=C。也就是说Ak+1=Ck，此外W等于顶层的C，B等于底层的A，这样就减少了一半的参数量。</li><li>Layer-wise（RNN-like)：与RNN相似，采用完全共享参数的方法，即各层之间参数均相等。Ak=…=A2=A1,Ck=…=C2=C1。由于这样会大大的减少参数量导致模型效果变差，所以提出一种改进方法，即令uk+1=Huk+ok，也就是在每一层之间加一个线性映射矩阵H。</li></ul><p>对于最后的输出，其计算公式如下：<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/E2Ememnn/8.jpg" alt="预测答案"></p><h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3 小结"></a>3 小结</h3><p>（1）看了最开始的Memory network那篇论文，虽然还是能够看懂，但是至于怎么将其很好的用起来，还是很模糊，于是这篇端到端的记忆网络就提供了很好的使用范例，论文中也设计了两个实验任务（问答和语言模型），看了实验代码清楚多了。<br>（2）其实在这个结构上还是有很多改进的地方，当初看这篇论文的时候也想到一些，比如在记忆模块引入混沌概念，动态表示记忆。后面才发现这一系列的最后一篇论文中就是动态记忆-_-||</p><h3 id="资料来源"><a href="#资料来源" class="headerlink" title="资料来源"></a>资料来源</h3><p>（1）<a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">论文地址</a><br>（2）<a href="https://github.com/domluna/memn2n" target="_blank" rel="noopener">论文代码</a><br>（3）<a href="https://zhuanlan.zhihu.com/p/29679742" target="_blank" rel="noopener">参考笔记</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> 记忆网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记08】Dynamic Entity Representation with Max-pooling Improves Machine Reading</title>
      <link href="/2018/11/17/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B008%E3%80%91Dynamic%20Entity%20Representation%20with%20Max-pooling%20Improves%20Machine%20Reading/"/>
      <url>/2018/11/17/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B008%E3%80%91Dynamic%20Entity%20Representation%20with%20Max-pooling%20Improves%20Machine%20Reading/</url>
      
        <content type="html"><![CDATA[<p>本文模型之前的模型都是用一个静态的向量来表示一个entity，与上下文没有关系。而本文最大的贡献在于提出了一种动态表示entity的模型，根据不同的上下文对同样的entity有不同的表示。<br>模型还是采用双向LSTM来构建，这时实体表示由四部分构成，包括两个方向上的隐层状态，，以及该实体所在句子的最后隐层状态，也就是该实体所在的上下文表示。如图所示。<img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/DER/1.png" alt="实体标识方法"><br>问题向量的计算与动态实体计算过程类似，这里需要填空的地方用占位符表示。<br>如果遇到一个entity在document中出现多次的情况，该entity就会有不同的表示，论文中采用CNN中常用的max-pooling方法，从各个表示中的每个维度获取最大的那一个组成该实体的最终表示，这个表示包括了该实体在文章中各种上下文情况下的信息，具有最全面的信息，<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/DER/2.png" alt="动态实体标识"><br>计算出实体的动态表示之后，通过注意力机制计算得到问题与每个实体之间的权重，根据语义相近程度选出最可能是答案的那个实体，找到最终的答案。<br>这里有一个maxpooling的例子，左边是示意图，右边是对应的解释。大家可以先看一下，主要是说明maxpooling的作用。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/DER/3.png" alt="实验分析"><br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/DER/4.png" alt="实验分析"><br>本文的实验在CNN数据上对模型进行了对比，效果比之前的Attentive Reader好很多，验证了本文的有效性。<br><img src="https://raw.githubusercontent.com/IrvingBei/hexo_photo/master/DER/5.png" alt="实验结果对比"><br>本文模型的一个好玩之处在于用了一种变化的眼光和态度来审视每一个实体，不同的context会给同样的entity带来不同的意义，因此用一种动态的表示方法来捕捉原文中entity最准确的意思，才能更好地理解原文，找出正确答案。实际生活中，我们做阅读理解的时候，最简单的方法是从问题中找到关键词，接着从原文中找到同样的词所在的句子，然后仔细理解这个句子最终得到答案，本文的动态表示正是有意在更加复杂的阅读理解题目上做文章，是一个非常好的探索。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记06】Memory Network</title>
      <link href="/2018/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B006%E3%80%91Memory%20Network/"/>
      <url>/2018/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B006%E3%80%91Memory%20Network/</url>
      
        <content type="html"><![CDATA[<h3 id="1-问题和解决办法"><a href="#1-问题和解决办法" class="headerlink" title="1  问题和解决办法"></a>1  问题和解决办法</h3><p>（1）问题</p><ul><li>当遇到有若干个句子并且句子之间有联系的时候,RNN和LSTM就不能很好地解决；</li><li>对于句子间的这种长期依赖，于是需要从记忆中提取信息；</li></ul><p>（2）解决办法</p><ul><li>本文提出了实现长期记忆的框架，实现了如何从长期记忆中读取和写入，此外还加入了推理功能；</li><li>在QA问题中，长期记忆是很重要的，充当知识库的作用，从其中获取长期记忆来回答问题。<h3 id="2-模型框架"><a href="#2-模型框架" class="headerlink" title="2  模型框架"></a>2  模型框架</h3>（1）模型由4个模块组成，分别是I、G、O、R<br>（2）各模块的作用分别是:<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/memnn/m1.jpg" alt="各个模块的作用"><br>（3）模型结构<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/memnn/m2.jpg" alt="模型结构"><br>（4）基本模型的具体流程</li><li>对于原始文本，将其送入模块I，主要是将原始文本转化为词向量的形式，将其作为模块G的输入；</li><li>在模块G中，直接将输入的向量存储在memory数组的下一个位置，不做其他操作，也就是直接写入新的记忆，对老的记忆不做修改。（在复杂的模型中，还会考虑到记忆的遗忘、记忆单元的替换等因素）</li><li>对于输入的问题（输入的问题也需要通过模块I进行向量化），O模块会在所有的记忆中选择出与问题topk相关的记忆，选择出这topk个记忆后，将其作为R模块的输入；<br>a.在给定问题输入的情况下，选择最相关的记忆：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/memnn/m3.jpg" alt="选择最相关的记忆"><br>b.在给定问题输入和最相关记忆的条件下选择次相关的记忆：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/memnn/m4.png" alt="选择次相关的记忆"><br>c.对于上面这个等式，如果x和o1采用的都是线性的向量表示（BOW等），则可以拆分成下面加和的方式，否则不可以。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/memnn/m5.jpg" alt="向量相加"><br>d.对于评分函数，需满足：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/memnn/m7.jpg" alt="模型结构"></li><li>在R模块中，使用与上面相同的评分函数计算所有候选词与R输入（前一个模块选择出的最相关的K个记忆）的相关性，得分最高的词语就作为正确答案输出。</li></ul><h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3  小结"></a>3  小结</h3><p>这篇论文的重点在于他提出了一种普适性的模型架构（Memory Network），但是很多部分并没有做得很完善，框架中的每一个模块都是可以变更的，这样可以适应不同的应用。后面有几篇基于这个架构的论文，我先看了来继续完善</p><h3 id="资料来源"><a href="#资料来源" class="headerlink" title="资料来源"></a>资料来源</h3><p>【1】 <a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">论文地址</a><br>【2】 <a href="https://zhuanlan.zhihu.com/p/29590286" target="_blank" rel="noopener">论文笔记参考1</a>（这要参考这篇笔记）<br>【3】<a href="https://blog.csdn.net/xizero00/article/details/51181948" target="_blank" rel="noopener">论文笔记参考2</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> 记忆网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记05】WORDS OR CHARACTERS? FINE-GRAINED GATING FOR READING COMPREHENSION</title>
      <link href="/2018/10/25/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B005%E3%80%91WORDS%20OR%20CHARACTERS_%20FINE-GRAINED%20GATING%20FOR%20READING%20COMPREHENSION/"/>
      <url>/2018/10/25/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B005%E3%80%91WORDS%20OR%20CHARACTERS_%20FINE-GRAINED%20GATING%20FOR%20READING%20COMPREHENSION/</url>
      
        <content type="html"><![CDATA[<h3 id="1-问题和数据集"><a href="#1-问题和数据集" class="headerlink" title="1  问题和数据集"></a>1  问题和数据集</h3><h4 id="1-1-问题"><a href="#1-1-问题" class="headerlink" title="1.1  问题"></a>1.1  问题</h4><p> 这是一篇以阅读理解为任务的文章，但在具体处理这个任务时，主要解决数据特征等的表示问题。在提取文本特征时，通常只对单词做词嵌入，而忽略了字符级的特征。</p><h4 id="1-2-数据集"><a href="#1-2-数据集" class="headerlink" title="1.2  数据集"></a>1.2  数据集</h4><ul><li>CBT</li><li>WDW</li><li>SQuAD</li></ul><h3 id="2-目前已有方法"><a href="#2-目前已有方法" class="headerlink" title="2  目前已有方法"></a>2  目前已有方法</h3><h4 id="2-1-单词级表示"><a href="#2-1-单词级表示" class="headerlink" title="2.1  单词级表示"></a>2.1  单词级表示</h4><p>（1）from a lookup table<br>（2）每个单词用一个向量表示<br>（3） 擅长表示单词的语义</p><h4 id="2-2-字符级表示"><a href="#2-2-字符级表示" class="headerlink" title="2.2  字符级表示"></a>2.2  字符级表示</h4><p>（1） 在单词的字符序列上运用RNN或者CNN，隐层状态合并来形成字符表示<br>（2）更适合子词形态建模<br>（3）可以减轻模型的OOV问题</p><h4 id="2-3-单词级和字符级结合"><a href="#2-3-单词级和字符级结合" class="headerlink" title="2.3  单词级和字符级结合"></a>2.3  单词级和字符级结合</h4><p>（1）组合例子</p><ul><li>C2W model based on bidirectional LSTMs</li><li><p>gate units（Miyamoto &amp; cho等提出的标量门控条件实际上并未提高性能）</p><p>（2）面临问题</p></li><li>对于频繁单词，可以准确估计，加入字符级表示可以产生干扰；</li><li>对于非频繁词，加入字符级会带来负面影响</li></ul><h3 id="3-本文提出的方法"><a href="#3-本文提出的方法" class="headerlink" title="3  本文提出的方法"></a>3  本文提出的方法</h3><p>本文提出了一个细粒度门控机制来合并单词级和字符级表示</p><h3 id="4-具体内容"><a href="#4-具体内容" class="headerlink" title="4  具体内容"></a>4  具体内容</h3><h4 id="4-1-特征融合"><a href="#4-1-特征融合" class="headerlink" title="4.1  特征融合"></a>4.1  特征融合</h4><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/w1.png" alt="特征融合示意图"><br> step1：对于单词特征，先做one-hot编码，再做word-embedding作为单词级表示（记为hp）<br>step2：对于字符特征，对单词里的每个字符做one-hot，再送人RNN，得到最后隐层向量来作为字符级表示（记为hq）<br> step3：利用NER,POS,Frequency特征等拼接而得到v，用来计算门控单元g。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/word2.png" alt="门控g的计算"></p><p> step4：将两个特征用门控机制来融合<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/word3.png" alt="特征融合"></p><h4 id="4-2-检索答案"><a href="#4-2-检索答案" class="headerlink" title="4.2  检索答案"></a>4.2  检索答案</h4><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/w2.png" alt="检索答案示意图"><br> step1：计算Iij，于是qj可以看成是过滤Pi中的信息<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/word4.png" alt="计算Iij"><br> step2：计算hi，相当于在Iij上运用一个注意力机制，从而得到输出hi。（wi和wj分别是pi和qj的one-hot编码，原因是加强匹配，当k的值非常大时，这样的信息并不是完全保留。）<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/word5.png" alt="计算Hi"></p><h3 id="5-小结"><a href="#5-小结" class="headerlink" title="5  小结"></a>5  小结</h3><p>（1）.这是一个新的文本特征表示方法，当g值较大时，更多的信息流来自字符级，当g的信息较小时，更多的信息流来自单词级；<br>（2）本文使用的细粒度的门控机制，采用的是向量门而不是标量门；并且根据特征来设置门，能够更好的反映单词的属性</p><h3 id="论文思维导图"><a href="#论文思维导图" class="headerlink" title="论文思维导图"></a>论文思维导图</h3><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/word_char/fg.png" alt="思维导图"></p><h3 id="资料来源"><a href="#资料来源" class="headerlink" title="资料来源"></a>资料来源</h3><h4 id="论文代码"><a href="#论文代码" class="headerlink" title="论文代码"></a>论文代码</h4><h4 id="https-github-com-kimiyoung-fg-gating"><a href="#https-github-com-kimiyoung-fg-gating" class="headerlink" title="https://github.com/kimiyoung/fg-gating"></a><a href="https://github.com/kimiyoung/fg-gating" target="_blank" rel="noopener">https://github.com/kimiyoung/fg-gating</a></h4><h4 id="参考论文笔记"><a href="#参考论文笔记" class="headerlink" title="参考论文笔记"></a>参考论文笔记</h4><h4 id="https-zhuanlan-zhihu-com-p-43554818"><a href="#https-zhuanlan-zhihu-com-p-43554818" class="headerlink" title="https://zhuanlan.zhihu.com/p/43554818"></a><a href="https://zhuanlan.zhihu.com/p/43554818" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/43554818</a></h4>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
      <link href="/2018/10/22/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B004%E3%80%91TriviaQA_A%20Large%20Scale%20Distantly%20Supervised%20Challenge%20Dataset%20for%20RC/"/>
      <url>/2018/10/22/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B004%E3%80%91TriviaQA_A%20Large%20Scale%20Distantly%20Supervised%20Challenge%20Dataset%20for%20RC/</url>
      
        <content type="html"><![CDATA[<h3 id="1-论文主要内容"><a href="#1-论文主要内容" class="headerlink" title="1  论文主要内容"></a>1  论文主要内容</h3><ul><li>本文是一篇资源论文，主要发布了用于检索式问答或阅读理解的数据集Trivia QA；</li><li>对该数据集的质量和数量进行了分析，并创建了baseline，用于具体评估数据集的质量。</li></ul><h3 id="2-Trivia-QA数据集的特点"><a href="#2-Trivia-QA数据集的特点" class="headerlink" title="2  Trivia QA数据集的特点"></a>2  Trivia QA数据集的特点</h3><ul><li>问题比较复杂</li><li>在问题和相应的答案句子中有大量的句法或词汇变化</li><li>需要更多的跨句推理来得到答案</li></ul><h3 id="3-本文的主要贡献"><a href="#3-本文的主要贡献" class="headerlink" title="3  本文的主要贡献"></a>3  本文的主要贡献</h3><ul><li>发布了一个阅读理解数据集</li><li>提出了分析量化数据集质量的方法和在解决这项任务时遇到的问题</li><li>提供了两个baseline，证明了Trivia QA数据集有难度，问题不容易回答，值得未来的研究</li><li>提供了一个小规模的干净的问答数据集</li></ul><h3 id="4-个人小结"><a href="#4-个人小结" class="headerlink" title="4  个人小结"></a>4  个人小结</h3><p>（1）提到数据集，貌似目前的问答数据集大多数都是英文的，问答的形式都比较简单，比如常见的填空式问答数据集，并且问答知识通常仅限于某一个特殊的领域，这种简单形式的特定领域的问答是在简化现实问答的条件下进行的，比较适合问答任务初期研究；针对后期的深入研究，Trivia QA这一类复杂的数据集或许更值得关注，慢慢去掉假设条件，使问答更接近于人类的问答。<br>（2）英文类的问答数据集提出了不少，但是针对中文类的数据集，屈指可数，目前有哈工大和科大讯飞发布的问答数据集、百度机器阅读理解竞赛中的问答数据集等数据集，今年京东举办的第一届人机对话竞赛发布的关于客服和用户之间对话的数据集经过整理勉强可以看成是一个可以用的阅读理解数据集。我平时很少读到资源类的文章，读这篇文章时在想，针对中文问答数据集较少的现象，设想可不可以这样构造一个阅读理解数据集：我们从小学、初中、高中，一直大学，语文试卷中总是有阅读理解题，于是可以考虑将这些文章，每个文章对应的问题及其答案收集起来组成一个阅读理解数据集。这样的数据集涉及到的背景知识不多（可能部分问题会考虑作者的背景、生活年代啥的），但是大部分问题都需要经过推理才能够回答的，需要真正的对文章的篇章结构，语义信息等内容进行理解后才能回答的。这种类别的问答数据集是否可以作为上述的两类数据集之间的过渡数据集。或许可以试一下！或许这一类问题还是太难了！（手动捂脸</p><p>笔记思维导图：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/TriviaQA/triviaqa.png" alt="笔记思维导图"></p><h3 id="资料来源"><a href="#资料来源" class="headerlink" title="资料来源"></a>资料来源</h3><p><a href="http://nlp.cs.%20washington.edu/triviaqa/" target="_blank" rel="noopener">论文源代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）</title>
      <link href="/2018/10/15/tensorflow%E4%B8%ADsequence_loss_by_example()%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%EF%BC%88%E7%BB%93%E5%90%88TF%E7%9A%84ptb%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BE%8B%E5%AD%90%EF%BC%89/"/>
      <url>/2018/10/15/tensorflow%E4%B8%ADsequence_loss_by_example()%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%EF%BC%88%E7%BB%93%E5%90%88TF%E7%9A%84ptb%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BE%8B%E5%AD%90%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>注：由于tensorflow版本的不同，这个函数所在的模块可能不同，如：tf.nn.seq2seq.sequence_loss_by_example和tf.contrib.legacy_seq2seq.sequence_loss_by_example</p></blockquote><p>在正式进入sequence_loss_by_example（）函数的计算过程之前，需要先复习下两个基本的知识点，softmax的计算和交叉熵的计算。</p><h3 id="1-softmax的计算过程"><a href="#1-softmax的计算过程" class="headerlink" title="1  softmax的计算过程"></a>1  softmax的计算过程</h3><p>可以直接网上已经写好的博客：<a href="https://blog.csdn.net/red_stone1/article/details/80687921" target="_blank" rel="noopener">三分钟带你对 Softmax 划重点</a>，这篇文章中有举具体的例子，最好自己动手算一下，不自己动手计算，往往看了就忘了。</p><h3 id="2-交叉熵的计算过程"><a href="#2-交叉熵的计算过程" class="headerlink" title="2  交叉熵的计算过程"></a>2  交叉熵的计算过程</h3><p>交叉熵网上的文章也很多，<a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="noopener">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉</a>这篇文章讲得非常详细，还举了各种例子。</p><p>以上复习了softmax和交叉熵的计算过程，为啥要使用softmax和交叉熵，可以自行网上搜搜。接下来就进入sequence_loss_by_example（）函数的计算过程。</p><h3 id="3-sequence-loss-by-example（）函数的计算过程（以TF的ptb构建语言模型例子为例）"><a href="#3-sequence-loss-by-example（）函数的计算过程（以TF的ptb构建语言模型例子为例）" class="headerlink" title="3  sequence_loss_by_example（）函数的计算过程（以TF的ptb构建语言模型例子为例）"></a>3  sequence_loss_by_example（）函数的计算过程（以TF的ptb构建语言模型例子为例）</h3><blockquote><p>注：例子中的batch_size=20，num_steps=20，为了更直观的查看各个数据的维度，我将num_steps改为了15.（因为本例是通过上一个词预测下一个词，其实num_steps改为多少并没有影响）。</p></blockquote><h4 id="（1）LSTM的输出"><a href="#（1）LSTM的输出" class="headerlink" title="（1）LSTM的输出"></a>（1）LSTM的输出</h4><p>LSTM的隐藏层的单元个数为200，因此，LSTM每一步的输出数据的维度为（batch_size,hidden_size）。有因为LSTM展开的时间步数为num_steps，于是通过</p><pre><code>outputs.append(cell_output)</code></pre><p>将每一时刻的输出都收集起来，这样，最后的outputs是一个list，其样式为：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/1.png" alt="LSTM的全部输出"><br>图中黄色的部分表示同一个序列在LSTM不同时刻的输出。<br>紧接着对outputs进行拼接和reshape，其过程如下图：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/2.png" alt="拼接输出"><br>将每一时刻的输出在第1维上拼接（上图），这样每一行就完整的表示了一个序列。reshape后的结构如下图：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/3.png" alt="outputs的reshape结果"><br>其中每一种颜色表示一个序列，同一种颜色中的各个块表示这个序列的不同时刻。<br>以上就是LSTM的输出，并对其适当变形。接下来通过一个全连接层，将每一时刻的输出映射成字典大小。</p><h4 id="（2）通过全连接层"><a href="#（2）通过全连接层" class="headerlink" title="（2）通过全连接层"></a>（2）通过全连接层</h4><p>这部分就是常见的y=wx+b的构造形式，通过以下代码实现：</p><pre><code>softmax_w = tf.get_variable( &quot;softmax_w&quot;, [size, vocab_size], dtype=tf.float32)softmax_b = tf.get_variable(&quot;softmax_b&quot;, [vocab_size], dtype=tf.float32) # 网络的最后输出(相当于最后添加了一个全连接层)logits = tf.matmul(output, softmax_w) + softmax_b   # logits shape:batch_size*num_step,vocab_size</code></pre><p>通过全连接层后，得到logits，其维度为（batch_size<em>num_step,vocab_size），在本例中就是300</em>10000（本例的词汇表大小是10000）。</p><h4 id="（3）执行tf-contrib-legacy-seq2seq-sequence-loss-by-example函数"><a href="#（3）执行tf-contrib-legacy-seq2seq-sequence-loss-by-example函数" class="headerlink" title="（3）执行tf.contrib.legacy_seq2seq.sequence_loss_by_example函数"></a>（3）执行tf.contrib.legacy_seq2seq.sequence_loss_by_example函数</h4><p>关于这个函数的定义，解释之类的，可以参考<a href="https://blog.csdn.net/appleml/article/details/54017873" target="_blank" rel="noopener">这个解释</a>，小例子可以参考<a href="https://blog.csdn.net/UESTC_C2_403/article/details/72792889" target="_blank" rel="noopener">这个</a>。前一篇博客的解释看得有些稀里糊涂的，后面就找了后面那个例子来跑跑，但是这两个都没有讲清楚内部是怎么计算的，后面我又参考了tensorflow的<a href="https://blog.csdn.net/u012436149/article/details/52874718" target="_blank" rel="noopener">损失函数源代码</a>，找到这个函数，可以看到这个函数在内部调用的是sparse_softmax_cross_entropy_with_logits()函数，好嘛，接下来就一步一步的来看整个计算过程。<br><strong>首先</strong>来看输入的数据： logits和targets</p><ul><li>logits数据的格式在前面已经介绍了，为（300<em>10000）的矩阵，300为num_steps</em>batch_size得到，10000为词汇表大小，这个数据表示的意思是：每一行表示一个时刻（对应一个预测的单词），每num_steps行对应一个序列，一共有batch_size个num_steps行（因为这是一个batch大小的数据）。</li><li>输出targets的维度可以看到其形状为（batch_size,num_steps）（即20<em>15），表示的意思是，一个batch中有20条数据，而每一条数据有15个时间步，一个时间步对应一个单词。为了让预测的单词的顺序和targets中真实单词的顺序对应上，于是将targets的维度变成了（300，）（即：20</em>15），这样一个元素对应logits中的一行，每15个数据就表示一个序列。</li></ul><p><strong>接下来</strong>，就将上面的输入以及权重w（通常设置为1）传入sequence_loss_by_example函数。下面是这个函数的实现代码：</p><pre><code>def sequence_loss_by_example(logits, targets, weights,                             average_across_timesteps=True,                             softmax_loss_function=None, name=None):#logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].#targets: List of 1D batch-sized int32 Tensors of the same length as logits.#weights: List of 1D batch-sized float-Tensors of the same length as logits.#return:log_pers 形状是 [batch_size].   for logit, target, weight in zip(logits, targets, weights):      if softmax_loss_function is None:        # TODO(irving,ebrevdo): This reshape is needed because        # sequence_loss_by_example is called with scalars sometimes, which        # violates our general scalar strictness policy.        target = array_ops.reshape(target, [-1])        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(            logit, target)      else:        crossent = softmax_loss_function(logit, target)      log_perp_list.append(crossent * weight)    log_perps = math_ops.add_n(log_perp_list)    if average_across_timesteps:      total_size = math_ops.add_n(weights)       total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.      log_perps /= total_size  return log_perps</code></pre><p>可以看到函数内部主要是调用sparse_softmax_cross_entropy_with_logits函数，然后再加权平均后返回。<br>那么这个sparse_softmax_cross_entropy_with_logits函数是怎么计算的呢？这里有个<a href="https://blog.csdn.net/u012193416/article/details/77918732" target="_blank" rel="noopener">小例子</a>，可以看到他是将softmax和cross_entropy放在一起计算，这里就涉及到文章开头所复习得softmax和交叉熵的计算过程了。<br><strong>于是</strong>，我们知道了，针对于一个logits元素和一个targets元素，比如这个例子中取logits[0]，其维度为（10000，），targets[0]，它就是单独的一个整型的数，表示单词在词汇表中的id号。<strong>先计算logits[0]中各个元素的相对概率（即计算softmax），然后利用交叉熵公式计算预测值和真实值之间的交叉熵。</strong></p><blockquote><p>这个函数直接使用标签数据的，而不是采用one-hot编码形式（另一个函数softmax_cross_entropy_with_logits必须是one-hot形式的数据，具体见<a href="https://blog.csdn.net/zchang81/article/details/70225220" target="_blank" rel="noopener">softmax_cross_entropy_with_logits函数详解</a>）</p></blockquote><p>为了验证这个计算过程，我将这个例子中的logits[0]，targets[0]以及通过</p><pre><code>loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(    # 这个函数先求解softmax，再求解交叉熵        [logits],        [tf.reshape(input_.targets, [-1])],        [tf.ones([batch_size * num_steps], dtype=tf.float32)])</code></pre><p>得到的loss（这里取loss[0]）取出，单独通过tf.nn.softmax以及交叉熵公式来依存计算这个过程。<br>首先来看下通过例子源代码取出的数据情况：</p><pre><code>logits shape:(10000,)logits value:[ 7.8470936   8.238499    8.979608   ... -0.73421586 -0.913356 -0.7552418 ]target value:9971loss value:11.060587</code></pre><p>自己计算的代码为：</p><pre><code>import tensorflow as tf# 将单词id转化为one-hot模式target=list([0]*10000)target[9971]=1target=np.array(target)# 转化为张量logits=tf.convert_to_tensor(r[&#39;logits&#39;],dtype=float)y_=tf.convert_to_tensor(target,dtype=float)# 法1，分开计算----------------------------------------------------# 计算softmaxy=tf.nn.softmax(logits)# 计算交叉熵cross_entropy=-tf.reduce_sum(y_*tf.log(y))# 法2，调用函数计算-------------------------------------------------# 调用函数计算cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_))with tf.Session() as sess:    softmax=sess.run(y)    c_e=sess.run(cross_entropy)    c_e2=sess.run(cross_entropy2)    print(&quot;softmax:\n&quot;,softmax)    print(&quot;cross_entropy:\n&quot;,c_e)    print(&quot;function:\n&quot;,c_e2)</code></pre><p>运行结果为：</p><pre><code>softmax: [5.8356632e-02 8.6312823e-02 1.8110682e-01 ... 1.0946491e-05 9.1511438e-06 1.0718737e-05]cross_entropy: 11.060587function: 11.060587</code></pre><p>可以看到，不管是分开计算还是调用函数计算，其结果和例子源代码中得到的结果相同。这就是sparse_softmax_cross_entropy_with_logits函数的计算过程，tf.contrib.legacy_seq2seq.sequence_loss_by_example函数的计算过程就是在其内部的每个时间步中调用sparse_softmax_cross_entropy_with_logits函数即可。</p><h3 id="4-小结"><a href="#4-小结" class="headerlink" title="4  小结"></a>4  小结</h3><p>本文通过tensorflow官方提供的基于LSTM的语言模型ptb_word_lm.py例子中的部分代码，对tf.contrib.legacy_seq2seq.sequence_loss_by_example函数的计算过程进行了简单的介绍，这其中的理论知识可以查看前文中链接到的哪些博文，这里只是纯介绍计算过程，因为这也是我最近遇到的问题。我也不知道有没有错，如果文中有写错或者理解错误的地方，请大家及时联系我纠正，谢谢！</p><h3 id="参考文献（时间仓促，就没单独整理了）"><a href="#参考文献（时间仓促，就没单独整理了）" class="headerlink" title="参考文献（时间仓促，就没单独整理了）"></a>参考文献（时间仓促，就没单独整理了）</h3><p>【1】前文提到的所有链接<br>【2】tensorflow官方文档以及官方代码</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensorflow </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记03】ReasoNet-Learning to Stop Reading in Machine Comprehension</title>
      <link href="/2018/10/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B003%E3%80%91ReasoNet_%20Learning%20to%20Stop%20Reading%20in%20Machine%20Comprehension/"/>
      <url>/2018/10/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B003%E3%80%91ReasoNet_%20Learning%20to%20Stop%20Reading%20in%20Machine%20Comprehension/</url>
      
        <content type="html"><![CDATA[<h4 id="1-问题及数据集"><a href="#1-问题及数据集" class="headerlink" title="1  问题及数据集"></a>1  问题及数据集</h4><h5 id="1-1-问题"><a href="#1-1-问题" class="headerlink" title="1.1  问题"></a>1.1  问题</h5><p>本论文主要解决一种面向Cloze-style（填空式）的阅读理解（问答）问题</p><h5 id="1-2-数据集"><a href="#1-2-数据集" class="headerlink" title="1.2  数据集"></a>1.2  数据集</h5><p>（1）CNN&amp;Daily Mail<br>（2）SQuAD<br>（3）Graph Reachability datase</p><h4 id="2-已有方法"><a href="#2-已有方法" class="headerlink" title="2  已有方法"></a>2  已有方法</h4><h5 id="2-1-单轮推理"><a href="#2-1-单轮推理" class="headerlink" title="2.1  单轮推理"></a>2.1  单轮推理</h5><p>（1）特点<br>单轮推理模型主要利用注意力机制来强调文档中与问题相关的那些部分，计算问题和文档子单元的相应加权表示之间的相关度，为候选目标评分。这好比在处理一些不太重要的部分的同时聚焦其他重要的部分，从而找到最后可能的答案。这种模型较简单，推理能力也不是很强。<br>（2） 方法</p><ul><li>Hermann et al. propose the attentive reader and the impatient reader models using neural networks with an attention over passages to predict candidates.</li><li>Hill et al. use attention over window-based memory, which encodes a window of words around entity candidates, by leveraging an end- to-end memory network。</li><li><a href="https://blog.csdn.net/xyz1584172808/article/details/83035597" target="_blank" rel="noopener">Kadlec et al. propose the attention-sum reader to sum up all the attention scores for the same entity，This score captures the relevance between a query and a candidate. Chen</a></li><li>Chen et al. propose using a bilinear term similarity function to calculate attention scores with pretrained word embeddings.</li><li>Trischler et al. propose the EpiReader which uses two neural network structures: one extracts candidates using the attention-sum reader; the other reranks candidates based on a bilinear term similarity score calculated from query and passage representations。</li></ul><h5 id="2-2-多轮推理"><a href="#2-2-多轮推理" class="headerlink" title="2.2  多轮推理"></a>2.2  多轮推理</h5><p>（1）特点<br>对于复杂的段落以及复杂的问题，读者通常会再次阅读文档以获得更深层次的信息。多轮推理模型就是将问题和前面推理中获得的新信息结合，从而模拟这个重读过程，得到新的推理信息。不断的迭代推理，在若干推理后预测出答案。但现存的多轮推理模型通常都预定义了迭代的次数，而忽视了每一个问题或文档的复杂度。<br>（2）方法</p><ul><li>Hill et al. use multiple hops memory network to augment the query with new information from the previous hop。</li><li>Gated Attention reader is an extension of the attention-sum reader with multiple iterations by pushing the query encoding into an attention-based gate in each iteration.</li><li>Iterative Alternative (IA) reader produces a new query glimpse and document glimpse in each iteration and utilizes them alternatively in the next iteration.</li><li>Cui et al. propose to extend the query-specific attention to both query-to-document attention and document-to-query attention, which is built from the intermediate results in the query-specific attention。</li></ul><h5 id="2-3-现有方法的问题"><a href="#2-3-现有方法的问题" class="headerlink" title="2.3  现有方法的问题"></a>2.3  现有方法的问题</h5><p>上面的两类模型，第一种模型可以看作是第二种模型的一种特例，其迭代次数为1，于是目前已有的方法都采用了固定推理的轮数。而人在面临阅读理解的时候，会根据问题和文章的难度动态的决定推理次数。</p><h4 id="3-本文提出的方法"><a href="#3-本文提出的方法" class="headerlink" title="3  本文提出的方法"></a>3  本文提出的方法</h4><p>本文提出了ReasoNet模型，用于模拟人类阅读的过程，带着问题多次阅读原文，并设置一个终止状态，动态的决定推理的轮数，直到获得的信息足够用于回答问题时才停止推理过程。此外，又引入了强化学习算法CR（对比奖励）来训练模型。</p><h4 id="4-具体内容"><a href="#4-具体内容" class="headerlink" title="4  具体内容"></a>4  具体内容</h4><h5 id="4-1-ReasoNet网络结构"><a href="#4-1-ReasoNet网络结构" class="headerlink" title="4.1  ReasoNet网络结构"></a>4.1  ReasoNet网络结构</h5><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/reasonet.png" alt="ReasoNet网络结构"><br>（1）<strong>Memory</strong>：外部记忆，Document Encoder产生的每个单词的上下文表示<br>（2）<strong>Attention</strong>：注意力向量（也叫上下文向量）xt是由当前状态和外部记忆进行计算得出的<br>（3）<strong>Internal State</strong>：内部状态S，初始状态s1是Query Encoder产生的问题表示，t时刻状态由RNN产生st=RNN(st−1,xt;θs)；<br>（4）<strong>Termination Gate</strong>：根据当前内部状态产生一个binary随机变量。如果为1，那么结束推理预测答案；如果为0，那么继续推理<br>（5）<strong>Answer</strong>：当终止状态为1时，回答问题</p><h5 id="4-2-具体步骤"><a href="#4-2-具体步骤" class="headerlink" title="4.2  具体步骤"></a>4.2  具体步骤</h5><p>（1）计算过程<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/reasonet1.png" alt="推理算法"></p><p>（2）需要学习的参数</p><ul><li>embedding matrices θW </li><li>attention network θx </li><li>the state RNN network θs</li><li>the answer action network θa </li><li>the termination gate network θtg Query</li><li>Encoder和Document Encoder的参数(参考笔记中提出)</li></ul><p>（3）期望奖励<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/reasonet3.png" alt="期望奖励"></p><p>如果最终时刻的回答正确，则置Rt=1，否则为0，中间时刻的Rt=0。（具体细节还不是很清楚，先占坑，后面看了强化学习来填坑）</p><h4 id="5-实验"><a href="#5-实验" class="headerlink" title="5  实验"></a>5  实验</h4><h5 id="5-1-CNN-amp-Daily-Mail-Datasets"><a href="#5-1-CNN-amp-Daily-Mail-Datasets" class="headerlink" title="5.1  CNN&amp;Daily Mail Datasets"></a>5.1  CNN&amp;Daily Mail Datasets</h5><p>（1）实验配置</p><ul><li><strong>vocab_size</strong>：|V| = 101k words in CNN，|V| = 151k words in Daily Mail</li><li><strong>Embedding Layer</strong>：300-dimensional word embeddings，300-dimensional pretrained Glove word embeddings，dropout with probability 0.2</li><li><strong>Bi-GRU Encoder</strong>：bidirectional GRU for encoding query and passage into vector representations，隐藏层单元个数：CNN256个，Daily Mail384个；用正交矩阵初始化GRU的权重；GRU的其他权重采用-0.01到+0.01之间的均匀随机分布。</li><li><strong>Memory and Attention</strong>：composed of query memory and pas- sage memory；</li><li><strong>Internal State Controller</strong>：choose GRU model as the internal state controller</li><li><strong>TerminationModule</strong>：adopt a logistical regression to model the termination variable at each time step<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/reasonet4.png" alt=""></li><li><strong>最大推理轮数为5</strong></li></ul><p>（2）实验结果<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/reasonet5.png" alt="实验结果"></p><h5 id="5-2-SQuAD"><a href="#5-2-SQuAD" class="headerlink" title="5.2  SQuAD"></a>5.2  SQuAD</h5><p>（1）实验配置<br>与前一个数据集的配置有一些区别，也是按照那几部分分别设置<br>（2）实验结果<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/reasonet6.png" alt="实验结果"></p><h5 id="5-3-Graph-Reachability-Task"><a href="#5-3-Graph-Reachability-Task" class="headerlink" title="5.3  Graph Reachability Task"></a>5.3  Graph Reachability Task</h5><p>这种数据之前没了解过，先占坑，后面来填</p><h4 id="6-笔记小结"><a href="#6-笔记小结" class="headerlink" title="6  笔记小结"></a>6  笔记小结</h4><p>（1）通过引入终止状态，使得模型能够动态的决定推理轮数<br>（2）通过强化学习来训练模型<br>（3）本篇论文只是读了个大概，很多细节都还没有具体去研究，不了解的点还非常多，先占坑，后面研究了强化学习再来把相应的坑补上。</p><h4 id="资料来源"><a href="#资料来源" class="headerlink" title="资料来源"></a>资料来源</h4><p>（1）<a href="https://arxiv.org/pdf/1609.05284.pdf" target="_blank" rel="noopener">论文地址</a><br>（2）相关论文笔记：<a href="http://cairohy.github.io/2017/05/22/deeplearning/NLP-RC-ReasoNet-NIPS2016-%E3%80%8AReasoNet%20Learning%20to%20Stop%20Reading%20in%20Machine%20Comprehension%E3%80%8B/" target="_blank" rel="noopener">学习如何停止</a><a href="http://cairohy.github.io/2017/05/22/deeplearning/NLP-RC-ReasoNet-NIPS2016-%E3%80%8AReasoNet%20Learning%20to%20Stop%20Reading%20in%20Machine%20Comprehension%E3%80%8B/" target="_blank" rel="noopener">阅读</a><br>（3）数据集<a href="https://github.com/deepmind/rcdata" target="_blank" rel="noopener">CNN&amp;Daily Mail</a></p><h4 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h4><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/swdt.png" alt="论文笔记思维导图"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记01】Phrase-Based &amp; Neural Unsupervised Machine Translation</title>
      <link href="/2018/10/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B001%E3%80%91Phrase-Based%20&amp;%20Neural%20Unsupervised%20Machine%20Translation/"/>
      <url>/2018/10/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B001%E3%80%91Phrase-Based%20&amp;%20Neural%20Unsupervised%20Machine%20Translation/</url>
      
        <content type="html"><![CDATA[<p>这篇论文来源于EMNLP2018，<a href="https://arxiv.org/abs/1804.07755" target="_blank" rel="noopener">论文地址</a>，这次采用了思维导图的方式记录笔记，思维导图的缩略图如下：<img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/pnumt/p1.png" alt="论文笔记思维导图"></p><p>该思维导图较大，于是上传时进行了压缩，可以直接点击思维导图地址进行交互式的访问或下载，<a href="http://219.153.130.77:8094/swdt.html" target="_blank" rel="noopener">思维导图地址</a>，<a href="http://118.24.7.16/swdt.html" target="_blank" rel="noopener">备用思维导图地址</a></p><p>效果图：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/pnumt/p2.png" alt="交互式思维导图截图"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文笔记02】Text Understanding with the Attention Sum Reader Network</title>
      <link href="/2018/10/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B002%E3%80%91Text%20Understanding%20with%20the%20Attention%20Sum%20Reader%20Network/"/>
      <url>/2018/10/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B002%E3%80%91Text%20Understanding%20with%20the%20Attention%20Sum%20Reader%20Network/</url>
      
        <content type="html"><![CDATA[<h4 id="1-问题及数据集"><a href="#1-问题及数据集" class="headerlink" title="1 问题及数据集"></a>1 问题及数据集</h4><h5 id="1-1-问题"><a href="#1-1-问题" class="headerlink" title="1.1 问题"></a>1.1 问题</h5><p>给定较长一段话的context和一个较短的问题，以及一些candidate answers，训练出可以准确预测正确答案的模型，本模型主要针对命名实体和常用名词这两种词性的单词进行填空。</p><h5 id="1-2数据集"><a href="#1-2数据集" class="headerlink" title="1.2数据集"></a>1.2数据集</h5><p>（1）CNN&amp;Daily Mail<br>（2）CBT</p><h4 id="2-已有方法"><a href="#2-已有方法" class="headerlink" title="2 已有方法"></a>2 已有方法</h4><p>（1）Attentive and Impatient Readers<br>（2）Attentive<br>（3）Chen et al. 2016<br>（4）MemNNs<br>（5）Pointer Networks<br>（6）Dynamic Entity Representation</p><h4 id="3-本文提出的方法"><a href="#3-本文提出的方法" class="headerlink" title="3 本文提出的方法"></a>3 本文提出的方法</h4><p>ASReader模型使用注意力机制计算每个单词的注意力权重之和，从而从上下文中选择答案，而不是像在之前的模型一样，使用文档与问题的相似度或提取特征构建特征工程等方式来定位答案。</p><h4 id="4-具体内容"><a href="#4-具体内容" class="headerlink" title="4 具体内容"></a>4 具体内容</h4><h5 id="4-1-网络结构"><a href="#4-1-网络结构" class="headerlink" title="4.1 网络结构"></a>4.1 网络结构</h5><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/asreader/asreader.jpg" alt="AS reader网络结构"></p><h5 id="4-2具体过程"><a href="#4-2具体过程" class="headerlink" title="4.2具体过程"></a>4.2具体过程</h5><p><strong>step1</strong>：通过一层Embedding层将document和query中的word分别映射成向量。<br><strong>step2</strong>：用一个单层双向GRU来encode document，得到context representation，每个time step的拼接来表示该词<br><strong>step3</strong>：用一个单层双向GRU来encode query，用两个方向的last state拼接来表示query。<br><strong>step4</strong>：每个word vector与query vector作点积后归一化的结果作为attention weights，就query与document中的每个词之前的相关性度量。<br><strong>step5</strong>：最后做一次相同词概率的合并，得到每个词的概率，最大概率的那个词即为answer。为节约计算时间，可以只选择candidate answer里的词来计算概率。</p><h5 id="4-3-评估方法"><a href="#4-3-评估方法" class="headerlink" title="4.3 评估方法"></a>4.3 评估方法</h5><p><strong>average ensemble by top 20%</strong>:更改初始化参数，训练多个模型，然后取在验证集上效果最好的前20%个模型做bagging.<br><strong>average ensemble</strong>:取前效果排名前70%的model做bagging<br><strong>greedy ensemble</strong>:根据效果排序从效果最好的模型开始bagging，如果bagging后的模型在验证集上效果更好就加入，一直持续到最后。</p><h4 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5.实验结果"></a>5.实验结果</h4><h5 id="5-1-CNN-Daily-Mail"><a href="#5-1-CNN-Daily-Mail" class="headerlink" title="5.1.CNN/Daily Mail"></a>5.1.CNN/Daily Mail</h5><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/asreader/asreader2.jpg" alt="CNN/Daily Mail上的实验室结果"></p><h5 id="5-2-CBT"><a href="#5-2-CBT" class="headerlink" title="5.2.CBT"></a>5.2.CBT</h5><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/asreader/asreader3.jpg" alt="CBT上的实验结果"></p><h4 id="6-小结"><a href="#6-小结" class="headerlink" title="6 小结"></a>6 小结</h4><p>（1）利用点积来计算注意力权重，简化了模型，但是能达到同样或者更好的效果。<br>（2）利用注意力权重之和来选择答案，而不是像以前的工作那样通过权重提取特征从而预测答案，但该模型更倾向于选择重复次数较多的单词作为答案。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/asreader/swdt.png" alt="论文笔记思维导图"></p><h4 id="资源来源"><a href="#资源来源" class="headerlink" title="资源来源"></a>资源来源</h4><h5 id="论文地址"><a href="#论文地址" class="headerlink" title="论文地址"></a><a href="http://www.aclweb.org/anthology/P/P16/P16-1086.pdf" target="_blank" rel="noopener">论文地址</a></h5><h5 id="论文代码"><a href="#论文代码" class="headerlink" title="论文代码"></a><a href="https://github.com/rkadlec/asreader" target="_blank" rel="noopener">论文代码</a></h5><h4 id="相关论文笔记"><a href="#相关论文笔记" class="headerlink" title="相关论文笔记"></a>相关论文笔记</h4><h5 id="https-zhuanlan-zhihu-com-p-23462480（本笔记主要参考该链接的笔记）"><a href="#https-zhuanlan-zhihu-com-p-23462480（本笔记主要参考该链接的笔记）" class="headerlink" title="https://zhuanlan.zhihu.com/p/23462480（本笔记主要参考该链接的笔记）"></a><a href="https://zhuanlan.zhihu.com/p/23462480" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23462480</a>（本笔记主要参考该链接的笔记）</h5><h5 id="https-www-imooc-com-article-28709"><a href="#https-www-imooc-com-article-28709" class="headerlink" title="https://www.imooc.com/article/28709"></a><a href="https://www.imooc.com/article/28709" target="_blank" rel="noopener">https://www.imooc.com/article/28709</a></h5><h5 id="https-zhuanlan-zhihu-com-p-21354432"><a href="#https-zhuanlan-zhihu-com-p-21354432" class="headerlink" title="https://zhuanlan.zhihu.com/p/21354432"></a><a href="https://zhuanlan.zhihu.com/p/21354432" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21354432</a></h5>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 阅读理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文翻译01】How Much Reading Does Reading Comprehension Require？</title>
      <link href="/2018/10/04/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%9101%E3%80%91How%20Much%20Reading%20Does%20Reading%20Comprehension%20Require/"/>
      <url>/2018/10/04/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%9101%E3%80%91How%20Much%20Reading%20Does%20Reading%20Comprehension%20Require/</url>
      
        <content type="html"><![CDATA[<h3 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0 Abstract"></a>0 Abstract</h3><p>最近的许多论文都涉及到阅读理解，他们一般都包括（问题，段落，答案）元组。或许，一个模型必须综合问题和段落两者的信息来预测相应的答案。然而，尽管人们对这个话题产生了浓厚的兴趣，数百篇已发表的论文争夺排行榜的主导地位，但关于许多流行基准测试难度的基本问题仍未得到解答。在这篇论文中，我们为bAbI、SQuAD、CBT、CNN和Who-did-What的数据集建立了合理的基线，发现只考虑问题和只考虑段落的模型通常表现得出奇的好。在20个bAbI任务的14个中，只考虑文章的模型都能达到超过50%的准确率，有时能够达到完整模型的性能。有趣的是，尽管CBT提供了20个句子的故事，但只有最后一句是相对准确的预测所需要的。通过比较，SQuAD和CNN看起来更有实力。</p><h3 id="1-Intoduction"><a href="#1-Intoduction" class="headerlink" title="1 Intoduction"></a>1 Intoduction</h3><p>目前，研究人员们提出了各种各样的端到端深度学习算法，以推动各种基准测试，阅读理解（RC）已经成为一项受欢迎的任务。以Hermann(2015);Onishi（2016）等人为代表的，不像之前的工作那样从一般的结构知识中处理问题答案。阅读理解需要从一个给定的无结构化的文章（段落）中抽取信息。不难想象，这样的系统该是何等的有用。与一般的文本摘要相比，阅读理解系统可以回答关于特定文档有目的性的问题，有效地提取事实和见解。<br>尽管多年来已经提出了许多阅读理解数据集（Hirschman 1999;Breck 2001;Penas 2011; Penas 2012;Sutcliffe 2013;Richardson 2013;Berant 2014;），最近，人们提出了更大的数据集，以适应深度学习的数据强度。这些语料库的来源和规模以及他们如何处理预测问题都不相同，这些问题主要有分类问题、跨度选择、句子检索或者开放式答案生成。研究人员在这些基准上稳步推进，提出了无数的神经网络架构，旨在利用问题和段落来产生答案。<br>在这篇论文中，我们认为，在经验基准的快速发展中，关键的步骤有时会被跳过。特别是，我们证明了其中一些任务的难度等级很低。例如，对于许多阅读理解数据集，无论是在介绍数据集的论文中，还是在那些提出模型的论文中，都没有给出在忽略问题或文章的情况下的表现如何。在其他数据集里，尽管文章可能包含了许多行文本，但也不清楚到底需要多少文本信息来回答这个问题。比如，答案可能总是出现在第一句或者最后一句。<br>我们对几个流行的阅读理解数据集和模型进行了介绍，并对它们的性能在只提供问题或只提供段落的情况下进行了分析。我们还展示了在许多任务中，在提供问题和段落的情况下。获得的结果出奇的好，超过了许多基线，有时甚至超过了相同的模型。<br>我们注意到类似的问题在Goyal的视觉问答和 Gururangan、 Poliak、Glockner的自然语言推理中显示出来。其他几篇论文也讨论了各种阅读理解基准测试的不足。我们将在下面介绍相应的数据集的段落中讨论这些研究。</p><h3 id="2-Datasets"><a href="#2-Datasets" class="headerlink" title="2 Datasets"></a>2 Datasets</h3><p>在下面的这部分中，我们将提供我们所调查的每个数据集的上下文，然后描述破坏数据的过程，这个数据是我们的只考虑问题和只考虑段落的实验所要求的数据。<br><strong>CBT</strong> Hill（2016）通过使用儿童书籍中的文章。准备了一个cloze-style（填空式）阅读理解数据集。在他们的数据集中，每一篇文章包含了20个连续的句子，每个问题都是删除了一个单词的第21句。这个缺失的单词背当做答案。数据集被分为四类答案：命名实体，普通名词，动词和介词。训练语料包含了37000条数据，每个问题和10个答案对应，这些答案的词性和正确答案的词性相匹配。作者建立了基于lstm/词嵌入的只有问题的基线系统，但是没有展示他们使用只考虑问题或只考虑段落信息的最佳模型获得的结果。<br><strong>CNN</strong> Hermann（2015）介绍了包含超过100万篇新闻文章，每一篇都有几个突出的句子。此外，他们还为填空式的数据集做准备，他们从一个突出句子（问题）中删除了一个实体（答案）。他们将所有实体匿名化，以确保模型依赖于文章中包含的信息，而不是通过例子来记忆给定实体的特征而忽略了文章。平均而言，文章包含26个实体，包括了全部可能的候选答案。Chen（2016）分析了CNN和Daily Mail任务的困难。他们为每个实体e（e在问题中出现，在文章中出现等等）手工设计了8个特性的集合，这展示了这个简单的分类器比许多早期的深度学习结果要好。<br><strong>Who-did-What</strong> Onishi（2016）摘录了成对的新闻文章，每对都指向相同的事件。采用了填空式的方式，他们从一篇文章的第一句话（问题）中删除了一个人的名字（答案）。模型必须根据问题和这一对中的其他文章（段落）来预测答案。与CNN不同，Who-did-What不将实体匿名化。平均而言，每个问题和3.5个答案对应。作者从他们的数据集中删除了几个问题，以阻止一些简单的策略，比如总是预测在文章中出现最多（或第一次）的名字。<br><strong>bAbI</strong> Weston（2016）提出了20个任务的集合，帮助研究人员识别和纠正他们的阅读理解系统的缺陷。与目前为止所讨论的数据集不同，这个任务中的问题不是填空式的，而是使用模板合生的。这限制了文章中出现短语的多样性，此外，这还将数据集词汇限制为150个单词，相比之下，CNN数据集的词汇量接近12万字。具有自适应记忆、ngrams语言模型和非线性匹配的记忆网络，在20个bAbI任务中的12个中得到了百分百的准确率。我们注意到Lee等人（2016）先前指出，bAbI的任务可能无法达到作为“AI完全问答”的一种衡量标准，提出了基于==张量积表示==的两种模型，这些模型在许多bAbI任务中达到了百分之百的精度。<br><strong>SQuAD</strong> 最近，Rajpurkar等人发布了斯坦福问答数据集，这个数据集包含了10多万个众包问题，涉及536篇文章。每个问题都与从一篇文章中提取的段落相关联。这些段落比CNN和Who-did-What数据集中的段落更短。模型通过从这些段落中选择（可变长度的）答案。<br><strong>Generating Corrupt Data</strong> 为了使问题或段落中的某些信息无效，但同时使每个架构保持完整，我们通过随机分配问题或者随机化段落，保留段落和答案之间的对应关系，来创建每个数据集的损坏版本。对于那些需要从文章中选择范围或候选答案的问答任务，我们创建的段落包含了随机位置的候选答案，但除此之外，还有一些随机的胡言乱语。</p><h3 id="3-Models"><a href="#3-Models" class="headerlink" title="3 Models"></a>3 Models</h3><p>在我们对各种阅读理解基线测试的调查中，我们依赖于以下三个最近提出的模型：key-value memory networks、gated attention readers和QA nets。尽管篇幅的限制阻碍了对每一个结构的全面讨论，但我们提供了对源文件的引用，并简要讨论了复现我们结果所必需的任何实现决策。<br><strong>Key-Value Memory Networks</strong> 我们实现了一个Key-Value Memory Networks（KV-MemNet）（2016），并将它运用在bAbI和CBT两个数据集上。KV-MemNets是基于Memory Networks(Sukhbaatar 2015)，在两个数据集上都表现良好。对于bAbI任务，键和值以及段落都被编码成词带模型。对于CBT任务，键是对候选答案周围5个单词窗口进行词带编码，值就是候选答案本身。我们将跳跃的数量固定到3，词嵌入大小调整为128。<br><strong>Gated Attention Reader</strong>由Dhingra（2017）引入，Gated Attention Reader 像MemNets一样在段落中执行多个跳跃，单词表示在每一跳上都有改进，并被一个注意力集中模块映射到最后一跳的候选答案集上的概率分布。该模型几乎抵得上许多填空式的阅读理解数据集上的最佳报告结果，因此我们将它运用到Who-did-What,CNN,CBT-NE和CBT-CN。<br><strong>QA Net</strong>最近由Yu等人引入。最近，QA-Net在SQuAD数据集上的表现优于所有以前的模型。段落和问题被作为输入给独立的编码器，这些编码器包括深度可分的卷积和全局自注意力机制。接下来是一个段落-问题的注意力层，然后是堆叠的编码器。这些编码器的输出被用来预测段落内的答案范围。</p><h3 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h3><p><strong>bAbI tasks</strong> 表1展示了在bAbI任务中，取消在问题或段落中出现的信息后，Key-Value Memory Network所获得的结果。在任务2,7,13和20中，只有段落的模型在随机分配的问题中获得超过80%的准确率。此外，在任务3,13,16和20中，只有段落的模型与在全数据集上训练的模型的性能相当。在任务18中，只有问题的模型的准确率达到了91%，几乎与全模型所取得的93%的最佳性能相当。这些结果表明，一些bAbI的任务比人们想象的要容易。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/rp1.png" alt="表1"><br><strong>Children’s Books Test</strong> 在NE和CN CBT任务中，只有问题的模型在KV-MemNets上获得的精度接近于完全准确，在动词（V）和介词（P）任务中，只有问题的模型优于完整的模型（表2）。只有问题的模型在Gated attention readers上的命名实体（NE）和普通名词（CN）任务分别达到了50.6%和54%的准确率，而只有段落模型的精确度分别为40.8%和36.7%。我们注意到，在Hill等人（2016）的NE任务中报道的19个结果中，我们使用问题模型信息的结果可以超过16个。表3显示，如果我们只使用最后一句话而不是使用文章中的全部20句话，我们基于KV-MemNet的句子记忆实现了比在大多数子任务上的完整模型更好或相当的性能。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/rp2.png" alt="表2"><br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/rp3.png" alt="表3"><br><strong>CNN</strong> 表格2展示了Gated Attention Reader在CNN数据集上的性能。只有问题模型和只有段落模型分别获得了25.6%和38.3%的精确度，而真正的数据集则是77.8%。这种精确性的下降可能是由于实体的匿名化，这些匿名化的实体阻止了模型构建实体的特定信息。尽管Chen等人提出了其缺陷，我们发现，对于CNN和我们评估的所有填空式的阅读理解数据集，似乎都是最精心设计的。<br><strong>Who-did-What</strong> 在严格和放松的环境下，只有段落的模型能达到超过50%的准确率，在严格的环境中达到完全模型的15%。只有问题的模型在放松环境中也达到了50%的准确率，同时在严格的环境中达到了41.8%的准确率。我们的只有段落的模型也比Onishi等人（2016）报告的所有被抑制的基线和5个额外基线的性能都要高。我们怀疑这些模型会记住特定实体的属性，从而证明赫尔曼等人（2015）使用的实体匿名化来构建CNN数据集。<br><strong>SQuAD</strong> 我们的研究结果表明，SQuAD是一项异常精心设计和具有挑战性的阅读理解任务。答案的范围选择模式要求模型考虑段落，因此Q-only在QANet上展现出了糟糕性能（表4)。由于SQuAD需要通过范围选择来回答，所以我们在这里构造只有问题模型的变体：通过随机排列所有相关问题的答案，用随机的单词填满空白。此外，Q-only型和P-only模型的F1得分分别仅为4%和14.8%（表4），明显低于正常任务的79.1。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/rp4.png" alt="表4"></p><h3 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5 Discussion"></a>5 Discussion</h3><p>我们简要地讨论了我们的发现，为评估新的基准和算法提供了一些指导原则，并推测为什么有些问题可能已经被忽视了。我们的目标不是责怪过去数据集的创造者，而是为未来的研究人员通过提供实用的指导来支持社区。<br><strong>Provide rigorous RC baselines</strong> 公开的阅读理解数据集应该包含合理的基线，以描述任务的难度，特别是问题和段落的重要性。此外，改进的后续报告应该展示在全部任务和不涉及问题和段落的变化方面的表现。尽管许多技术创新据说是通过更好地匹配问题和段落中的信息来工作的，但如果没有这些基线，人们就无法判断结果是否来自于声称的原因，或者模型是否能更好地完成段落分类工作（不考虑问题）。<br><strong>Test that full context is essential</strong> 即使在需要问题和段落的任务中，问题也可能比实际情况更困难。乍一看，CBT中长度为20的段落，可能表明需要对所有20句话进行推理，以确定每个问题的正确答案。然而，事实证明，对于某些模型，可以通过只考虑最后一句话来达到类似的性能。我们建议研究人员提供合理的==消融==，以描述每个模型真正需要的上下文的数量。<br><strong>Caution with cloze-style RC datasets</strong> 我们注意到，填空式数据集通常是由编程的方式创建的。因此，可以将数据集生成、发布和合并到许多下游研究中，而这些研究都没有人工检查数据的时间。我们推测，结果是这些数据集往往不太考虑回答这些问题所涉及的内容因此特别容易受到我们研究中所描述的那些被忽视的弱点影响。<br><strong>A note on publishing incentives</strong> 我们担心推荐的实验严密性可能会与当前的公开激励措施相违背。我们推测，引入数据集的论文，通过省略不愉快的数据消融，而不是把它们包括在内，或许更有可能在会议上被接受。此外，由于评审者常常要求结构新颖，通过提供未经证实的故事来解释为什么一个给定的体系结构起作用的原因，，而不是提供严格的消融研究来排除虚假的解释和不必要的模型组件，方法类论文可能会找到更容易接受的途径。为了更广泛地讨论机器学习研究中的偏差激励和经验严密性，我们将感兴趣的读者指向 Lipton and Steinhardt (2018) and Sculley et al.(2018)</p><h3 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h3><p>之前看论文，也没怎么记录，导致很多时候看了没过多久，基本上也就忘得差不多了。于是今后考虑以论文翻译，论文笔记或者复现论文中模型的方式巩固从而加深印象。<br>这是翻译的第一篇，本渣也是一个英语渣，有很多单词翻译都拿不太准（某些专业词汇还不能很好的翻译，所以在文中也用黄色阴影来标注出来）。<br>还是回到这篇<a href="https://arxiv.org/pdf/1808.04926.pdf" target="_blank" rel="noopener">论文</a>上来，这篇论文是EMNLP2018的最佳短论文，其中首先介绍了阅读理解常用的数据集以及比较流行的网络模型，通过对这些的介绍，引入了本文的主要工作。在具体实验中，利用更简单的实验模型却能达到相当的结果，进而引入本文的主题——阅读理解真正读到了多少？缺少基线对数据集的测试，就不能真正了解到那部分数据是有用的，那部分数据是无用的，而且数据集往往都是为了某个实验而精心设计的。文中最后给出了一些<strong>指导建议（划重点）</strong>，这些建议一阵见血的指出了目前学术界的问题，这些大家一看就懂，自行体会。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 阅读理解 </tag>
            
            <tag> 论文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种最原始的混沌神经元构造过程</title>
      <link href="/2018/09/25/%E4%B8%80%E7%A7%8D%E6%9C%80%E5%8E%9F%E5%A7%8B%E7%9A%84%E6%B7%B7%E6%B2%8C%E7%A5%9E%E7%BB%8F%E5%85%83%E6%9E%84%E9%80%A0%E8%BF%87%E7%A8%8B/"/>
      <url>/2018/09/25/%E4%B8%80%E7%A7%8D%E6%9C%80%E5%8E%9F%E5%A7%8B%E7%9A%84%E6%B7%B7%E6%B2%8C%E7%A5%9E%E7%BB%8F%E5%85%83%E6%9E%84%E9%80%A0%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>一个混沌神经元的输出与①每一个外部输入在每一个离散时刻的状态，②其他每一个混沌神经元在每一个时刻的状态，以及③该混沌神经元在每一个离散时刻的状态（主要为不应性情况）和④预设的阈值有关。<br>由于该博客公式显示问题，请移步CSDN博文<a href="https://blog.csdn.net/xyz1584172808/article/details/82838532" target="_blank" rel="noopener">一种最原始的混沌神经元构造过程</a></p><h3 id="一-混沌神经元的构造"><a href="#一-混沌神经元的构造" class="headerlink" title="一  混沌神经元的构造"></a>一  混沌神经元的构造</h3><p>（1）一种混沌神经元的构造方法如下：</p><script type="math/tex; mode=display">x_{i}\left( t + 1 \right) = f\left\lbrack \sum_{j = 1}^{M}v_{\text{ij}}\sum_{d = 0}^{t}{k_{e}^{d}A_{j}(t - d)} + \sum_{j = 1}^{N}\text{wij}\sum_{d = 0}^{t}{k_{f}^{d}x_{j}(t - d)} - \alpha\sum_{d = 0}^{t}{k_{r}^{d}g\left\{ x_{i}\left( t - d \right) \right\}} - \Theta_{i} \right\rbrack</script><p>对于式中的第一部分：</p><script type="math/tex; mode=display">\sum_{j = 1}^{M}v_{\text{ij}}\sum_{d = 0}^{t}{k_{e}^{d}A_{j}(t - d)}$$该项表示外部输入对当前混沌神经元的影响。M表示外部输入的个数；$v_{\text{ij}}$表示第j个外部输入到第i个混沌神经元的连接权值；t表示离散时间点；$k_{e}^{d}\$表示外部输入的衰变参数；$A_{j}(t - d)$表示在离散t-d时刻，第j个外部输入的强度。于是，外部输入对当前混沌神经元的影响包括了每一个外部输入在每一个离散时刻t时的强度。对于式中的第二项：$$\sum_{j = 1}^{N}\text{wij}\sum_{d = 0}^{t}{k_{f}^{d}x_{j}(t - d)}</script><p>该项表示混沌神经元之间的影响。</p><p>N表示混沌神经元的个数；</p><p>$w_{ij}$表示当前混沌神经元与第j个混沌神经元之间的连接权重；</p><p>t表示离散时间点；</p><p>$k_{f}^{d}$表示其他混沌神经元输入的衰变参数；</p><p>$x_{j}(t - d)$表示在离散t-d时刻，混沌神经元$x_{j}$对当前神经元的影响强度。</p><p>于是该项主要描述当前混沌神经元与其他所有神经元之间的联系，其中包括了其他每一个混沌神经元在每一个离散时刻对当前神经元的影响。</p><p>此外，$g$表示不应性函数，$α$、$k_{r}^{d}$和$\Theta_{i}$分别表示不应性强度参数、不应性衰减参数和阈值。</p><p><strong>综上，一个混沌神经元的输出与①每一个外部输入在每一个离散时刻的状态，②其他每一个混沌神经元在每一个时刻的状态，以及③该混沌神经元在每一个离散时刻的状态（主要为不应性情况）和④预设的阈值有关。</strong></p><p>上述公式可以简化为：</p><script type="math/tex; mode=display">x_{i}\left( t + 1 \right) = f\left\{ \xi_{i}\left( t + 1 \right) + \eta_{i}\left( t + 1 \right) + \zeta_{i}(t + 1) \right\}</script><p>其中：</p><script type="math/tex; mode=display">\xi_{i}\left( t + 1 \right) = \left( \sum_{j = 1}^{M}v_{\text{ij}}\sum_{d = 0}^{t}{k_{e}^{d}A_{j}(t - d)} \right) = \sum_{j = 1}^{M}v_{\text{ij}}A_{j}\left( t \right) + k_{e}\xi_{i}\left( t \right)</script><script type="math/tex; mode=display">\eta_{i}\left( t + 1 \right) = \left( \sum_{j = 1}^{N}\text{wij}\sum_{d = 0}^{t}{k_{f}^{d}x_{j}(t - d)} \right) = \sum_{j = 1}^{N}w_{\text{ij}}x_{j}\left( t \right) + k_{f}\eta_{i}\left( t \right)</script><script type="math/tex; mode=display">\zeta_{i}\left( t + 1 \right) = \left( - \alpha\sum_{d = 0}^{t}{k_{r}^{d}g\left\{ x_{i}\left( t - d \right) \right\}} - \Theta_{i} \right) = - \alpha g\left\{ x_{i}\left( t \right) \right\} + k_{r}\zeta_{i}\left( t \right) - \theta_{i}</script><script type="math/tex; mode=display">\theta_{i} \equiv \Theta_{i}(1 - k_{r})</script><p>分别记为外部输入、其他神经元输入和自身不应性的内部状态。</p><p>（2）构建一个简单的自动联想神经网络（实例）</p><p>首先假设对神经网络的外部刺激是暂时不变，于是可以使用只有两个内部状态的简化方程：</p><script type="math/tex; mode=display">x_{i}\left( t + 1 \right) = f\left\{ \eta_{i}\left( t + 1 \right) + \zeta_{i}(t + 1) \right\}</script><script type="math/tex; mode=display">\eta_{i}\left( t + 1 \right) = k_{f}\eta_{i}\left( t \right) + \sum_{j = 1}^{N}w_{\text{ij}}x_{j}\left( t \right)</script><script type="math/tex; mode=display">\zeta_{i}\left( t + 1 \right) = k_{r}\zeta_{i}\left( t \right) - \alpha x_{i}\left( t \right) + a_{i}</script><p>其中使用<script type="math/tex">a_{i}</script>来表示外部输入和阈值的和，暂时约定为不变的常数。</p><h3 id="二-参考文献"><a href="#二-参考文献" class="headerlink" title="二  参考文献"></a>二  参考文献</h3><p><a href="http://xueshu.baidu.com/s?wd=paperuri:%282688d3af472c7a565f316b4ad1c75066%29&amp;filter=sc_long_sign&amp;sc_ks_para=q=Associative%20Dynamics%20in%20a%20Chaotic%20Neural%20Network&amp;tn=SE_baiduxueshu_c1gjeupa&amp;ie=utf-8&amp;sc_us=5114660386115963632" target="_blank" rel="noopener">Aihara K, Adachi M. Associative Dynamics in a Chaotic Neural Network</a>[J]. Neural Netw, 1997, 10(1):83-98.</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 混沌神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用sklearn实现文本多分类实验demo</title>
      <link href="/2018/09/03/%E5%88%A9%E7%94%A8sklearn%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BBdemo%20(1)/"/>
      <url>/2018/09/03/%E5%88%A9%E7%94%A8sklearn%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BBdemo%20(1)/</url>
      
        <content type="html"><![CDATA[<p>常见的文本分类中，二分类问题居多，多分类问题其实也挺常见的，这里简单给出一个多分类的实验demo。</p><h3 id="1-引入相应的库"><a href="#1-引入相应的库" class="headerlink" title="1 引入相应的库"></a>1 引入相应的库</h3><pre class=" language-lang-python"><code class="language-lang-python"># 引入必要的库import numpy as npimport matplotlib.pyplot as pltfrom itertools import cyclefrom sklearn import svm, datasetsfrom sklearn.metrics import roc_curve, aucfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import label_binarizefrom sklearn.multiclass import OneVsRestClassifierfrom scipy import interp% matplotlib inline</code></pre><h3 id="2-加载数据及数据格式转化"><a href="#2-加载数据及数据格式转化" class="headerlink" title="2 加载数据及数据格式转化"></a>2 加载数据及数据格式转化</h3><p>实验数据直接使用sklearn中的鸢尾花（iris）数据</p><h5 id="（1）-加载数据"><a href="#（1）-加载数据" class="headerlink" title="（1） 加载数据"></a>（1） 加载数据</h5><pre class=" language-lang-python"><code class="language-lang-python">iris = datasets.load_iris()X = iris.datay = iris.target</code></pre><h4 id="（2）-标签二值化"><a href="#（2）-标签二值化" class="headerlink" title="（2） 标签二值化"></a>（2） 标签二值化</h4><pre class=" language-lang-python"><code class="language-lang-python"># 查看原来标签数据格式print(y.shape)print(y)# 标签转化y = label_binarize(y, classes=[0, 1, 2])print(y[:3])</code></pre><pre><code>(150,)[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2][[1 0 0] [1 0 0] [1 0 0]]</code></pre><p>转化示意图<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/iris2.png" alt="标签转化"></p><h4 id="（3）划分训练集和测试集"><a href="#（3）划分训练集和测试集" class="headerlink" title="（3）划分训练集和测试集"></a>（3）划分训练集和测试集</h4><pre class=" language-lang-python"><code class="language-lang-python"># 设置种类n_classes = y.shape[1]# 训练模型并预测random_state = np.random.RandomState(0)n_samples, n_features = X.shape# 随机化数据，并划分训练数据和测试数据X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)</code></pre><h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3 训练模型"></a>3 训练模型</h3><pre class=" language-lang-python"><code class="language-lang-python"># Learn to predict each class against the othermodel = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,random_state=random_state))clt = model.fit(X_train, y_train)</code></pre><h3 id="4-性能评估"><a href="#4-性能评估" class="headerlink" title="4 性能评估"></a>4 性能评估</h3><h4 id="（1）分别在训练集和测试集上查看得分"><a href="#（1）分别在训练集和测试集上查看得分" class="headerlink" title="（1）分别在训练集和测试集上查看得分"></a>（1）分别在训练集和测试集上查看得分</h4><p>在训练集上查看分类得分</p><pre class=" language-lang-python"><code class="language-lang-python">clt.score(X_train, y_train)</code></pre><pre><code>0.8133333333333334</code></pre><p>在测试集上查看得分</p><pre class=" language-lang-python"><code class="language-lang-python">clt.score(X_test,y_test)</code></pre><pre><code>0.6533333333333333</code></pre><h4 id="（2）查看预测的各类别情况"><a href="#（2）查看预测的各类别情况" class="headerlink" title="（2）查看预测的各类别情况"></a>（2）查看预测的各类别情况</h4><p>①利用SVM的方法decision_function给每个样本中的每个类一个评分</p><pre class=" language-lang-python"><code class="language-lang-python">y_preds_scores=clt.decision_function(X_test)y_preds_scores[:5]</code></pre><pre><code>array([[-3.58459897, -0.3117717 ,  1.78242707],       [-2.15411929,  1.11394949, -2.393737  ],       [ 1.89199335, -3.89592195, -6.29685764],       [-4.52609987, -0.63396965,  1.96065819],       [ 1.39684192, -1.77722963, -6.26300472]])</code></pre><p>根据评分将其转化为原始标签格式</p><pre class=" language-lang-python"><code class="language-lang-python">np.argmax(clt.decision_function(X_test), axis=1)[:5]</code></pre><pre><code>array([2, 1, 0, 2, 0])</code></pre><p>②利用predict_proba查看每一类的预测概率</p><pre class=" language-lang-python"><code class="language-lang-python">clt.predict_proba(X_test)[:4]</code></pre><pre><code>array([[3.80289117e-03, 4.01872348e-01, 9.31103883e-01],       [4.57780355e-02, 7.88455913e-01, 3.39207219e-02],       [9.81843900e-01, 8.97766449e-03, 1.27447369e-04],       [7.34898836e-04, 3.12667406e-01, 9.45766977e-01]])</code></pre><pre class=" language-lang-python"><code class="language-lang-python">np.argmax(clt.predict_proba(X_test),axis=1)[:5]</code></pre><pre><code>array([2, 1, 0, 2, 0])</code></pre><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>【1】<a href="https://blog.csdn.net/xiaodongxiexie/article/details/76229042" target="_blank" rel="noopener">sklearn通过OneVsRestClassifier实现svm.SVC的多分类</a><br>【2】<a href="https://blog.csdn.net/BabyBirdToFly/article/details/72886879" target="_blank" rel="noopener">sklearn学习笔记（3）svm多分类</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras examples-imdb_cnn[利用卷积神经网络对文本进行分类]</title>
      <link href="/2018/08/28/Keras%20examples-imdb_cnn%E3%80%90%E5%88%A9%E7%94%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E3%80%91/"/>
      <url>/2018/08/28/Keras%20examples-imdb_cnn%E3%80%90%E5%88%A9%E7%94%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h3 id="1-任务描述"><a href="#1-任务描述" class="headerlink" title="1  任务描述"></a>1  任务描述</h3><p>本实验室利用卷积神经网络对imdb数据进行文本分类</p><h3 id="2-实验过程"><a href="#2-实验过程" class="headerlink" title="2 实验过程"></a>2 实验过程</h3><h4 id="（1）引入实验中所涉及到的包"><a href="#（1）引入实验中所涉及到的包" class="headerlink" title="（1）引入实验中所涉及到的包"></a>（1）引入实验中所涉及到的包</h4><p>数据集包、数据预处理包、网络模型包、网络各层结构所对应的包</p><pre class=" language-lang-python"><code class="language-lang-python">from __future__ import print_functionfrom keras.preprocessing import sequencefrom keras.models import Sequentialfrom keras.layers import Dense,Dropout,Activationfrom keras.layers import Embedding from keras.layers import Conv1D,GlobalMaxPooling1Dfrom keras.datasets import imdb</code></pre><h4 id="（2）设置网络结构中的一些常数"><a href="#（2）设置网络结构中的一些常数" class="headerlink" title="（2）设置网络结构中的一些常数"></a>（2）设置网络结构中的一些常数</h4><p>主要包含了两方面的常数：一是数据处理过程中，词向量维度，词汇表长度等相关的参数；二是网络结构中参数</p><pre class=" language-lang-python"><code class="language-lang-python"># 设置词汇表的长度，在数据预处理过程中，选择词汇字典中前max_features索引的词汇。max_features=5000# 将每个句子填充或截断至maxlen长度maxlen=400batch_size=32# 设置词向量的维度embedding_dims=50filters=250kernel_size=3# 设置全连接层中，神经元的个数hidden_dims=250epochs=2</code></pre><h4 id="（3）加载数据与数据预处理"><a href="#（3）加载数据与数据预处理" class="headerlink" title="（3）加载数据与数据预处理"></a>（3）加载数据与数据预处理</h4><pre class=" language-lang-python"><code class="language-lang-python">print("loading data...")(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)print(len(x_train),"train sequences")print(len(x_test),"test sequences")print("pad sequences (samples*time)")# 将每一条数据填充至相同的长度x_train=sequence.pad_sequences(x_train,maxlen=maxlen)x_test=sequence.pad_sequences(x_test,maxlen=maxlen)print("x_train.shape:",x_train.shape)print("x_test.shape:",x_test.shape)</code></pre><pre><code>loading data...25000 train sequences25000 test sequencespad sequences (samples*time)x_train.shape: (25000, 400)x_test.shape: (25000, 400)</code></pre><h4 id="（4）建立模型"><a href="#（4）建立模型" class="headerlink" title="（4）建立模型"></a>（4）建立模型</h4><pre class=" language-lang-python"><code class="language-lang-python">print("Build model...")# 采用序列模型model=Sequential()# 添加词嵌入层，词嵌入层只能作为神经网络的第一层model.add(Embedding(max_features,embedding_dims,input_length=maxlen))model.add(Dropout(0.2))model.add(Conv1D(filters,kernel_size,padding="valid",activation='relu',strides=1))# 使用maxpoolingmodel.add(GlobalMaxPooling1D())# 添加全连接层model.add(Dense(hidden_dims))model.add(Dropout(0.2))model.add(Activation('relu'))# 输出层model.add(Dense(1))model.add(Activation("sigmoid"))model.compile(loss="binary_crossentropy",optimizer='adam',metrics=['accuracy'])model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test))</code></pre><pre><code>Build model...Train on 25000 samples, validate on 25000 samplesEpoch 1/225000/25000 [==============================] - 64s 3ms/step - loss: 0.4040 - acc: 0.8005 - val_loss: 0.3088 - val_acc: 0.8657Epoch 2/225000/25000 [==============================] - 63s 3ms/step - loss: 0.2305 - acc: 0.9086 - val_loss: 0.2977 - val_acc: 0.8765</code></pre><h4 id="（5）网络结构"><a href="#（5）网络结构" class="headerlink" title="（5）网络结构"></a>（5）网络结构</h4><pre class=" language-lang-python"><code class="language-lang-python">from keras.utils import plot_modelplot_model(model,to_file="./imdb_cnn.png")</code></pre><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/keras04.png" alt="网络结构"></p><h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3 小结"></a>3 小结</h3><p>这次实验采用了卷积神经网络来对文本数据进行分类。而不是使用以往的RNN（LSTM/BiLSTM等），这样实验下来，对keras中其他结构的使用也进一步熟悉。在本实验中，主要有以下几点收获：</p><h4 id="（1）常用层的使用"><a href="#（1）常用层的使用" class="headerlink" title="（1）常用层的使用"></a>（1）常用层的使用</h4><p>常用层对应于core模块，其定义了一系列常用的网络层，在本次实验中主要用到了全连接层、激活层和Dropout层。这几层都比较简单，具体介绍见<a href="https://keras-cn.readthedocs.io/en/latest/layers/core_layer/" target="_blank" rel="noopener">https://keras-cn.readthedocs.io/en/latest/layers/core_layer/</a></p><h4 id="（2）一维卷积层"><a href="#（2）一维卷积层" class="headerlink" title="（2）一维卷积层"></a>（2）一维卷积层</h4><p>一维卷积即为时域卷积，用以在一维输入信号上进行邻域滤波。其原型为keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding=’valid’, dilation_rate=1, activation=None, use_bias=True, kernel_initializer=’glorot_uniform’, bias_initializer=’zeros’, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)<br>该层生成将输入信号与卷积核按照单一的空域（或时域）方向进行卷积。主要参数如下：<br>①filters：卷积核的数目（即输出的维度）<br>②kernel_size：整数或由单个整数构成的list/tuple，卷积核的空域或时域窗长度<br>③strides：整数或由单个整数构成的list/tuple，为卷积的步长。<br>④padding：补0策略，为“valid”, “same” 或“causal”，“causal”将产生因果（膨胀的）卷积，即output不依赖于input。<br>⑤activation：激活函数，为预定义的激活函数名。</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用nltk可视化stanford coreNLP构建的中文语法树</title>
      <link href="/2018/08/22/%E5%88%A9%E7%94%A8nltk%E5%8F%AF%E8%A7%86%E5%8C%96stanford%20coreNLP%E6%9E%84%E5%BB%BA%E7%9A%84%E4%B8%AD%E6%96%87%E8%AF%AD%E6%B3%95%E6%A0%91/"/>
      <url>/2018/08/22/%E5%88%A9%E7%94%A8nltk%E5%8F%AF%E8%A7%86%E5%8C%96stanford%20coreNLP%E6%9E%84%E5%BB%BA%E7%9A%84%E4%B8%AD%E6%96%87%E8%AF%AD%E6%B3%95%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>在stanford coreNLP的网页中直接以树的形式可视化了解析结果。但在IDE中，利用python调用coreNLP server后返回的是字符串格式。这是可以利用nltk中的Tree类来可视化解析结果。代码如下：</p><pre><code>from nltk.tree import Treefrom stanfordcorenlp import StanfordCoreNLPsentence = &#39;我叫小米&#39;with StanfordCoreNLP(r&#39;E:\ProgramData\Anaconda3\coreNLP\stanford-corenlp-full-2016-10-31&#39;, lang=&#39;zh&#39;) as nlp:    Tree.fromstring(nlp.parse(sentence)).draw()</code></pre><p>这里是通过stanfordcorenlp库来使用coreNLP的。<br>直接返回的结果是：</p><pre><code>(ROOT  (IP    (NP (PN 我))    (VP (VV 叫)      (NP (NN 小米)))))</code></pre><p>可视化：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ksh.png" alt="句法结构"><br>关于环境部署可见：<a href="https://blog.csdn.net/xyz1584172808/article/details/81940664" target="_blank" rel="noopener">两种在Python中使用Stanford CoreNLP的方法</a><br>当然这里也可以使用Stanford parser 来构建中文语法树，可视化方法类似。链接见：<a href="https://blog.csdn.net/baiyi_canggou/article/details/59056759" target="_blank" rel="noopener">NLTK中使用Stanford parser 构建中文语法树</a></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 句法分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两种在Python中使用Stanford CoreNLP的方法</title>
      <link href="/2018/08/22/%E4%B8%A4%E7%A7%8D%E5%9C%A8Python%E4%B8%AD%E4%BD%BF%E7%94%A8Stanford%20CoreNLP%E7%9A%84%E6%96%B9%E6%B3%95/"/>
      <url>/2018/08/22/%E4%B8%A4%E7%A7%8D%E5%9C%A8Python%E4%B8%AD%E4%BD%BF%E7%94%A8Stanford%20CoreNLP%E7%9A%84%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>这两种方法都需要提前下载CoreNLP最新的压缩包，再下载对应的语言jar包。<a href="https://stanfordnlp.github.io/CoreNLP/index.html#download" target="_blank" rel="noopener">从CoreNLP下载页面下载</a>。将压缩包解压得到目录，再将语言的jar包放到这个目录下即可。并且要求java -version&gt;=1.8。<br>接下来就是利用python对该工具的使用进行一个封装，这里主要给出两种常用的python wrapper。</p><h3 id="1、使用stanfordcorenlp接口"><a href="#1、使用stanfordcorenlp接口" class="headerlink" title="1、使用stanfordcorenlp接口"></a>1、使用stanfordcorenlp接口</h3><p>见链接：<a href="https://blog.csdn.net/qq_35203425/article/details/80451243" target="_blank" rel="noopener">Python中使用Stanford CoreNLP</a><br>这篇文档中将这种方法讲得很清楚，因此直接贴上链接，访问原文即可、、</p><h3 id="2、使用官方Python接口python-stanford-corenlp"><a href="#2、使用官方Python接口python-stanford-corenlp" class="headerlink" title="2、使用官方Python接口python-stanford-corenlp"></a>2、使用官方Python接口python-stanford-corenlp</h3><p>见链接：<a href="https://blog.csdn.net/thriving_fcl/article/details/76595253?locationNum=4&amp;fps=1" target="_blank" rel="noopener">CoreNLP Python接口处理中文</a><br>在这篇文章中，作者是用的linux系统，并且拥有root权限。我这里提供另外两种不同环境的配置过程。</p><h4 id="1-windows-10-环境"><a href="#1-windows-10-环境" class="headerlink" title="(1)windows 10 环境"></a>(1)windows 10 环境</h4><p>在该环境下，配置环境变量与上文中的方法有一些不一样。具体方法是：<br>电脑右键-&gt;属性-&gt;高级属性-&gt;环境变量-&gt;新增-&gt;添加变量名javanlp，路径值为之前解压coreNLP得到的目录。</p><h4 id="2-linux-下不具有root权限的环境"><a href="#2-linux-下不具有root权限的环境" class="headerlink" title="(2)linux 下不具有root权限的环境"></a>(2)linux 下不具有root权限的环境</h4><p>在这种环境下，只需要在原文修改client.py里面初始化部分的代码的基础上再修改下即可。（路径：python-stanford-corenlp\corenlp\client.py）</p><pre><code># assert os.getenv(#     &quot;JAVANLP_HOME&quot;) is not None, &quot;Please define $JAVANLP_HOME where your CoreNLP Java checkout is&quot;start_cmd = &#39;java -Xmx{memory}g -cp &quot;{javanlp}/*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port {port} -timeout {timeout}&#39;.format(                memory=allocate_mem,                javanlp=&quot;/home/xiongzy/anaconda3/coreNLP/stanford-corenlp-full-2016-10-31&quot;,</code></pre><p>注释掉：</p><pre><code># assert os.getenv(#     &quot;JAVANLP_HOME&quot;) is not None, &quot;Please define $JAVANLP_HOME where your CoreNLP Java checkout is&quot;</code></pre><p>将javanlp改为具体的值：</p><pre><code>javanlp=&quot;/home/xiongzy/anaconda3/coreNLP/stanford-corenlp-full-2016-10-31&quot;</code></pre><p>然后进入python-stanford-corenlp目录重新重新安装即可python setup.py install</p><h3 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h3><h4 id="（1）关闭server"><a href="#（1）关闭server" class="headerlink" title="（1）关闭server"></a>（1）关闭server</h4><p>每次第一种方法的代码中，每次使用后需要手动关闭server，nlp.close()。<br>第二种方法不用手动关闭，其自动关闭</p><h4 id="（2）利用网页进行分析"><a href="#（2）利用网页进行分析" class="headerlink" title="（2）利用网页进行分析"></a>（2）利用网页进行分析</h4><p>当server处于开启状态时，可以通过ip:port(如：localhost:9000)打开网页版的分析器。效果如图。<br><img src="https://img-blog.csdn.net/20180822111913416?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="Stanford coreNLP"></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 实验记录 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROC原理介绍及基于python实现二分类和多分类的ROC曲线</title>
      <link href="/2018/08/19/ROC%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%88%A9%E7%94%A8python%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E7%B1%BB%E5%92%8C%E5%A4%9A%E5%88%86%E7%B1%BB%E7%9A%84ROC%E6%9B%B2%E7%BA%BF%20(1)/"/>
      <url>/2018/08/19/ROC%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%88%A9%E7%94%A8python%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E7%B1%BB%E5%92%8C%E5%A4%9A%E5%88%86%E7%B1%BB%E7%9A%84ROC%E6%9B%B2%E7%BA%BF%20(1)/</url>
      
        <content type="html"><![CDATA[<p>对于分类器，或者说分类算法，评价指标主要有precision，recall，F-score1，以及即将要讨论的ROC和AUC。本文通过对这些指标的原理做一个简单的介绍，然后用python分别实现二分类和多分类的ROC曲线。<br><a id="more"></a></p><h3 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1  基本概念"></a>1  基本概念</h3><p>一个分类模型(分类器)是一个将某个实例映射到一个特定类的过程.分类器的结果可以是给出该实例所属的类别，也可以给定该实例属于某个类别的概率。<br>首先来考虑一个两类预测问题(双分类器),其结果要么是真(p)要么是假(n).在双分类器中有4类可能的输出.如果输出的预测是p而真实的结果也是p,那么这就叫做真阳性(TP);然而如果真实的结果是n,则这就叫做假阳性(FP).相反的来说,一个真阴性发生在预测结果和实际结果都为n的时候,而假阴性是当预测输出是n而实际值是p的时候，这几种情况可以用下面的矩阵来组织。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/roc1.png" alt="分类情况"><br>举一个现实世界中的恰当的例子,考虑一个检测一个人是否得一种病的测试.一个假阳性就是一个人被测试是有这种病的,但实际却没有的情况.一个假阴性就是一个人被测试是健康的,但实际却是得病的情况。<br>上面那个矩阵就称为混淆矩阵：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/fpr-and-tpr.png" alt="混淆矩阵"><br>由图中公式可知各个指标的含义：<br><strong>precision</strong>：预测为对的当中，原本为对的比例（越大越好，1为理想状态）<br><strong>recall：</strong>原本为对的当中，预测为对的比例（越大越好，1为理想状态）<br><strong>F-measure</strong>：F度量是对准确率和召回率做一个权衡（越大越好，1为理想状态，此时precision为1，recall为1）<br><strong>accuracy</strong>：预测对的（包括原本是对预测为对，原本是错的预测为错两种情形）占整个的比例（越大越好，1为理想状态）<br><strong>fp rate</strong>：原本是错的预测为对的比例（越小越好，0为理想状态）<br><strong>tp rate</strong>：原本是对的预测为对的比例（越大越好，1为理想状态）<br>在了解了上述的一些指标的含义以及计算公式后，接下来就可以进入ROC曲线了。<br>如下示例图中，曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/Roccurves.png" alt="ROC曲线示例"><br>要生成一个ROC曲线,只需要真阳性率(TPR)和假阳性率(FPR)。TPR决定了一个分类器或者一个诊断测试在所有阳性样本中能正确区分的阳性案例的性能.而FPR是决定了在所有阴性的样本中有多少假阳性的判断. ROC曲线中分别将FPR和TPR定义为x和y轴,这样就描述了真阳性(获利)和假阳性(成本)之间的博弈.而TPR就可以定义为灵敏度,而FPR就定义为1-特异度,因此ROC曲线有时候也叫做灵敏度和1-特异度图像.每一个预测结果在ROC曲线中以一个点代表.<br>有了ROC曲线后，可以引出AUC的含义：ROC曲线下的面积（越大越好，1为理想状态）</p><h3 id="2-ROC曲线图中的特殊点和线"><a href="#2-ROC曲线图中的特殊点和线" class="headerlink" title="2  ROC曲线图中的特殊点和线"></a>2  ROC曲线图中的特殊点和线</h3><h4 id="（1）ROC曲线图中的四个点"><a href="#（1）ROC曲线图中的四个点" class="headerlink" title="（1）ROC曲线图中的四个点"></a>（1）ROC曲线图中的四个点</h4><p>第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，，ROC曲线越接近左上角，该分类器的性能越好。</p><h4 id="（2）ROC曲线图中的一条特殊线"><a href="#（2）ROC曲线图中的一条特殊线" class="headerlink" title="（2）ROC曲线图中的一条特殊线"></a>（2）ROC曲线图中的一条特殊线</h4><p>考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。</p><h3 id="3-如何画ROC曲线"><a href="#3-如何画ROC曲线" class="headerlink" title="3  如何画ROC曲线"></a>3  如何画ROC曲线</h3><p>对于一个特定的分类器和测试数据集，每一个实例都会得到一个分类结果，通过统计，利用上述公式，可以得到一组FPR和TPR结果，而要得到一个曲线，实际上需要一系列FPR和TPR的值。那么这一系列值是怎么构造出来的呢？<br>在ROC曲线的定义中，有“as its discrimination threshold is varied.”这样一句话，而在本文最开始也提到了分类器的结果可以是“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本），于是如果设置不同的threshold，那么分类结果就会有所变动，因此可以得到一系列FPR和TPR的值。<br>接下来就是利用python实现ROC曲线，sklearn.metrics有roc_curve, auc两个函数，本文主要就是通过这两个函数实现二分类和多分类的ROC曲线。</p><pre><code>fpr, tpr, thresholds  =  roc_curve(y_test, scores)</code></pre><p>其中y_test为测试集的结果，scores为模型预测的测试集得分（注意：通过decision_function(x_test)计算scores的值）；fpr,tpr,thresholds 分别为假正率、真正率和阈值。（应该是不同阈值下的真正率和假正率）。</p><pre><code>roc_auc =auc(fpr, tpr)</code></pre><p>其中roc_auc为计算的acu的值。</p><h4 id="（1）二分类问题中的ROC曲线"><a href="#（1）二分类问题中的ROC曲线" class="headerlink" title="（1）二分类问题中的ROC曲线"></a>（1）二分类问题中的ROC曲线</h4><p>本实例中的数据来源于sklearn中的鸢尾花（iris）数据，代码来源于LZ_Zack的博客，在最后参考链接里面会给出原博客的链接。</p><pre><code># -*- coding: utf-8 -*-import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svm, datasetsfrom sklearn.metrics import roc_curve, auc  ###计算roc和aucfrom sklearn import cross_validation# Import some data to play withiris = datasets.load_iris()X = iris.datay = iris.target##变为2分类X, y = X[y != 2], y[y != 2]# Add noisy features to make the problem harderrandom_state = np.random.RandomState(0)n_samples, n_features = X.shapeX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]# shuffle and split training and test setsX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=.3,random_state=0)# Learn to predict each class against the othersvm = svm.SVC(kernel=&#39;linear&#39;, probability=True,random_state=random_state)###通过decision_function()计算得到的y_score的值，用在roc_curve()函数中y_score = svm.fit(X_train, y_train).decision_function(X_test)# Compute ROC curve and ROC area for each classfpr,tpr,threshold = roc_curve(y_test, y_score) ###计算真正率和假正率roc_auc = auc(fpr,tpr) ###计算auc的值plt.figure()lw = 2plt.figure(figsize=(10,10))plt.plot(fpr, tpr, color=&#39;darkorange&#39;,         lw=lw, label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc) ###假正率为横坐标，真正率为纵坐标做曲线plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;)plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#39;False Positive Rate&#39;)plt.ylabel(&#39;True Positive Rate&#39;)plt.title(&#39;Receiver operating characteristic example&#39;)plt.legend(loc=&quot;lower right&quot;)plt.show()</code></pre><p>该实例的ROC图如下所示：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/roc2.png" alt="二分类的ROC曲线"></p><h4 id="（2）多分类问题中的ROC曲线"><a href="#（2）多分类问题中的ROC曲线" class="headerlink" title="（2）多分类问题中的ROC曲线"></a>（2）多分类问题中的ROC曲线</h4><p>对于多分类问题，ROC曲线的获取主要有两种方法：<br>假设测试样本个数为m，类别个数为n。在训练完成后，计算出每个测试样本的在各类别下的概率或置信度，得到一个[m， n]形状的矩阵P，每一行表示一个测试样本在各类别下概率值（按类别标签排序）。相应地，将每个测试样本的标签转换为类似二进制的形式，每个位置用来标记是否属于对应的类别（也按标签排序，这样才和前面对应），由此也可以获得一个[m， n]的标签矩阵L。<br>①方法一：每种类别下，都可以得到m个测试样本为该类别的概率（矩阵P中的列）。所以，根据概率矩阵P和标签矩阵L中对应的每一列，可以计算出各个阈值下的假正例率（FPR）和真正例率（TPR），从而绘制出一条ROC曲线。这样总共可以绘制出n条ROC曲线。最后对n条ROC曲线取平均，即可得到最终的ROC曲线。<br>②方法二：<br>首先，对于一个测试样本：1）标签只由0和1组成，1的位置表明了它的类别（可对应二分类问题中的‘’正’’），0就表示其他类别（‘’负‘’）；2）要是分类器对该测试样本分类正确，则该样本标签中1对应的位置在概率矩阵P中的值是大于0对应的位置的概率值的。基于这两点，将标签矩阵L和概率矩阵P分别按行展开，转置后形成两列，这就得到了一个二分类的结果。所以，此方法经过计算后可以直接得到最终的ROC曲线。<br>上面的两个方法得到的ROC曲线是不同的，当然曲线下的面积AUC也是不一样的。 在python中，方法1和方法2分别对应sklearn.metrics.roc_auc_score函数中参数average值为’macro’和’micro’的情况。下面参考sklearn官网提供的例子，对两种方法进行实现。</p><pre><code># 引入必要的库import numpy as npimport matplotlib.pyplot as pltfrom itertools import cyclefrom sklearn import svm, datasetsfrom sklearn.metrics import roc_curve, aucfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import label_binarizefrom sklearn.multiclass import OneVsRestClassifierfrom scipy import interp# 加载数据iris = datasets.load_iris()X = iris.datay = iris.target# 将标签二值化y = label_binarize(y, classes=[0, 1, 2])# 设置种类n_classes = y.shape[1]# 训练模型并预测random_state = np.random.RandomState(0)n_samples, n_features = X.shape# shuffle and split training and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)# Learn to predict each class against the otherclassifier = OneVsRestClassifier(svm.SVC(kernel=&#39;linear&#39;, probability=True,                                 random_state=random_state))y_score = classifier.fit(X_train, y_train).decision_function(X_test)# 计算每一类的ROCfpr = dict()tpr = dict()roc_auc = dict()for i in range(n_classes):    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])    roc_auc[i] = auc(fpr[i], tpr[i])# Compute micro-average ROC curve and ROC area（方法二）fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])# Compute macro-average ROC curve and ROC area（方法一）# First aggregate all false positive ratesall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))# Then interpolate all ROC curves at this pointsmean_tpr = np.zeros_like(all_fpr)for i in range(n_classes):    mean_tpr += interp(all_fpr, fpr[i], tpr[i])# Finally average it and compute AUCmean_tpr /= n_classesfpr[&quot;macro&quot;] = all_fprtpr[&quot;macro&quot;] = mean_tprroc_auc[&quot;macro&quot;] = auc(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;])# Plot all ROC curveslw=2plt.figure()plt.plot(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;],         label=&#39;micro-average ROC curve (area = {0:0.2f})&#39;               &#39;&#39;.format(roc_auc[&quot;micro&quot;]),         color=&#39;deeppink&#39;, linestyle=&#39;:&#39;, linewidth=4)plt.plot(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;],         label=&#39;macro-average ROC curve (area = {0:0.2f})&#39;               &#39;&#39;.format(roc_auc[&quot;macro&quot;]),         color=&#39;navy&#39;, linestyle=&#39;:&#39;, linewidth=4)colors = cycle([&#39;aqua&#39;, &#39;darkorange&#39;, &#39;cornflowerblue&#39;])for i, color in zip(range(n_classes), colors):    plt.plot(fpr[i], tpr[i], color=color, lw=lw,             label=&#39;ROC curve of class {0} (area = {1:0.2f})&#39;             &#39;&#39;.format(i, roc_auc[i]))plt.plot([0, 1], [0, 1], &#39;k--&#39;, lw=lw)plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#39;False Positive Rate&#39;)plt.ylabel(&#39;True Positive Rate&#39;)plt.title(&#39;Some extension of Receiver operating characteristic to multi-class&#39;)plt.legend(loc=&quot;lower right&quot;)plt.show()</code></pre><p>画出的ROC曲线如下图所示。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/roc3.png" alt="多分类的ROC曲线"><br>如果是只需画出具体的某一类的ROC曲线，只需要修改下画图代码即可。</p><pre><code>plt.figure()lw = 2plt.plot(fpr[2], tpr[2], color=&#39;darkorange&#39;,         lw=lw, label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc[2])plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;)plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#39;False Positive Rate&#39;)plt.ylabel(&#39;True Positive Rate&#39;)plt.title(&#39;Receiver operating characteristic example&#39;)plt.legend(loc=&quot;lower right&quot;)plt.show()</code></pre><p>画出的ROC曲线如下图所示。<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/roc4.png" alt="ROC曲线"></p><h3 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4  参考文献"></a>4  参考文献</h3><p>【1】(Fawcett, 2006)，Fawcett, T. (2006). An introduction to ROC analysis. Pattern recognition letters, 27(8), 861-874.<br>【2】Davis, J., &amp; Goadrich, M. (2006, June). The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd international conference on Machine learning (pp. 233-240). ACM.<br>【3】<a href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/" target="_blank" rel="noopener">ROC和AUC介绍以及如何计算AUC</a><br>【4】<a href="https://www.cnblogs.com/haoguoeveryone/p/haoguo_5.html" target="_blank" rel="noopener">ROC曲线、AUC、Precision、Recall、F-measure理解及Python实现</a><br>【5】<a href="https://www.cnblogs.com/linkr/articles/2317072.html" target="_blank" rel="noopener">ROC曲线</a><br>【6】<a href="https://blog.csdn.net/YE1215172385/article/details/79443552" target="_blank" rel="noopener">多分类下的ROC曲线和AUC</a><br>【7】<a href="https://blog.csdn.net/lz_peter/article/details/78054914" target="_blank" rel="noopener">用Python画ROC曲线</a></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 分类指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras examples-imdb_bidirectional_lstm</title>
      <link href="/2018/08/16/Keras%20examples-imdb_bidirectional_lstm%20(1)/"/>
      <url>/2018/08/16/Keras%20examples-imdb_bidirectional_lstm%20(1)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-任务描述"><a href="#1-任务描述" class="headerlink" title="1  任务描述"></a>1  任务描述</h3><p>本实验是训练一个双向LSTM，并在IMDB数据集上完成情感分类任务</p><h3 id="2-具体实现"><a href="#2-具体实现" class="headerlink" title="2 具体实现"></a>2 具体实现</h3><h4 id="（1）引入必要的包"><a href="#（1）引入必要的包" class="headerlink" title="（1）引入必要的包"></a>（1）引入必要的包</h4><pre class=" language-lang-python"><code class="language-lang-python">from __future__ import print_functionimport numpy as npfrom keras.preprocessing import sequencefrom keras.models import Sequentialfrom keras.layers import Dense,Dropout,Embedding,LSTM,Bidirectionalfrom keras.datasets import imdb</code></pre><h4 id="（2）设置一些常量"><a href="#（2）设置一些常量" class="headerlink" title="（2）设置一些常量"></a>（2）设置一些常量</h4><pre class=" language-lang-python"><code class="language-lang-python"># 设置最大特征的数量，对于文本，就是处理的最大单词数量。若被设置为整数，则被限制为待处理数据集中最常见的max_features个单词max_features=20000# 设置每个文本序列的最大长度，当序列的长度小于maxlen时，将用0来进行填充，当序列的长度大于maxlen时，则进行截断maxlen=100# 设置训练的轮次batch_size=32</code></pre><h4 id="（3）加载数据"><a href="#（3）加载数据" class="headerlink" title="（3）加载数据"></a>（3）加载数据</h4><p>在Keras的数据集中有IMDB数据集，因此只需要按照格式加载即可</p><pre class=" language-lang-python"><code class="language-lang-python">print("loading data ...")# 加载数据(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)# 查看数据大小print(len(x_train),'train sequences')print(len(x_test),'test sequences')</code></pre><pre><code>loading data ...25000 train sequences25000 test sequences</code></pre><h4 id="（4）数据处理"><a href="#（4）数据处理" class="headerlink" title="（4）数据处理"></a>（4）数据处理</h4><p>首先查看数据格式</p><pre class=" language-lang-python"><code class="language-lang-python">print(x_train[0])</code></pre><pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]</code></pre><p>可以看出，每一条文本都已经处理成了序列形式。在序列中，每一个单词都用其字典中对应的索引表示。因此这里省去了很多文本基本预处理的步骤。<br>接下来查看标签数据格式</p><pre class=" language-lang-python"><code class="language-lang-python">print(y_train[:10])</code></pre><pre><code>[1 0 0 1 0 0 1 0 1 0]</code></pre><p>看来是极性情感分类，积极，消极两种情感。</p><p>由于训练集中的文本已经表示成了序列模型，因此只需要利用pad_sequences将每个序列整理成相同长度即可。</p><pre class=" language-lang-python"><code class="language-lang-python">print('Pad sequences (samples x time)')# 将文本序列处理成长度相同的序列x_train = sequence.pad_sequences(x_train, maxlen=maxlen)x_test = sequence.pad_sequences(x_test, maxlen=maxlen)print('x_train shape:', x_train.shape)print('x_test shape:', x_test.shape)y_train = np.array(y_train)y_test = np.array(y_test)</code></pre><pre><code>Pad sequences (samples x time)x_train shape: (25000, 100)x_test shape: (25000, 100)</code></pre><h4 id="（5）创建模型及训练"><a href="#（5）创建模型及训练" class="headerlink" title="（5）创建模型及训练"></a>（5）创建模型及训练</h4><pre class=" language-lang-python"><code class="language-lang-python"># 创建网络结构model=Sequential()model.add(Embedding(max_features,128,input_length=maxlen))model.add(Bidirectional(LSTM(64)))model.add(Dropout(0.5))model.add(Dense(1,activation='sigmoid'))# 编译模型model.compile('adam','binary_crossentropy',metrics=['accuracy'])# 训练模型print('Train...')model.fit(x_train, y_train,batch_size=batch_size,epochs=4,validation_data=[x_test, y_test])</code></pre><pre><code>Train...Train on 25000 samples, validate on 25000 samplesEpoch 1/425000/25000 [==============================] - 72s 3ms/step - loss: 0.4298 - acc: 0.8014 - val_loss: 0.3477 - val_acc: 0.8498Epoch 2/425000/25000 [==============================] - 71s 3ms/step - loss: 0.2295 - acc: 0.9122 - val_loss: 0.4055 - val_acc: 0.8426Epoch 3/425000/25000 [==============================] - 71s 3ms/step - loss: 0.1345 - acc: 0.9510 - val_loss: 0.4588 - val_acc: 0.8368Epoch 4/425000/25000 [==============================] - 71s 3ms/step - loss: 0.0791 - acc: 0.9731 - val_loss: 0.5615 - val_acc: 0.8232</code></pre><h4 id="（6）可视化网络结构"><a href="#（6）可视化网络结构" class="headerlink" title="（6）可视化网络结构"></a>（6）可视化网络结构</h4><pre class=" language-lang-python"><code class="language-lang-python">from keras.utils import plot_modelplot_model(model,to_file="./imdb_bidirectional_lstm.png")</code></pre><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/imdb_bidirectional_lstm.png" alt="网络结构"></p><h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3 小结"></a>3 小结</h3><p>大概半个月没写这一类代码了，有些方法的使用有点模糊了，通过这个例子，可以快速的回忆起相关的知识点来，于是也把相关的知识点又重新整理了下。在本例子中，主要的知识点有以下几点：</p><h4 id="（1）填充序列pad-sequences"><a href="#（1）填充序列pad-sequences" class="headerlink" title="（1）填充序列pad_sequences"></a>（1）填充序列pad_sequences</h4><p>方法原型：<br>keras.preprocessing.sequence.pad_sequences(sequences,maxlen=None, dtype=’int32’,padding=’pre’, truncating=’pre’, value=0.)<br>其主要功能就是将sequences列表中的每个序列填充或截断成maxlen的长度，填充的位置由padding参数决定，截断的位置由truncating参数决定。</p><h4 id="（2）嵌入层-Embedding"><a href="#（2）嵌入层-Embedding" class="headerlink" title="（2）嵌入层 Embedding"></a>（2）嵌入层 Embedding</h4><p>方法原型：<br>keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer=’uniform’, embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)<br>其主要功能是将正整数（下标）转化为具有固定大小的向量。该层只能作为模型的第一层。<br>参数：<br>①input_dim：字典长度，即输入数据的最大下标+1，针对本实验，由于在读取数据时，就限制了所要保留的单词个数为max_features个，因此，本实验室的input_dim=max_features<br>②output_dim：代表全连接嵌入的维度，通俗一点就是词向量的维度。<br>③input_length：当输入序列的长度固定是，该值为其长度。也就是每个序列的长度（由于一般情况下都会将所有序列的长度填充成相同长度了）如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。</p><h4 id="（3）文本预处理-分词器Tokenizer"><a href="#（3）文本预处理-分词器Tokenizer" class="headerlink" title="（3）文本预处理-分词器Tokenizer"></a>（3）文本预处理-分词器Tokenizer</h4><p>尽管本实验中没有涉及到文本预处理部分，但是对于和文本打交道，这些文本预处理方法是必须掌握的，于是这里也一并贴出来。<br>利用Keras提供的文本预处理方法貌似非常方便，可以简单的归纳为四步：<br>①创建分词器：tokenizer=Tokenizer(filters=’!”#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~\t\n’,lower=True,split=” “)<br>②用文本列表来训练分词器：tokenizer.fit_on_texts(X)<br>③建立词汇表，并为每个单词或字符串创建对应的索引：vocab=tokenizer.word_index<br>④将训练集文本和测试集文本都映射成单词id组成的序列：tokenizer.texts_to_sequences(x_train)<br>Tokenizer是一个用于向量化文本，或将文本转换为序列的类。能过满足文本处理需求，具体可以见官方文档：<a href="https://keras-cn.readthedocs.io/en/latest/preprocessing/text/" target="_blank" rel="noopener">https://keras-cn.readthedocs.io/en/latest/preprocessing/text/</a></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras examples-babi_rnn</title>
      <link href="/2018/07/24/Keras%20examples-babi_rnn%20(1)/"/>
      <url>/2018/07/24/Keras%20examples-babi_rnn%20(1)/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务描述"><a href="#1-任务描述" class="headerlink" title="1  任务描述"></a>1  任务描述</h2><p>本实验利用提供的20个种类的数据集，完成KQA的任务。</p><h2 id="2-具体实现"><a href="#2-具体实现" class="headerlink" title="2 具体实现"></a>2 具体实现</h2><p>任务整体流程如图所示：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/babi_rnn_1.png" alt="任务整体流程"></p><h3 id="（1）引入必要的包"><a href="#（1）引入必要的包" class="headerlink" title="（1）引入必要的包"></a>（1）引入必要的包</h3><pre class=" language-lang-python"><code class="language-lang-python">from __future__ import print_functionfrom functools import reduceimport reimport tarfile # 处理压缩文件import numpy as npfrom keras.utils import plot_modelfrom keras.utils.data_utils import get_filefrom keras.layers.embeddings import Embeddingfrom keras import layersfrom keras.layers import recurrentfrom keras.models import Modelfrom keras.preprocessing.sequence import pad_sequences</code></pre><h3 id="（2）设置一些网络结构常量"><a href="#（2）设置一些网络结构常量" class="headerlink" title="（2）设置一些网络结构常量"></a>（2）设置一些网络结构常量</h3><pre class=" language-lang-python"><code class="language-lang-python">RNN=recurrent.LSTMEMBED_HIDDEN_SIZE=50SENT_HIDDEN_SIZE=100QUERY_HIDDEN_SIZE=100BATCH_SIZE=32EPOCHS=40print("RNN/Embed/Sent/Query={},{},{},{}".format(RNN,EMBED_HIDDEN_SIZE,SENT_HIDDEN_SIZE,QUERY_HIDDEN_SIZE))</code></pre><h3 id="（3）下载数据集"><a href="#（3）下载数据集" class="headerlink" title="（3）下载数据集"></a>（3）下载数据集</h3><p>数据集的获取，可以直接通过get_file()方法下载，也可以通过<a href="https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz" target="_blank" rel="noopener">https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz</a> 网址提前下载，然后获取文件路径即可。这里对get_file()方法做简单介绍（提前下载好的文件也可以通过该方法来加载）<br><strong>get_file()方法：</strong>从给定的URL中下载文件, 可以传递MD5值用于数据校验(下载后或已经缓存的数据均可)<br><strong>fname:</strong> 文件名，如果指定了绝对路径/path/to/file.txt,则文件将会保存到该位置。<br><strong>origin:</strong> 文件的URL地址<br><strong>返回：</strong>下载后的文件地址<br>关于该方法的更多参数说明：<a href="http://keras-cn.readthedocs.io/en/latest/utils/#get_file" target="_blank" rel="noopener">http://keras-cn.readthedocs.io/en/latest/utils/#get_file</a></p><pre class=" language-lang-python"><code class="language-lang-python"># 从网络上获取数据集try:    path = get_file('babi-tasks-v1-2.tar.gz',                    origin='https://s3.amazonaws.com/text-datasets/'                           'babi_tasks_1-20_v1-2.tar.gz')except:    print('Error downloading dataset, please download it manually:\n'          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'          '.tar.gz\n'          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')    raise</code></pre><p>数据集中一共有20类任务数据，每一类问题又提供了两种不同数量级大小的数据集，1000个问题（默认）和10K个问题，20类问题如下：<br>QA1 - Single Supporting Fact<br>QA2 - Two Supporting Facts<br>QA3 - Three Supporting Facts<br>QA4 - Two Arg. Relations<br>QA5 - Three Arg. Relations<br>QA6 - yes/No Questions<br>QA7 - Counting<br>QA8 - Lists/Sets<br>QA9 - Simple Negation<br>QA10 - Indefinite Knowledge<br>QA11 - Basic Coreference<br>QA12 - Conjunction<br>QA13 - Compound Coreference<br>QA14 - Time Reasoning<br>QA15 - Basic Deduction<br>QA16 - Basic Induction<br>QA17 - Positional Reasoning<br>QA18 - Size Reasoning<br>QA19 - Path Finding<br>QA20 - Agent’s Motivations  </p><p>打开某个数据集的txt文件，再来看看具体的数据具体内容：<br>1 John travelled to the hallway.<br>2 Mary journeyed to the bathroom.<br>3 Where is John?     hallway    1<br>4 Daniel went back to the bathroom.<br>5 John moved to the bedroom.<br>6 Where is Mary?     bathroom    2<br>7 John went to the hallway.<br>8 Sandra journeyed to the kitchen.<br>9 Where is Sandra?     kitchen    8<br>10 Sandra travelled to the hallway.<br>11 John went to the garden.<br>12 Where is Sandra?     hallway    10<br>13 Sandra went back to the bathroom.<br>14 Sandra moved to the kitchen.<br>15 Where is Sandra?     kitchen    14<br>这是一个故事的具体内容（长度不一定为15），从中可以看到，每一行数据中包含了编号和文本，而文本可能是陈述句文本（这里记为事实类文本），也可以是“问句\t答案\t支撑答案的行所在编号（可能不止1个）”这种格式的（这里记为问题类文本）。</p><h3 id="（4）获取训练集和测试集"><a href="#（4）获取训练集和测试集" class="headerlink" title="（4）获取训练集和测试集"></a>（4）获取训练集和测试集</h3><p>数据集下载下来后可以解压后读入，这里使用的是tarfile模块，linux上常用tarfile模块来处理tar文件，无论tar文件是否被压缩还是仅仅被打包，都可以读取和写入tar文件，这里涉及到的方法为open()和extractfile(),<br>其中：<br><strong>①open()：</strong>除了指出打开文件的方式以外还指出了文件的压缩方式。通过filemode[:compression]的方式可以指出很多种文件模式(比如’r:gz’表示读打开，使用gzip压缩文件)<br><strong>②extractfile()：</strong>从tar包中提取一个子文件，但返回的是个类文件对象，可以通过read，write等方法来操作文件的内容<br>更多资料见：<a href="https://www.cnblogs.com/franknihao/p/6613236.html" target="_blank" rel="noopener">https://www.cnblogs.com/franknihao/p/6613236.html</a></p><pre class=" language-lang-python"><code class="language-lang-python"># 构造需要的数据集格式# Default QA1 with 1000 samples# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'# QA1 with 10,000 samples# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'# QA2 with 1000 sampleschallenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'# QA2 with 10,000 samples# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'with tarfile.open(path) as tar:    # 从压缩文件中获取tasks_1-20_v1-2/en/路径下的qa2_two-supporting-facts_train.txt文件，返回的是一个类文件对象，可以通过read，write等方法来操作文件的内容，此处将该对象传递给get_stories()方法    train = get_stories(tar.extractfile(challenge.format('train')))    # 同理，获取tasks_1-20_v1-2/en/路径下的qa2_two-supporting-facts_train.txt文件    test=get_stories(tar.extractfile(challenge.format("test")))</code></pre><h3 id="（5）数据处理过程中所用到的一些方法"><a href="#（5）数据处理过程中所用到的一些方法" class="headerlink" title="（5）数据处理过程中所用到的一些方法"></a>（5）数据处理过程中所用到的一些方法</h3><p>在获取数据集后，需要将原始数据构建成我们能够使用的数据，其中包括，按照要求构建支撑文本，对支撑文本和问题进行分词，以及向量化操作。</p><h4 id="①分词方法"><a href="#①分词方法" class="headerlink" title="①分词方法"></a>①分词方法</h4><pre class=" language-lang-python"><code class="language-lang-python"># 分词def tokenize(sent):    # 返回分词后所形成的列表，保留了标点符号    # 采用正则来分词，对于正则匹配的每一个字符串，如果该字符串去除左右的空白符以后不为空，则将其保留下来    return [x.strip() for x in re.split("(\w+)?",sent) if x.strip()]</code></pre><h4 id="②将原始数据集构建成（story-question-answer）三元组"><a href="#②将原始数据集构建成（story-question-answer）三元组" class="headerlink" title="②将原始数据集构建成（story,question,answer）三元组"></a>②将原始数据集构建成（story,question,answer）三元组</h4><pre class=" language-lang-python"><code class="language-lang-python">def parse_stories(lines,only_supporting=False):    data = []    story = []    for line in lines:        # 对于读入的每一行，将其解码成utf-8格式，并去掉前后空白符        line = line.decode('utf-8').strip()        # 将该行以空格为分割符进行切割，切割一次，只需要将序号和故事内容分隔开即可。        # str.split(str="", num=string.count(str)) 其中num指定分割次数        nid, line = line.split(' ', 1)        nid = int(nid)        # 如果当前行的编号为1的话，则接下来是一个新故事的开始，首先将当前故事列表清空        if nid == 1:            story = []        # 如果这一行中包含\t，则说明是问题类文本，处理支撑文本，问题，答案，并将其加入结果list中        if '\t' in line:            # 将改行文本通过\t分割，分别获取，问题、答案、支撑行编号（可能不止一个）            q, a, supporting = line.split('\t')            # 将问题分词            q = tokenize(q)            # 根据参数选择是否保留其他不相干的文本行            substory = None            if only_supporting: # 如果only_supporting为True，则只保留支持答案的哪些文本                # Only select the related substory                # 将支撑行编号以空格符分割开，并将每个编号转为为int型                supporting = map(int, supporting.split())                # 在遇到问题类文本之前，story会将所有遇到的事实类文本都保留下来，因此，对于每一个支撑行编号，通过story[i-1]即可获取该支撑行文本                substory = [story[i - 1] for i in supporting]            else: # 如果是保留全部文本                # Provide all the substories                # 将前面所有的事实类文本都保留在substory中，作为支撑文本                substory = [x for x in story if x]            # 将处理好的支撑文本，问题，答案加入结果list            data.append((substory, q, a))            story.append('')        # 如果是事实类文本，则直接分词后加入story列表中，直到遇到问题类文本，取出里面相应的文本，作为支撑文本        else:            sent = tokenize(line)            story.append(sent)    # data中的数据格式为[(_,_,_)...]    return data</code></pre><h4 id="③获取数据记录"><a href="#③获取数据记录" class="headerlink" title="③获取数据记录"></a>③获取数据记录</h4><pre class=" language-lang-python"><code class="language-lang-python"># 获取文本内容函数def get_stories(f,only_supporting=False,max_length=None):    # 给定文件名，读取这个文件，取回故事，并且将句子转换成单个故事    # only_supporting参数决定是否只有支持答案的句子被保留下来。    data = parse_stories(f.readlines(), only_supporting=only_supporting)    # 创建一个函数    '''    reduce() 函数会对参数序列中元素进行累积。    函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中    的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。    '''    # 创建一个将支撑材料列表中的元素合并成一个的匿名方法    flatten = lambda data: reduce(lambda x, y: x + y, data)    # 如果没有限制支撑材料的最大长度，或者支撑材料的最大长度小于给定的max_len，于是就将这条记录保留下来    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]    return data</code></pre><p>其中创建了匿名函数，匿名函数的直观效果如下：</p><pre class=" language-lang-python"><code class="language-lang-python"># flatten 方法示例flatten = lambda data: reduce(lambda x, y: x + y, data)story=[['a','b','c'],['d','e'],['x','y','z']]flatten(story)</code></pre><pre><code>[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;]</code></pre><h4 id="④文本向量化方法"><a href="#④文本向量化方法" class="headerlink" title="④文本向量化方法"></a>④文本向量化方法</h4><pre class=" language-lang-python"><code class="language-lang-python">def vectorize_stories(data,word_idx,story_maxlen,query_maxlen):    xs=[]    xqs=[]    ys=[]    # 获取每一条记录的支撑材料、问题、答案    for story,query,answer in data:        # 对于支撑材料中的每一个单词，获取其id号，结果得到的是这个支撑文本的序列模型        x=[word_idx[w] for w in story]        # 对于问题中的每一个单词，获取其id号，得到该问题的序列模型        xq=[word_idx[w] for w in query]        # y的维度应该是字典的长度+1，先初试化为0向量        y=np.zeros(len(word_idx)+1)        # 将答案所在的位置的元素值设置为1        y[word_idx[answer]]=1        # 这条记录的支撑文本序列加入结果集        xs.append(x)        # 加入处理后的问题序列        xqs.append(xq)        # 加入结果        ys.append(y)    # 将支撑文本序列和文本序列填充至最大长度，返回    return (pad_sequences(xs,maxlen=story_maxlen),pad_sequences(xqs,maxlen=query_maxlen),np.array(ys))</code></pre><h3 id="（6）数据向量化"><a href="#（6）数据向量化" class="headerlink" title="（6）数据向量化"></a>（6）数据向量化</h3><pre class=" language-lang-python"><code class="language-lang-python"># 构建词汇表   vocab=set()## 将所以文本加起来，获取词汇表for story,q,answer in train+test:    vocab |=set(story+q+[answer])vocab=sorted(vocab)vocab_size=len(vocab)+1# 给词汇表中的每个单词建立对应的一个ID号word_idx=dict((c,i+1) for i,c in enumerate(vocab))print(word_idx)</code></pre><pre><code>{&#39;.&#39;: 1, &#39;?&#39;: 2, &#39;Daniel&#39;: 3, &#39;John&#39;: 4, &#39;Mary&#39;: 5, &#39;Sandra&#39;: 6, &#39;Where&#39;: 7, &#39;apple&#39;: 8, &#39;back&#39;: 9, &#39;bathroom&#39;: 10, &#39;bedroom&#39;: 11, &#39;discarded&#39;: 12, &#39;down&#39;: 13, &#39;dropped&#39;: 14, &#39;football&#39;: 15, &#39;garden&#39;: 16, &#39;got&#39;: 17, &#39;grabbed&#39;: 18, &#39;hallway&#39;: 19, &#39;is&#39;: 20, &#39;journeyed&#39;: 21, &#39;kitchen&#39;: 22, &#39;left&#39;: 23, &#39;milk&#39;: 24, &#39;moved&#39;: 25, &#39;office&#39;: 26, &#39;picked&#39;: 27, &#39;put&#39;: 28, &#39;the&#39;: 29, &#39;there&#39;: 30, &#39;to&#39;: 31, &#39;took&#39;: 32, &#39;travelled&#39;: 33, &#39;up&#39;: 34, &#39;went&#39;: 35}</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 获取支撑材料中单词的最大数目story_maxlen=max(map(len,(x for x,_,_ in train+test)))# 获取问题中单词的最大长度query_maxlen=max(map(len,(x for _,x,_ in train+test)))# 分别将训练集和测试集中的文本向量化x,xq,y=vectorize_stories(train,word_idx,story_maxlen,query_maxlen)tx,txq,ty=vectorize_stories(test,word_idx,story_maxlen,query_maxlen)print("vocab={}".format(vocab))print("x.shape={}".format(x.shape))print("xq,shape={}".format(xq.shape))print("y.shape={}".format(y.shape))print("story_maxlen,query_maxlen={},{}".format(story_maxlen,query_maxlen))</code></pre><pre><code>vocab=[&#39;.&#39;, &#39;?&#39;, &#39;Daniel&#39;, &#39;John&#39;, &#39;Mary&#39;, &#39;Sandra&#39;, &#39;Where&#39;, &#39;apple&#39;, &#39;back&#39;, &#39;bathroom&#39;, &#39;bedroom&#39;, &#39;discarded&#39;, &#39;down&#39;, &#39;dropped&#39;, &#39;football&#39;, &#39;garden&#39;, &#39;got&#39;, &#39;grabbed&#39;, &#39;hallway&#39;, &#39;is&#39;, &#39;journeyed&#39;, &#39;kitchen&#39;, &#39;left&#39;, &#39;milk&#39;, &#39;moved&#39;, &#39;office&#39;, &#39;picked&#39;, &#39;put&#39;, &#39;the&#39;, &#39;there&#39;, &#39;to&#39;, &#39;took&#39;, &#39;travelled&#39;, &#39;up&#39;, &#39;went&#39;]x.shape=(1000, 552)xq,shape=(1000, 5)y.shape=(1000, 36)story_maxlen,query_maxlen=552,5</code></pre><p>这里再次用到了RepeatVector，keras.layers.core.RepeatVector(n)：将输入重复n次<br><strong>参数n</strong>：整数，重复的次数<br><strong>输入shape</strong>：形如（nb_samples, features）的2D张量<br><strong>输出shape</strong>：形如（nb_samples, n, features）的3D张量</p><h3 id="（7）搭建网络结构"><a href="#（7）搭建网络结构" class="headerlink" title="（7）搭建网络结构"></a>（7）搭建网络结构</h3><pre class=" language-lang-python"><code class="language-lang-python">print("Build model...")# 第一个输入层，用于处理的支撑材料文本向量sentence=layers.Input(shape=(story_maxlen,),dtype="int32")encoded_sentence=layers.Embedding(vocab_size,EMBED_HIDDEN_SIZE)(sentence)encoded_sentence=layers.Dropout(0.3)(encoded_sentence)# 第二个输入层，用于处理问题文本向量question=layers.Input(shape=(query_maxlen,),dtype="int32")encoded_question=layers.Embedding(vocab_size,EMBED_HIDDEN_SIZE)(question)encoded_question=layers.Dropout(0.3)(encoded_question)encoded_question=RNN(EMBED_HIDDEN_SIZE)(encoded_question)# 重复向量encoded_question=layers.RepeatVector(story_maxlen)(encoded_question)# 将两个经过处理后的输入合并merged=layers.add([encoded_sentence,encoded_question])merged=RNN(EMBED_HIDDEN_SIZE)(merged)merged=layers.Dropout(0.3)(merged)preds=layers.Dense(vocab_size,activation="softmax")(merged)model=Model([sentence,question],preds)model.compile(optimizer="adam",loss='categorical_crossentropy',metrics=['accuracy'])print("Training")model.fit([x,xq],y,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_split=0.05)loss,acc=model.evaluate([tx,txq],ty,batch_size=BATCH_SIZE)print("Test loss / test accuracy={:.4f}/{:.4f}".format(loss,acc))</code></pre><pre><code>Build model...TrainingTrain on 950 samples, validate on 50 samplesEpoch 1/40950/950 [==============================] - 10s 11ms/step - loss: 2.9108 - acc: 0.1947 - val_loss: 2.1052 - val_acc: 0.0600...Epoch 40/40950/950 [==============================] - 10s 10ms/step - loss: 1.6499 - acc: 0.3284 - val_loss: 1.6824 - val_acc: 0.38001000/1000 [==============================] - 2s 2ms/stepTest loss / test accuracy=1.7385/0.2640</code></pre><h3 id="（8）可视化网络结构"><a href="#（8）可视化网络结构" class="headerlink" title="（8）可视化网络结构"></a>（8）可视化网络结构</h3><pre class=" language-lang-python"><code class="language-lang-python">plot_model(model, to_file='babi_rnn_model.png')</code></pre><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/babi_rnn.png" alt="网络结构图"></p><h2 id="3-小结"><a href="#3-小结" class="headerlink" title="3 小结"></a>3 小结</h2><p>通过这次实验，收获也是挺多的。首先是数据的处理方式，这里将词汇映射成索引，采用了最基本的python的enumerate方法，对于词汇较少的情况来说非常快，也非常容易上手，此外将词汇映射成id号的方法还有：①利用TensorFlow提供的数据预处理接口；②利用gensim提供的接口建立字典，再将其映射成id号；③利用keras提供的数据预处理接口，同样能将文本映射成id号。接着就是tarfile对压缩文件的操作，由于之前接触linux较少，很多都还是不是很了解，通过这次实验，学到了可以直接处理压缩文件。最后就是，利用keras搭建多输入的网络结构，发现keras真的很方便，网络结构可视化也非常方便（TensorFlow绕来绕去，代码还没敲完，头都晕了_(¦3」∠)_），好啦，就酱！</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速将非数值型目标变量转化为数值型变量</title>
      <link href="/2018/07/21/%E5%BF%AB%E9%80%9F%E5%B0%86%E9%9D%9E%E6%95%B0%E5%80%BC%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F%E8%BD%AC%E5%8C%96%E4%B8%BA%E6%95%B0%E5%80%BC%E5%9E%8B%E5%8F%98%E9%87%8F/"/>
      <url>/2018/07/21/%E5%BF%AB%E9%80%9F%E5%B0%86%E9%9D%9E%E6%95%B0%E5%80%BC%E5%9E%8B%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F%E8%BD%AC%E5%8C%96%E4%B8%BA%E6%95%B0%E5%80%BC%E5%9E%8B%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<p>快速将非数值型目标变量转化为数值型变量，比如把[‘L’,’M’,’N’]转化为[1,2,3]或多维向量类型<a id="more"></a></p><p>转化前：</p><pre><code>df_train.y.value_counts()</code></pre><pre><code>C    620R    477G    361M    353L    267T    216E     87Name: y, dtype: int64</code></pre><h3 id="法1：转化为一维型"><a href="#法1：转化为一维型" class="headerlink" title="法1：转化为一维型"></a>法1：转化为一维型</h3><pre><code>from sklearn import preprocessing# 获取目标变量列data_y=df_train[&#39;y&#39;]# 获取目标变量值y_labels=list(data_y.value_counts().index)# 创建标签预处理器le=preprocessing.LabelEncoder()le.fit(y_labels)# 对每一个标签值进行映射y=data_y.map(lambda x :le.transform([x])[0])y.value_counts()</code></pre><p>转化后：</p><pre><code>0    6205    4772    3614    3533    2676    2161     87Name: y, dtype: int64</code></pre><h3 id="法2：转化为多维型"><a href="#法2：转化为多维型" class="headerlink" title="法2：转化为多维型"></a>法2：转化为多维型</h3><pre><code>from keras.utils.np_utils import to_categoricalfrom sklearn import preprocessing# 获取目标变量列data_y=df_train[&#39;y&#39;]# 获取目标变量值y_labels=list(data_y.value_counts().index)# 创建标签预处理器le=preprocessing.LabelEncoder()le.fit(y_labels)# 获取标签的类别数num_labels=len(y_labels)# 对每一个标签值进行映射y=to_categorical(data_y.map(lambda x :le.transform([x])[0]),num_labels)print(y[:5])</code></pre><p>转化后：</p><pre><code>array([[0., 0., 0., 0., 0., 0., 1.],       [0., 0., 0., 0., 0., 0., 1.],       [0., 0., 1., 0., 0., 0., 0.],       [0., 0., 1., 0., 0., 0., 0.],       [0., 0., 1., 0., 0., 0., 0.]])</code></pre>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 实验记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras examples-addition_rnn</title>
      <link href="/2018/07/20/Keras%20examples-addition_rnn%20(1)/"/>
      <url>/2018/07/20/Keras%20examples-addition_rnn%20(1)/</url>
      
        <content type="html"><![CDATA[<h2 id="Keras-examples-addition-rnn"><a href="#Keras-examples-addition-rnn" class="headerlink" title="Keras examples-addition_rnn"></a>Keras examples-addition_rnn</h2><h3 id="1、任务描述"><a href="#1、任务描述" class="headerlink" title="1、任务描述"></a>1、任务描述</h3><p>（1）任务概述：主要是利用序列学习来实现两个整数（字符串）的加法<br>（2）任务分析：两个整数相加，需要通过神经网络来训练，于是不能够直接将整型作为神经网络的输入；想想办法，如果将整数和加号等都转化为字符，于是就可以用一个字符串来表示这个加法问题了；字符串表示的加法问题，可以看成是文本，于是紧接着考虑用什么语言模型来表示这个问题，由于加法问题中涉及到的字符种类不多，只有12类，‘0-9’、‘+’、‘ ’，因此可以直接考虑使用one-hot模型。</p><h3 id="2、具体实现"><a href="#2、具体实现" class="headerlink" title="2、具体实现"></a>2、具体实现</h3><h4 id="（1）引入相应的库"><a href="#（1）引入相应的库" class="headerlink" title="（1）引入相应的库"></a>（1）引入相应的库</h4><pre class=" language-lang-python"><code class="language-lang-python">from __future__ import print_functionfrom keras.models import Sequentialfrom keras import layersimport numpy as npfrom six.moves import rangefrom keras.utils import plot_model</code></pre><h4 id="（2）定义相应的方法、设置常量"><a href="#（2）定义相应的方法、设置常量" class="headerlink" title="（2）定义相应的方法、设置常量"></a>（2）定义相应的方法、设置常量</h4><p>这里主要定义了两个类，一个是编码解码类，另一个是预测结果的颜色处理类。<br><strong>①编码解码类：</strong>包含三个方法：构造方法，编码方法，解码方法。<br><strong>构造方法：</strong>在创建编码解码对象时，会接收一个字符串，这个字符串包含了所有涉及到的字符，构造函数利用该字符串创建了两个字典，一个是{字符：索引}型字典，一个是{索引：字符}型字典。前一个字典主要在编码阶段使用，用于将一个字符转化为one-hot类型，后裔字典主要在解码阶段使用，将one-hot矩阵表示的问题转化为对应的文本。<br>{字符：索引}型：{‘ ‘: 0, ‘+’: 1, ‘0’: 2, ‘1’: 3, ‘2’: 4, ‘3’: 5, ‘4’: 6, ‘5’: 7, ‘6’: 8, ‘7’: 9, ‘8’: 10, ‘9’: 11}<br>{索引，字符}型：{0: ‘ ‘, 1: ‘+’, 2: ‘0’, 3: ‘1’, 4: ‘2’, 5: ‘3’, 6: ‘4’, 7: ‘5’, 8: ‘6’, 9: ‘7’, 10: ‘8’, 11: ‘9’}<br><strong>编码方法：</strong>将传来的一个字符串编码城one-hot矩阵，具体实现是首先创建一个零矩阵，然后针对字符串的第i个字符，得到这个字符在字符字典中的索引为j，于是将零矩阵的第i行和第j列的元素置为1。直到将这个字符串的所有字符都编码。<br><strong>解码方法：</strong>解码方法可以看成是编码方法的逆，将one-hot矩阵转为字符串<br><strong>其实，这个类也可以利用keras内置的文本预处理方法来实现。</strong><br><strong>②颜色处理类：</strong>主要声明一些常量。</p><pre class=" language-lang-python"><code class="language-lang-python">class CharacterTable(object):    # 对于给定的一组字符    ## 将他们编码为one-hot的整型来表示    ## 将一个one-hot表示的解码为其字符    ## 将概率向量解码为其字符输出    def __init__(self,chars):        # 初始化字符表        self.chars=sorted(set(chars))        # 创建字符表中字符和其索引的对应关系（字符：索引）        self.char_indices=dict((c,i) for i,c in enumerate(self.chars))        print(self.char_indices)        # 创建字符表中字符和其索引的对应关系（索引：字符）        self.indices_char=dict((i,c) for i,c in enumerate(self.chars))        print(self.indices_char)    # 将给定的字符串C编码成one-hot模型，参数num_rows指定这个矩阵的行数（等于问题的最大长度），列数等于字符表中总的字符数    def encode(self,c,num_rows):             # 创建一个零矩阵，行数为问题的最大长度，列数等于字符表中总的字符数        x=np.zeros((num_rows,len(self.chars)))        # 针对问题字符串中的第i个字符        for i,c in enumerate(c):            # 将0矩阵的第i行和字符索引列的元素设置为1            x[i,self.char_indices[c]]=1        # 返回对字符串编码后的矩阵        return x    def decode(self, x, calc_argmax=True):        if calc_argmax:            x = x.argmax(axis=-1)        return ''.join(self.indices_char[x] for x in x)class colors:    ok = '\033[92m'    fail = '\033[91m'    close = '\033[0m'# 设置训练集大小    TRAINING_SIZE=50000# 设置加数的最大位数DIGITS=3# 是否逆序REVERSE=True# 问题的最大长度MAXLEN=DIGITS+1+DIGITS# 所有字符chars="0123456789+ "# 创建字符表对象ctable=CharacterTable(chars)</code></pre><pre><code>{&#39; &#39;: 0, &#39;+&#39;: 1, &#39;0&#39;: 2, &#39;1&#39;: 3, &#39;2&#39;: 4, &#39;3&#39;: 5, &#39;4&#39;: 6, &#39;5&#39;: 7, &#39;6&#39;: 8, &#39;7&#39;: 9, &#39;8&#39;: 10, &#39;9&#39;: 11}{0: &#39; &#39;, 1: &#39;+&#39;, 2: &#39;0&#39;, 3: &#39;1&#39;, 4: &#39;2&#39;, 5: &#39;3&#39;, 6: &#39;4&#39;, 7: &#39;5&#39;, 8: &#39;6&#39;, 9: &#39;7&#39;, 10: &#39;8&#39;, 11: &#39;9&#39;}</code></pre><h4 id="（3）构造训练数据"><a href="#（3）构造训练数据" class="headerlink" title="（3）构造训练数据"></a>（3）构造训练数据</h4><p>训练数据我们通过随机产生的整数来构造问题，通过整数相加的得到问题的答案，接着将问题和答案都字符化，并分别将每个问题和答案都用空格来填充至固定大小。最终问题的格式为：”——45+0”,答案的格式为：”54—“(“—”表示空格，这里已经将问题前后逆转了)。具体见下面代码。</p><pre class=" language-lang-python"><code class="language-lang-python">questions=[]expected=[]seen=set()print("Generating data...")while len(questions) < TRAINING_SIZE:    # ——————————————————————————————————————    # np.random.choice    # 参数意思分别是从给定的候选集中以概率P随机选择, p没有指定的时候相当于是一致的分布，replacement决定是否放回，size决定选择的个数    # randint是在[1,4)之间随机产生一个整数，这个整数决定此次产生的字符串的长度，然后依次随机从0-9中选择一个数字，最后拼接起来    # ——————————————————————————————————————    f=lambda: int("".join(np.random.choice(list("0123456789")) for i in range(np.random.randint(1,DIGITS+1))))    a,b=f(),f()    # 将随机选择的两个整数排序    key=tuple(sorted((a,b)))    # 如果此次选择的加法问题已经存在了，则忽略    if key in seen:        continue    seen.add(key)    # 构造问题，组装成a+b的字符串    q="{}+{}".format(a,b)    # 用空格来填充问题成预设的最大长度    query=q+" "*(MAXLEN-len(q))    # 构造问题的答案    ans=str(a+b)    # 用空格来填充答案，使其长度为4    ans+=" "*(DIGITS+1-len(ans))    # 如果需要逆序    if REVERSE:        query=query[::-1]    # 将构造的问题和答案加入相应的列表    questions.append(query)    expected.append(ans)print('Total addition questions:', len(questions))</code></pre><pre><code>Generating data...Total addition questions: 50000</code></pre><h4 id="（4）将训练数据向量化"><a href="#（4）将训练数据向量化" class="headerlink" title="（4）将训练数据向量化"></a>（4）将训练数据向量化</h4><p>这一部分就是调用预先写好的编码解码类来将问题和答案转化为one-hot模型。</p><pre class=" language-lang-python"><code class="language-lang-python">print("Vectorization...")# 将x初始化为（样本数，每个问题的最大长度，总的字符数）的布尔型矩阵x=np.zeros((len(questions),MAXLEN,len(chars)),dtype=np.bool)y=np.zeros((len(questions),DIGITS+1,len(chars)),dtype=np.bool)# 针对训练问题集中的每一个问题，对其进行编码，one-hot模型，一个问题编码后对应一个矩阵for i,sentence in enumerate(questions):    x[i]=ctable.encode(sentence,MAXLEN)for i,sentence in enumerate(expected):    y[i]=ctable.encode(sentence,DIGITS+1)</code></pre><pre><code>Vectorization...</code></pre><h4 id="（5）将训练集随机化"><a href="#（5）将训练集随机化" class="headerlink" title="（5）将训练集随机化"></a>（5）将训练集随机化</h4><pre class=" language-lang-python"><code class="language-lang-python">indices=np.arange(len(y))np.random.shuffle(indices)x=x[indices]y=y[indices]</code></pre><h4 id="（6）划分训练集和测试集"><a href="#（6）划分训练集和测试集" class="headerlink" title="（6）划分训练集和测试集"></a>（6）划分训练集和测试集</h4><pre class=" language-lang-python"><code class="language-lang-python">split_at=len(x)-len(x)//10(x_train,x_val)=x[:split_at],x[split_at:](y_train,y_val)=y[:split_at],y[split_at:]# 查看训练集和验证集数据信息print("Training Data:")print(x_train.shape)print(y_train.shape)print("Validation Data:")print(x_val.shape)print(y_val.shape)</code></pre><pre><code>Training Data:(45000, 7, 12)(45000, 4, 12)Validation Data:(5000, 7, 12)(5000, 4, 12)</code></pre><h4 id="（7）搭建网络结构"><a href="#（7）搭建网络结构" class="headerlink" title="（7）搭建网络结构"></a>（7）搭建网络结构</h4><p>网络结构通过keras的序列模型搭建起来，其结构图如图所示：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/addition_rnn_model.png" alt="网络结构图"></p><pre class=" language-lang-python"><code class="language-lang-python">RNN=layers.LSTMHIDDEN_SIZE=128BATCH_SIZE=128LAYERS=1print("Building model...")model=Sequential()model.add(RNN(HIDDEN_SIZE,input_shape=(MAXLEN,len(chars))))model.add(layers.core.RepeatVector(DIGITS+1))# 创建LAYERS层的RNN网络层for _ in range(LAYERS):    model.add(RNN(HIDDEN_SIZE,return_sequences=True))model.add(layers.TimeDistributed(layers.Dense(len(chars))))model.add(layers.Activation("softmax"))model.compile(loss="categorical_crossentropy",optimizer="adam",metrics=['accuracy'])model.summary()</code></pre><pre><code>Building model..._________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================lstm_1 (LSTM)                (None, 128)               72192     _________________________________________________________________repeat_vector_1 (RepeatVecto (None, 4, 128)            0         _________________________________________________________________lstm_2 (LSTM)                (None, 4, 128)            131584    _________________________________________________________________time_distributed_1 (TimeDist (None, 4, 12)             1548      _________________________________________________________________activation_1 (Activation)    (None, 4, 12)             0         =================================================================Total params: 205,324Trainable params: 205,324Non-trainable params: 0_________________________________________________________________</code></pre><p>其中，部分API的功能介绍如下：<br><strong>①keras.layers.core.RepeatVector(n)</strong>：将输入重复n次<br>参数n：整数，重复的次数<br>输入shape：形如（nb_samples, features）的2D张量<br>输出shape：形如（nb_samples, n, features）的3D张量<br><strong>②keras.layers.wrappers.TimeDistributed(layer)：</strong>该包装器可以把一个层应用到输入的每一个时间步上，这个有点难理解，在网上找到了一篇通俗易懂的解释，看了好半天才才理解到（智商是硬伤）。地址：<a href="https://blog.csdn.net/u012193416/article/details/79477220" target="_blank" rel="noopener">https://blog.csdn.net/u012193416/article/details/79477220</a></p><h4 id="（8）训练和预测"><a href="#（8）训练和预测" class="headerlink" title="（8）训练和预测"></a>（8）训练和预测</h4><pre class=" language-lang-python"><code class="language-lang-python"># train the model each generation and show predictions against the validataion datasetfor iteration in range(1,200):    print()    print("-"*50)    print("Iteration",iteration)    model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(x_val,y_val))</code></pre><pre><code>--------------------------------------------------Iteration 1Train on 45000 samples, validate on 5000 samplesEpoch 1/145000/45000 [==============================] - 7s 148us/step - loss: 1.8855 - acc: 0.3222 - val_loss: 1.8006 - val_acc: 0.3410.........--------------------------------------------------Iteration 199Train on 45000 samples, validate on 5000 samplesEpoch 1/145000/45000 [==============================] - 6s 136us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0027 - val_acc: 0.9991</code></pre><p>上面就是使用处理好的数据来训练网络的过程，一共训练200轮（不知道是否理解有误），这段代码与下面这句代码等效：</p><pre class=" language-lang-python"><code class="language-lang-python">model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=200,validation_data=(x_val,y_val))</code></pre><p>接下来就是测试模型的性能了，从验证集里随机产生10个问题，通过模型得到预测的答案，</p><pre class=" language-lang-python"><code class="language-lang-python"># select 10 samples from the validation set at random so we can see errorsfor i in range(10):    # 从验证集数据的索引范围内随机产生一个索引    ind=np.random.randint(0,len(x_val))    # 通过索引，获取问题的内容和问题答案    rowx,rowy=x_val[np.array([ind])],y_val[np.array([ind])]    # 通过模型预测答案    preds=model.predict_classes(rowx,verbose=0)    # 将问题解码    q=ctable.decode(rowx[0])    # 将答案解码    correct=ctable.decode(rowy[0])    # 将预测的结果解码    guess=ctable.decode(preds[0],calc_argmax=False)    # 构造输出格式    print("Q:",q[::-1] if REVERSE else q,end=" ")    print('T:',correct,end=" ")    # 根据是否正确选择不同的输出格式    if correct == guess:        print(colors.ok + '☑' + colors.close, end=' ')    else:        print(colors.fail + '☒' + colors.close, end=' ')        print(guess)</code></pre><pre><code>Q: 948+50  T: 998  [92m☑[0m Q: 324+89  T: 413  [92m☑[0m Q: 50+424  T: 474  [92m☑[0m Q: 910+2   T: 912  [92m☑[0m Q: 15+157  T: 172  [92m☑[0m Q: 660+0   T: 660  [92m☑[0m Q: 453+32  T: 485  [92m☑[0m Q: 4+263   T: 267  [92m☑[0m Q: 6+424   T: 430  [92m☑[0m Q: 92+942  T: 1034 [92m☑[0m </code></pre><pre class=" language-lang-python"><code class="language-lang-python"># model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=200,validation_data=(x_val,y_val))</code></pre><h4 id="（9）网络结构可视化"><a href="#（9）网络结构可视化" class="headerlink" title="（9）网络结构可视化"></a>（9）网络结构可视化</h4><pre class=" language-lang-python"><code class="language-lang-python">plot_model(model, to_file='addition_rnn_model.png')</code></pre><h3 id="3、小结"><a href="#3、小结" class="headerlink" title="3、小结"></a>3、小结</h3><p>通过这一次实验，收获肯定是有的。首先是将一个实际问题转化为网络结构能过处理的一个问题。转化为什么问题，采用什么模型来解，这些都是很重要的；接着就是数据的构造、one-hot模型的实现，虽然以前也手写代码实现过one-hot模型，但是这次通过自己构造数据，然后不通过接口来将数据转化为one-hot模型，这是第一次遇到，因此学会了这种情况的处理方式。最后就是keras的一些层的使用，比如说RepeatVector()和TimeDistributed()，由于才开始学keras，看了文档没多久就忘记了，即使不忘，有些层也只知道个大概，具体怎么用还是不清楚。通过这个例子，里面涉及到了这两个层，然后我也去查了这两个层的资料，至少现在知道了是怎么使用的。那这一篇就这样吧！</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bag of Words Meets Bags of Popcorn(3)-Word2Voc</title>
      <link href="/2018/06/14/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(3)-Word2Voc%20(1)/"/>
      <url>/2018/06/14/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(3)-Word2Voc%20(1)/</url>
      
        <content type="html"><![CDATA[<p>词带模型：<a href="http://bei.dreamcykj.com/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn%281%29-Bag%20of%20Words/">Bag of Words Meets Bags of Popcorn(1)-Bag of Words</a><br>Tfidf模型：<a href="http://bei.dreamcykj.com/2018/06/14/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn%282%29-tfidf%20%281%29/">Bag of Words Meets Bags of Popcorn(2)-tfidf</a><br>这一节采用词向量<br><a id="more"></a></p><h4 id="1、读取数据"><a href="#1、读取数据" class="headerlink" title="1、读取数据"></a>1、读取数据</h4><pre class=" language-lang-python"><code class="language-lang-python">import pandas as pdtrain=pd.read_csv('./data/labeledTrainData.tsv',header=0,delimiter="\t",quoting=3)test=pd.read_csv("./data/testData.tsv",header=0,delimiter="\t",quoting=3)unlabeled_train=pd.read_csv("./data/unlabeledTrainData.tsv",header=0,delimiter="\t",quoting=3)# 查看数据的大小print("read %d labeled train reviews,%d labeled test reviews and %d unlabeled reviews\n"%(train["review"].size,len(test),len(unlabeled_train)))</code></pre><pre><code>read 25000 labeled train reviews,25000 labeled test reviews and 50000 unlabeled reviews</code></pre><h4 id="2、数据清洗"><a href="#2、数据清洗" class="headerlink" title="2、数据清洗"></a>2、数据清洗</h4><h5 id="2、1-对每一句评论进行数据清洗"><a href="#2、1-对每一句评论进行数据清洗" class="headerlink" title="2、1 对每一句评论进行数据清洗"></a>2、1 对每一句评论进行数据清洗</h5><pre class=" language-lang-python"><code class="language-lang-python">from bs4 import BeautifulSoupimport re from nltk.corpus import stopwordsdef review_to_wordlist(review,remove_stopwords=False):    # 去掉html标签    review_text=BeautifulSoup(review).get_text()    # 去掉标点符号和非法字符    review_text=re.sub("[^a-zA-Z]"," ",review_text)    # 全部转化为小写,并以空格分割    words=review_text.lower().split()    # 去停用词    if remove_stopwords:        stops=set(stopwords.words("english"))        words=[w for w in words if w not in stops]    return words</code></pre><h5 id="2-2-对每一篇评论进行数据清洗"><a href="#2-2-对每一篇评论进行数据清洗" class="headerlink" title="2.2 对每一篇评论进行数据清洗"></a>2.2 对每一篇评论进行数据清洗</h5><p>将评论段落转换为句子，返回句子列表，每个句子由一堆词组成</p><pre class=" language-lang-python"><code class="language-lang-python">import nltk.datatokenizer=nltk.data.load("tokenizers/punkt/english.pickle")def review_to_sentences(review,tokenizer,remove_stopwords=False):    # 将评论按句子分割    raw_sentences=tokenizer.tokenize(review.strip())    # 对一个评论的每一句话进行清洗    sentences=[]    for raw_sentence in raw_sentences:        if len(raw_sentence)>0:            # 调用数据清洗方法，对该句评论进行清洗            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords))    # 返回清洗后的这一篇评论    return sentences</code></pre><h4 id="3、构造语料集"><a href="#3、构造语料集" class="headerlink" title="3、构造语料集"></a>3、构造语料集</h4><p>语料集中的数据包括已标注好的训练集和未标注好的训练集</p><pre class=" language-lang-python"><code class="language-lang-python">sentences=[] # 装有所有评论数据的一个list，相当于一个语料集,格式为：[[a.b.c].[a.b.c.d]...]print("Parsing sentences from training set")for review in train["review"]:    sentences+=review_to_sentences(review,tokenizer)print("Parsing sentence from unlabeled set")for review in unlabeled_train["review"]:    sentences+=review_to_sentences(review,tokenizer)# 查看语料集的大小print(len(sentences))</code></pre><pre><code>Parsing sentences from training set795538</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 查看部分数据print(sentences[0])print(sentences[1])</code></pre><pre><code>[&#39;with&#39;, &#39;all&#39;, &#39;this&#39;, &#39;stuff&#39;, &#39;going&#39;, &#39;down&#39;, &#39;at&#39;, &#39;the&#39;, &#39;moment&#39;, &#39;with&#39;, &#39;mj&#39;, &#39;i&#39;, &#39;ve&#39;, &#39;started&#39;, &#39;listening&#39;, &#39;to&#39;, &#39;his&#39;, &#39;music&#39;, &#39;watching&#39;, &#39;the&#39;, &#39;odd&#39;, &#39;documentary&#39;, &#39;here&#39;, &#39;and&#39;, &#39;there&#39;, &#39;watched&#39;, &#39;the&#39;, &#39;wiz&#39;, &#39;and&#39;, &#39;watched&#39;, &#39;moonwalker&#39;, &#39;again&#39;][&#39;maybe&#39;, &#39;i&#39;, &#39;just&#39;, &#39;want&#39;, &#39;to&#39;, &#39;get&#39;, &#39;a&#39;, &#39;certain&#39;, &#39;insight&#39;, &#39;into&#39;, &#39;this&#39;, &#39;guy&#39;, &#39;who&#39;, &#39;i&#39;, &#39;thought&#39;, &#39;was&#39;, &#39;really&#39;, &#39;cool&#39;, &#39;in&#39;, &#39;the&#39;, &#39;eighties&#39;, &#39;just&#39;, &#39;to&#39;, &#39;maybe&#39;, &#39;make&#39;, &#39;up&#39;, &#39;my&#39;, &#39;mind&#39;, &#39;whether&#39;, &#39;he&#39;, &#39;is&#39;, &#39;guilty&#39;, &#39;or&#39;, &#39;innocent&#39;]</code></pre><h4 id="4、构建word2vec模型"><a href="#4、构建word2vec模型" class="headerlink" title="4、构建word2vec模型"></a>4、构建word2vec模型</h4><p>这里主要使用gensim里的word2vec，相关介绍和使用情况见：python︱gensim训练word2vec及相关函数与功能理解 <a href="https://blog.csdn.net/sinat_26917383/article/details/69803018" target="_blank" rel="noopener">https://blog.csdn.net/sinat_26917383/article/details/69803018</a></p><h5 id="4、1训练模型"><a href="#4、1训练模型" class="headerlink" title="4、1训练模型"></a>4、1训练模型</h5><pre class=" language-lang-python"><code class="language-lang-python">import gensim,logginglogging.basicConfig(format="%(asctime)s : %(levelname)s : %(message)s",level=logging.INFO)num_features=300min_word_count=40num_workers=4context=10downsampling=1e-3print("Training model...")model=gensim.models.Word2Vec(sentences,workers=num_workers,size=num_features,min_count=min_word_count,window=context,sample=downsampling)model.init_sims(replace=True)model_name="300features_40minwords_10context"model.save("./data/"+model_name)</code></pre><pre><code>2018-06-14 16:17:04,394 : INFO : collecting all words and their counts2018-06-14 16:17:04,394 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types2018-06-14 16:17:04,439 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types2018-06-14 16:17:04,486 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types2018-06-14 16:17:04,530 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types2018-06-14 16:17:04,576 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word typesTraining model...2018-06-14 16:17:04,621 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types2018-06-14 16:17:04,666 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types2018-06-14 16:17:04,711 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types2018-06-14 16:17:04,759 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types2018-06-14 16:17:04,804 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types2018-06-14 16:17:04,849 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types2018-06-14 16:17:04,894 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types2018-06-14 16:17:04,939 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types2018-06-14 16:17:04,985 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types2018-06-14 16:17:05,029 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types2018-06-14 16:17:05,075 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types2018-06-14 16:17:05,120 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types2018-06-14 16:17:05,166 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types2018-06-14 16:17:05,212 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types2018-06-14 16:17:05,258 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types2018-06-14 16:17:05,304 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types2018-06-14 16:17:05,350 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types2018-06-14 16:17:05,397 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types2018-06-14 16:17:05,443 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types2018-06-14 16:17:05,490 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types2018-06-14 16:17:05,535 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types2018-06-14 16:17:05,581 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types2018-06-14 16:17:05,627 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types2018-06-14 16:17:05,674 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types2018-06-14 16:17:05,720 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types2018-06-14 16:17:05,767 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types2018-06-14 16:17:05,814 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types2018-06-14 16:17:05,861 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types2018-06-14 16:17:05,908 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types2018-06-14 16:17:05,956 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types2018-06-14 16:17:06,002 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types2018-06-14 16:17:06,048 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types2018-06-14 16:17:06,100 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types2018-06-14 16:17:06,146 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types2018-06-14 16:17:06,194 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types2018-06-14 16:17:06,240 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types2018-06-14 16:17:06,286 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types2018-06-14 16:17:06,331 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types2018-06-14 16:17:06,378 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types2018-06-14 16:17:06,425 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types2018-06-14 16:17:06,471 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types2018-06-14 16:17:06,519 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types2018-06-14 16:17:06,566 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types2018-06-14 16:17:06,612 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types2018-06-14 16:17:06,659 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types2018-06-14 16:17:06,704 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types2018-06-14 16:17:06,752 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types2018-06-14 16:17:06,798 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types2018-06-14 16:17:06,845 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types2018-06-14 16:17:06,893 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types2018-06-14 16:17:06,942 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types2018-06-14 16:17:06,990 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types2018-06-14 16:17:07,040 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types2018-06-14 16:17:07,089 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types2018-06-14 16:17:07,138 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types2018-06-14 16:17:07,197 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types2018-06-14 16:17:07,245 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types2018-06-14 16:17:07,294 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types2018-06-14 16:17:07,343 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types2018-06-14 16:17:07,392 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types2018-06-14 16:17:07,441 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types2018-06-14 16:17:07,490 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types2018-06-14 16:17:07,538 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types2018-06-14 16:17:07,587 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types2018-06-14 16:17:07,636 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types2018-06-14 16:17:07,686 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types2018-06-14 16:17:07,732 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types2018-06-14 16:17:07,779 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types2018-06-14 16:17:07,826 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types2018-06-14 16:17:07,872 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types2018-06-14 16:17:07,918 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types2018-06-14 16:17:07,964 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types2018-06-14 16:17:08,012 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types2018-06-14 16:17:08,060 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types2018-06-14 16:17:08,108 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types2018-06-14 16:17:08,134 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences2018-06-14 16:17:08,134 : INFO : Loading a fresh vocabulary2018-06-14 16:17:08,204 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)2018-06-14 16:17:08,205 : INFO : min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)2018-06-14 16:17:08,247 : INFO : deleting the raw counts dictionary of 123504 items2018-06-14 16:17:08,249 : INFO : sample=0.001 downsamples 48 most-common words2018-06-14 16:17:08,250 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)2018-06-14 16:17:08,286 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes2018-06-14 16:17:08,286 : INFO : resetting layer weights2018-06-14 16:17:08,497 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=102018-06-14 16:17:09,516 : INFO : EPOCH 1 - PROGRESS: at 6.40% examples, 812589 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:10,525 : INFO : EPOCH 1 - PROGRESS: at 13.20% examples, 832030 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:11,526 : INFO : EPOCH 1 - PROGRESS: at 20.16% examples, 847270 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:12,529 : INFO : EPOCH 1 - PROGRESS: at 27.14% examples, 856397 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:13,535 : INFO : EPOCH 1 - PROGRESS: at 34.33% examples, 865561 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:14,549 : INFO : EPOCH 1 - PROGRESS: at 41.21% examples, 865831 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:15,556 : INFO : EPOCH 1 - PROGRESS: at 48.00% examples, 866028 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:16,560 : INFO : EPOCH 1 - PROGRESS: at 55.16% examples, 871678 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:17,561 : INFO : EPOCH 1 - PROGRESS: at 62.03% examples, 873356 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:18,569 : INFO : EPOCH 1 - PROGRESS: at 70.05% examples, 887453 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:19,574 : INFO : EPOCH 1 - PROGRESS: at 78.55% examples, 905131 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:20,576 : INFO : EPOCH 1 - PROGRESS: at 87.08% examples, 920073 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:21,580 : INFO : EPOCH 1 - PROGRESS: at 95.30% examples, 929252 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:22,257 : INFO : worker thread finished; awaiting finish of 3 more threads2018-06-14 16:17:22,269 : INFO : worker thread finished; awaiting finish of 2 more threads2018-06-14 16:17:22,279 : INFO : worker thread finished; awaiting finish of 1 more threads2018-06-14 16:17:22,283 : INFO : worker thread finished; awaiting finish of 0 more threads2018-06-14 16:17:22,284 : INFO : EPOCH - 1 : training on 17798082 raw words (12749777 effective words) took 13.8s, 925879 effective words/s2018-06-14 16:17:23,304 : INFO : EPOCH 2 - PROGRESS: at 8.00% examples, 1007006 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:24,312 : INFO : EPOCH 2 - PROGRESS: at 16.61% examples, 1043247 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:25,319 : INFO : EPOCH 2 - PROGRESS: at 24.73% examples, 1036444 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:26,323 : INFO : EPOCH 2 - PROGRESS: at 31.73% examples, 997990 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:27,344 : INFO : EPOCH 2 - PROGRESS: at 38.79% examples, 974442 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:28,345 : INFO : EPOCH 2 - PROGRESS: at 45.83% examples, 962075 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:29,345 : INFO : EPOCH 2 - PROGRESS: at 52.83% examples, 952391 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:30,351 : INFO : EPOCH 2 - PROGRESS: at 59.61% examples, 942736 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:31,357 : INFO : EPOCH 2 - PROGRESS: at 66.74% examples, 938270 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:32,357 : INFO : EPOCH 2 - PROGRESS: at 73.62% examples, 932470 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:33,366 : INFO : EPOCH 2 - PROGRESS: at 80.53% examples, 926935 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:34,378 : INFO : EPOCH 2 - PROGRESS: at 87.08% examples, 918591 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:35,390 : INFO : EPOCH 2 - PROGRESS: at 94.27% examples, 917532 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:36,174 : INFO : worker thread finished; awaiting finish of 3 more threads2018-06-14 16:17:36,183 : INFO : worker thread finished; awaiting finish of 2 more threads2018-06-14 16:17:36,192 : INFO : worker thread finished; awaiting finish of 1 more threads2018-06-14 16:17:36,192 : INFO : worker thread finished; awaiting finish of 0 more threads2018-06-14 16:17:36,193 : INFO : EPOCH - 2 : training on 17798082 raw words (12750854 effective words) took 13.9s, 917479 effective words/s2018-06-14 16:17:37,202 : INFO : EPOCH 3 - PROGRESS: at 7.77% examples, 984985 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:38,212 : INFO : EPOCH 3 - PROGRESS: at 16.44% examples, 1034693 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:39,216 : INFO : EPOCH 3 - PROGRESS: at 25.10% examples, 1055381 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:40,227 : INFO : EPOCH 3 - PROGRESS: at 33.83% examples, 1063504 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:41,229 : INFO : EPOCH 3 - PROGRESS: at 42.30% examples, 1067870 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:42,236 : INFO : EPOCH 3 - PROGRESS: at 50.79% examples, 1069786 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:43,243 : INFO : EPOCH 3 - PROGRESS: at 59.22% examples, 1071100 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:44,251 : INFO : EPOCH 3 - PROGRESS: at 67.75% examples, 1071922 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:45,253 : INFO : EPOCH 3 - PROGRESS: at 76.27% examples, 1073438 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:46,257 : INFO : EPOCH 3 - PROGRESS: at 84.78% examples, 1074300 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:47,261 : INFO : EPOCH 3 - PROGRESS: at 93.12% examples, 1073178 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:48,053 : INFO : worker thread finished; awaiting finish of 3 more threads2018-06-14 16:17:48,059 : INFO : worker thread finished; awaiting finish of 2 more threads2018-06-14 16:17:48,062 : INFO : worker thread finished; awaiting finish of 1 more threads2018-06-14 16:17:48,071 : INFO : worker thread finished; awaiting finish of 0 more threads2018-06-14 16:17:48,072 : INFO : EPOCH - 3 : training on 17798082 raw words (12748789 effective words) took 11.9s, 1073879 effective words/s2018-06-14 16:17:49,091 : INFO : EPOCH 4 - PROGRESS: at 8.00% examples, 1005623 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:50,106 : INFO : EPOCH 4 - PROGRESS: at 16.61% examples, 1038972 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:51,116 : INFO : EPOCH 4 - PROGRESS: at 25.16% examples, 1051314 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:52,122 : INFO : EPOCH 4 - PROGRESS: at 33.77% examples, 1058149 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:53,133 : INFO : EPOCH 4 - PROGRESS: at 42.24% examples, 1061681 words/s, in_qsize 6, out_qsize 12018-06-14 16:17:54,141 : INFO : EPOCH 4 - PROGRESS: at 50.74% examples, 1064474 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:55,151 : INFO : EPOCH 4 - PROGRESS: at 59.17% examples, 1066074 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:56,161 : INFO : EPOCH 4 - PROGRESS: at 67.69% examples, 1067360 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:57,163 : INFO : EPOCH 4 - PROGRESS: at 75.94% examples, 1065426 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:58,166 : INFO : EPOCH 4 - PROGRESS: at 82.15% examples, 1038088 words/s, in_qsize 7, out_qsize 02018-06-14 16:17:59,183 : INFO : EPOCH 4 - PROGRESS: at 88.92% examples, 1020976 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:00,189 : INFO : EPOCH 4 - PROGRESS: at 95.65% examples, 1006423 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:00,821 : INFO : worker thread finished; awaiting finish of 3 more threads2018-06-14 16:18:00,829 : INFO : worker thread finished; awaiting finish of 2 more threads2018-06-14 16:18:00,830 : INFO : worker thread finished; awaiting finish of 1 more threads2018-06-14 16:18:00,833 : INFO : worker thread finished; awaiting finish of 0 more threads2018-06-14 16:18:00,833 : INFO : EPOCH - 4 : training on 17798082 raw words (12749357 effective words) took 12.8s, 999843 effective words/s2018-06-14 16:18:01,847 : INFO : EPOCH 5 - PROGRESS: at 8.11% examples, 1027303 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:02,858 : INFO : EPOCH 5 - PROGRESS: at 17.00% examples, 1069428 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:03,860 : INFO : EPOCH 5 - PROGRESS: at 25.86% examples, 1086074 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:04,863 : INFO : EPOCH 5 - PROGRESS: at 34.70% examples, 1093967 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:05,865 : INFO : EPOCH 5 - PROGRESS: at 43.30% examples, 1095208 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:06,868 : INFO : EPOCH 5 - PROGRESS: at 52.04% examples, 1097950 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:07,869 : INFO : EPOCH 5 - PROGRESS: at 60.69% examples, 1100176 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:08,878 : INFO : EPOCH 5 - PROGRESS: at 68.09% examples, 1079587 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:09,888 : INFO : EPOCH 5 - PROGRESS: at 74.91% examples, 1055508 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:10,900 : INFO : EPOCH 5 - PROGRESS: at 81.64% examples, 1034528 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:11,905 : INFO : EPOCH 5 - PROGRESS: at 88.40% examples, 1018906 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:12,916 : INFO : EPOCH 5 - PROGRESS: at 95.11% examples, 1004071 words/s, in_qsize 7, out_qsize 02018-06-14 16:18:13,626 : INFO : worker thread finished; awaiting finish of 3 more threads2018-06-14 16:18:13,635 : INFO : worker thread finished; awaiting finish of 2 more threads2018-06-14 16:18:13,642 : INFO : worker thread finished; awaiting finish of 1 more threads2018-06-14 16:18:13,643 : INFO : worker thread finished; awaiting finish of 0 more threads2018-06-14 16:18:13,643 : INFO : EPOCH - 5 : training on 17798082 raw words (12749291 effective words) took 12.8s, 996145 effective words/s2018-06-14 16:18:13,644 : INFO : training on a 88990410 raw words (63748068 effective words) took 65.1s, 978532 effective words/s2018-06-14 16:18:13,645 : INFO : precomputing L2-norms of word weight vectors2018-06-14 16:18:13,823 : INFO : saving Word2Vec object under ./data/300features_40minwords_10context, separately None2018-06-14 16:18:13,824 : INFO : not storing attribute vectors_norm2018-06-14 16:18:13,826 : INFO : not storing attribute cum_table2018-06-14 16:18:14,267 : INFO : saved ./data/300features_40minwords_10context</code></pre><h5 id="4、2-预览模型"><a href="#4、2-预览模型" class="headerlink" title="4、2 预览模型"></a>4、2 预览模型</h5><p>(1) 找出不匹配的词语</p><pre class=" language-lang-python"><code class="language-lang-python">model.doesnt_match("man woman child kitchen".split())</code></pre><pre><code>&#39;kitchen&#39;</code></pre><pre class=" language-lang-python"><code class="language-lang-python">model.doesnt_match("france england germany berlin".split())</code></pre><pre><code>&#39;berlin&#39;</code></pre><pre class=" language-lang-python"><code class="language-lang-python">model.doesnt_match("paris berlin london austria".split())</code></pre><pre><code>&#39;paris&#39;</code></pre><p>(2)匹配相似的词</p><pre class=" language-lang-python"><code class="language-lang-python">model.most_similar("man")</code></pre><pre><code>[(&#39;woman&#39;, 0.6306251883506775), (&#39;lady&#39;, 0.5747369527816772), (&#39;lad&#39;, 0.5607120990753174), (&#39;farmer&#39;, 0.5333519577980042), (&#39;soldier&#39;, 0.5211611986160278), (&#39;businessman&#39;, 0.5164321660995483), (&#39;men&#39;, 0.514143705368042), (&#39;guy&#39;, 0.5132095813751221), (&#39;gentleman&#39;, 0.5126838088035583), (&#39;chap&#39;, 0.5047199130058289)]</code></pre><pre class=" language-lang-python"><code class="language-lang-python">model.most_similar("queen")</code></pre><pre><code>[(&#39;princess&#39;, 0.665077805519104), (&#39;belle&#39;, 0.6354636549949646), (&#39;bride&#39;, 0.6334044933319092), (&#39;stepmother&#39;, 0.6092513799667358), (&#39;victoria&#39;, 0.603207528591156), (&#39;maid&#39;, 0.5809364318847656), (&#39;angela&#39;, 0.5791389346122742), (&#39;showgirl&#39;, 0.5766100883483887), (&#39;seductress&#39;, 0.576302170753479), (&#39;maria&#39;, 0.5733033418655396)]</code></pre><pre class=" language-lang-python"><code class="language-lang-python">model.most_similar("awful")</code></pre><pre><code>[(&#39;terrible&#39;, 0.7602646350860596), (&#39;atrocious&#39;, 0.7441849708557129), (&#39;horrible&#39;, 0.7187424898147583), (&#39;abysmal&#39;, 0.7162792682647705), (&#39;dreadful&#39;, 0.6996028423309326), (&#39;appalling&#39;, 0.6916007995605469), (&#39;horrendous&#39;, 0.6912209987640381), (&#39;horrid&#39;, 0.6631687879562378), (&#39;lousy&#39;, 0.6283937692642212), (&#39;amateurish&#39;, 0.616948127746582)]</code></pre><p>（3）查看模型内部词向量内容</p><pre class=" language-lang-python"><code class="language-lang-python">model['flower']</code></pre><pre><code>array([-0.04672493,  0.08882798, -0.00798039,  0.01940776,  0.00703776,       -0.0983417 , -0.03383616,  0.01253733,  0.1013744 , -0.06382444,        0.05575086, -0.03603694, -0.1734672 , -0.10947284,  0.04083061,       -0.02300067, -0.08912303, -0.01208846, -0.03741381, -0.03776918,        0.00958646, -0.04842193,  0.03746754,  0.00568479, -0.03371157,        0.05098233,  0.03344997, -0.04607128, -0.03911143, -0.09323646,        0.05823594,  0.05683371, -0.03013518,  0.03097427,  0.02941879,        0.05783584,  0.02902247,  0.02647943, -0.04254698, -0.01946488,       -0.07041467,  0.07577515,  0.01416771,  0.06094591,  0.05511365,        0.02002226, -0.09548184,  0.01083773,  0.11513695, -0.00510942,        0.10982258,  0.0311219 , -0.11257961, -0.04740267,  0.02418482,       -0.13684008, -0.01806154, -0.03081693, -0.02723986, -0.02626858,       -0.02271062,  0.02877414, -0.18940935,  0.15113354, -0.08325452,        0.13035862,  0.03562083,  0.05715419,  0.05659889,  0.11132292,       -0.11618023, -0.02525528, -0.04214492, -0.01676722, -0.11387489,        0.01289947,  0.06157506,  0.02216116,  0.03087287,  0.00053991,        0.00213429,  0.07381905, -0.09397278,  0.08932504, -0.00162343,       -0.07650761,  0.04067037, -0.14623344, -0.01521316,  0.03096902,       -0.00218932, -0.08328622, -0.05705699,  0.05337256,  0.03251749,        0.06499627, -0.03883306,  0.01217427,  0.0509025 , -0.03585384,        0.0689431 ,  0.02351258,  0.04486413, -0.01371281,  0.0296316 ,        0.02136545, -0.14522278,  0.00962873, -0.01190588, -0.02343949,       -0.07445157, -0.00743668,  0.06643493,  0.03143901,  0.03538921,        0.05261549,  0.00622071,  0.00512619,  0.13138519, -0.016271  ,        0.1280664 , -0.0281569 ,  0.05304267, -0.08197889,  0.03647416,       -0.0908702 ,  0.0907077 ,  0.0009073 , -0.00584355,  0.0216803 ,        0.09708863, -0.03843877,  0.07444821, -0.02166734,  0.11680321,        0.01058747, -0.03528195, -0.01671917,  0.00431578,  0.02983457,        0.05324177, -0.01105398, -0.0105139 ,  0.02842969, -0.02374061,       -0.09566777,  0.00259409, -0.00075548,  0.03930911,  0.00377064,        0.01211741, -0.04258214, -0.01305003, -0.01610391, -0.07661422,       -0.01903511, -0.06883642, -0.08518191, -0.01282962,  0.0184722 ,        0.04440661, -0.00693423, -0.00422318,  0.01570578, -0.00632003,       -0.00961587, -0.02984856,  0.0214809 ,  0.07136744,  0.03213349,       -0.04010129, -0.00035848, -0.01424925,  0.06443602,  0.02018777,        0.01943419, -0.00966864,  0.00596202, -0.03069373, -0.01564495,       -0.05931353,  0.02222474,  0.01904519, -0.02842978, -0.00069007,       -0.03887554, -0.05666535,  0.01283519, -0.00330482, -0.01628766,        0.00492181, -0.05608921, -0.01671099,  0.02535864, -0.00218894,        0.04458237,  0.07417594,  0.03866369, -0.09731697,  0.01891821,       -0.02167555,  0.04933357, -0.05242139,  0.03721772,  0.0229468 ,       -0.08311929, -0.00661719, -0.00621934, -0.07366215,  0.00854658,        0.05721372, -0.04334996,  0.00494872, -0.07360718, -0.04063034,       -0.01522357,  0.00445656, -0.09802664, -0.05989946, -0.05275297,       -0.03157848, -0.03841926,  0.06702825, -0.08821026,  0.06974641,        0.05465396,  0.06257062, -0.03873832,  0.11081573, -0.05601896,       -0.03000432, -0.02636354,  0.08552916, -0.09668133,  0.02696207,        0.00823885, -0.02628613,  0.04240276, -0.04750042,  0.02168193,        0.07290898, -0.02299476,  0.05413065,  0.08540422, -0.00282635,       -0.0536682 ,  0.0743058 ,  0.00220649, -0.01906369, -0.00633779,       -0.03175312, -0.02021488, -0.03198906, -0.04739904,  0.04052622,        0.0988438 ,  0.00562239,  0.03735855,  0.0457716 , -0.0033249 ,        0.01441622,  0.03325477,  0.09631445,  0.04915336,  0.05992904,       -0.00600277, -0.01433129,  0.04592302,  0.04359821, -0.13954614,       -0.00307374, -0.06090346, -0.13478954, -0.05491044, -0.0156491 ,        0.13602623,  0.04711536, -0.00213679, -0.11509449, -0.11057125,        0.02938064, -0.08248176,  0.01261425,  0.0245921 ,  0.07879733,        0.03867466, -0.07527684, -0.03336613,  0.10514716, -0.0246095 ,       -0.06571205,  0.05439528,  0.0557172 ,  0.02308842, -0.02448075,        0.00975912,  0.06725417,  0.04760227, -0.06176807, -0.0582995 ],      dtype=float32)</code></pre><h4 id="5、使用Word2vec特征"><a href="#5、使用Word2vec特征" class="headerlink" title="5、使用Word2vec特征"></a>5、使用Word2vec特征</h4><h5 id="5、1-构造向量化方法"><a href="#5、1-构造向量化方法" class="headerlink" title="5、1  构造向量化方法"></a>5、1  构造向量化方法</h5><pre class=" language-lang-python"><code class="language-lang-python">import numpy as np# 返回特征词向量def getWordVecs(wordList,model):    vecs = []    # 对于一条评论中的每一个单词    for word in wordList:        word = word.replace('\n','')        #print word        try:            vecs.append(model[word])        except KeyError:            continue    return np.array(vecs, dtype='float')# 将评论转化为向量模型def reviews_to_vec(review_list,model):    review_vecs=[]    # 对于每一条评论，将其向量化    for line in review_list:        # 将字符串按照空格分割成列表        #wordList=line.split()        # 得到一条评论的矩阵向量,一条语句即可得到一个二维矩阵，行数为词的个数，列数为模型设定的维度；        vecs = getWordVecs(line, model)        # 根据得到的矩阵计算矩阵均值作为当前语句的特征词向量        if len(vecs) > 0:            vecsArray = sum(np.array(vecs)) / len(vecs)  # mean            review_vecs.append(vecsArray)    # 返回词向量表示的所有评论    return review_vecs</code></pre><h5 id="5、2-将数据向量化"><a href="#5、2-将数据向量化" class="headerlink" title="5、2 将数据向量化"></a>5、2 将数据向量化</h5><pre class=" language-lang-python"><code class="language-lang-python">num_reviews=len(train)clean_train_reviews=[] # 存放的是评论列表，一条评论是由单词列表组成的for i in range(0,num_reviews):    temp_review=review_to_wordlist(train["review"][i],True)    clean_train_reviews.append(temp_review)# 查看清洗后的数据print(clean_train_reviews[0])# 对测试集做相同的操作num_reviews=len(test)clean_test_reviews=[]for i in range(0,num_reviews):    temp_review=review_to_wordlist(test['review'][i],True)    clean_test_reviews.append(temp_review)</code></pre><pre><code>    [&#39;stuff&#39;, &#39;going&#39;, &#39;moment&#39;, &#39;mj&#39;, &#39;started&#39;, &#39;listening&#39;, &#39;music&#39;, &#39;watching&#39;, &#39;odd&#39;, &#39;documentary&#39;, &#39;watched&#39;, &#39;wiz&#39;, &#39;watched&#39;, &#39;moonwalker&#39;, &#39;maybe&#39;, &#39;want&#39;, &#39;get&#39;, &#39;certain&#39;, &#39;insight&#39;, &#39;guy&#39;, &#39;thought&#39;, &#39;really&#39;, &#39;cool&#39;, &#39;eighties&#39;, &#39;maybe&#39;, &#39;make&#39;, &#39;mind&#39;, &#39;whether&#39;, &#39;guilty&#39;, &#39;innocent&#39;, &#39;moonwalker&#39;, &#39;part&#39;, &#39;biography&#39;, &#39;part&#39;, &#39;feature&#39;, &#39;film&#39;, &#39;remember&#39;, &#39;going&#39;, &#39;see&#39;, &#39;cinema&#39;, &#39;originally&#39;, &#39;released&#39;, &#39;subtle&#39;, &#39;messages&#39;, &#39;mj&#39;, &#39;feeling&#39;, &#39;towards&#39;, &#39;press&#39;, &#39;also&#39;, &#39;obvious&#39;, &#39;message&#39;, &#39;drugs&#39;, &#39;bad&#39;, &#39;kay&#39;, &#39;visually&#39;, &#39;impressive&#39;, &#39;course&#39;, &#39;michael&#39;, &#39;jackson&#39;, &#39;unless&#39;, &#39;remotely&#39;, &#39;like&#39;, &#39;mj&#39;, &#39;anyway&#39;, &#39;going&#39;, &#39;hate&#39;, &#39;find&#39;, &#39;boring&#39;, &#39;may&#39;, &#39;call&#39;, &#39;mj&#39;, &#39;egotist&#39;, &#39;consenting&#39;, &#39;making&#39;, &#39;movie&#39;, &#39;mj&#39;, &#39;fans&#39;, &#39;would&#39;, &#39;say&#39;, &#39;made&#39;, &#39;fans&#39;, &#39;true&#39;, &#39;really&#39;, &#39;nice&#39;, &#39;actual&#39;, &#39;feature&#39;, &#39;film&#39;, &#39;bit&#39;, &#39;finally&#39;, &#39;starts&#39;, &#39;minutes&#39;, &#39;excluding&#39;, &#39;smooth&#39;, &#39;criminal&#39;, &#39;sequence&#39;, &#39;joe&#39;, &#39;pesci&#39;, &#39;convincing&#39;, &#39;psychopathic&#39;, &#39;powerful&#39;, &#39;drug&#39;, &#39;lord&#39;, &#39;wants&#39;, &#39;mj&#39;, &#39;dead&#39;, &#39;bad&#39;, &#39;beyond&#39;, &#39;mj&#39;, &#39;overheard&#39;, &#39;plans&#39;, &#39;nah&#39;, &#39;joe&#39;, &#39;pesci&#39;, &#39;character&#39;, &#39;ranted&#39;, &#39;wanted&#39;, &#39;people&#39;, &#39;know&#39;, &#39;supplying&#39;, &#39;drugs&#39;, &#39;etc&#39;, &#39;dunno&#39;, &#39;maybe&#39;, &#39;hates&#39;, &#39;mj&#39;, &#39;music&#39;, &#39;lots&#39;, &#39;cool&#39;, &#39;things&#39;, &#39;like&#39;, &#39;mj&#39;, &#39;turning&#39;, &#39;car&#39;, &#39;robot&#39;, &#39;whole&#39;, &#39;speed&#39;, &#39;demon&#39;, &#39;sequence&#39;, &#39;also&#39;, &#39;director&#39;, &#39;must&#39;, &#39;patience&#39;, &#39;saint&#39;, &#39;came&#39;, &#39;filming&#39;, &#39;kiddy&#39;, &#39;bad&#39;, &#39;sequence&#39;, &#39;usually&#39;, &#39;directors&#39;, &#39;hate&#39;, &#39;working&#39;, &#39;one&#39;, &#39;kid&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;whole&#39;, &#39;bunch&#39;, &#39;performing&#39;, &#39;complex&#39;, &#39;dance&#39;, &#39;scene&#39;, &#39;bottom&#39;, &#39;line&#39;, &#39;movie&#39;, &#39;people&#39;, &#39;like&#39;, &#39;mj&#39;, &#39;one&#39;, &#39;level&#39;, &#39;another&#39;, &#39;think&#39;, &#39;people&#39;, &#39;stay&#39;, &#39;away&#39;, &#39;try&#39;, &#39;give&#39;, &#39;wholesome&#39;, &#39;message&#39;, &#39;ironically&#39;, &#39;mj&#39;, &#39;bestest&#39;, &#39;buddy&#39;, &#39;movie&#39;, &#39;girl&#39;, &#39;michael&#39;, &#39;jackson&#39;, &#39;truly&#39;, &#39;one&#39;, &#39;talented&#39;, &#39;people&#39;, &#39;ever&#39;, &#39;grace&#39;, &#39;planet&#39;, &#39;guilty&#39;, &#39;well&#39;, &#39;attention&#39;, &#39;gave&#39;, &#39;subject&#39;, &#39;hmmm&#39;, &#39;well&#39;, &#39;know&#39;, &#39;people&#39;, &#39;different&#39;, &#39;behind&#39;, &#39;closed&#39;, &#39;doors&#39;, &#39;know&#39;, &#39;fact&#39;, &#39;either&#39;, &#39;extremely&#39;, &#39;nice&#39;, &#39;stupid&#39;, &#39;guy&#39;, &#39;one&#39;, &#39;sickest&#39;, &#39;liars&#39;, &#39;hope&#39;, &#39;latter&#39;]</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 将清洗后的数据用向量表示train_vec=reviews_to_vec(clean_train_reviews,model)test_vec=reviews_to_vec(clean_test_reviews,model)</code></pre><pre class=" language-lang-python"><code class="language-lang-python">print(train_vec[0])</code></pre><pre><code>[-4.12214540e-02 -3.79349139e-02  4.92058553e-03  3.83373492e-02 -2.45657221e-03  8.18741966e-03 -8.79610906e-03 -1.77715521e-03  1.32088760e-02  1.10860562e-02  1.07725587e-02 -2.03826056e-03 -6.47818862e-03  2.19975994e-03  3.60669386e-03 -6.05855137e-03 -1.36809291e-02  4.23901244e-03 -8.60632074e-03 -1.22470358e-02  1.90984706e-02 -1.82547256e-02 -1.34975893e-02 -6.18288175e-03 -1.35384069e-03  1.21405336e-02  1.44483088e-02  1.13259193e-02  1.89741399e-03  5.12676010e-03 -1.39281715e-03  3.23074499e-03 -3.93561738e-03  8.09281687e-03  1.29367411e-02 -4.67313011e-03  1.23364448e-02  2.59944830e-04  1.00361077e-02  3.50261860e-03  1.37961732e-03 -5.03643784e-04 -8.41444710e-03 -1.40226181e-02 -1.58165179e-02 -3.45300662e-03  7.24950322e-03 -1.22461706e-02 -3.30349544e-03  7.58285267e-04  4.28325363e-03  1.03239213e-03  1.41914198e-02 -3.84485975e-03 -3.39659879e-03 -2.38006394e-03 -1.53069039e-04  3.38289491e-03 -9.98435193e-03 -2.60351885e-02  1.44659057e-02 -2.39534181e-03 -7.16520778e-03  9.46791547e-03 -1.04171550e-02 -3.98039501e-03  9.16166107e-03  3.70796656e-03 -5.85292180e-03 -8.93773824e-03  1.18012830e-02  1.22757631e-02 -9.76842746e-03 -1.13501498e-02 -6.58628962e-03  1.46483148e-02  8.67670502e-03  1.49435605e-02 -1.19698187e-02 -2.33288583e-03  4.06095162e-04 -1.06101402e-02 -1.60301849e-02  4.81574295e-03 -8.17539989e-03 -1.66170598e-02 -9.30875331e-04  7.81632784e-03 -9.27952358e-03 -9.36121523e-03 -1.26087192e-02 -1.80305672e-02  1.86318907e-03 -1.93715744e-02 -1.10955165e-02 -2.71567362e-03 -9.56838540e-03 -8.66433000e-03 -4.35272423e-03 -1.10486866e-02 -1.72972109e-04  2.89158798e-03  3.60159835e-03  6.59811498e-03 -6.54146849e-03 -7.31639369e-03  7.51837829e-04 -8.65462705e-03  9.15930662e-03  8.95285620e-03  1.02896624e-02  9.78818544e-04  5.63207011e-03  5.57235499e-03 -1.59190054e-03 -9.35532423e-03 -3.96249171e-03  2.50648517e-02 -5.84957375e-03  7.37975151e-03 -1.05417412e-02  1.02484963e-04 -1.50257244e-02  1.39091650e-02 -1.17712126e-02  7.77766726e-03 -2.24176808e-03  7.05043707e-04  8.67431739e-03  3.75606772e-03  9.08246107e-03  2.74517435e-03 -2.35922815e-03  1.31304846e-02 -9.95927341e-03 -2.59129936e-02  3.28526854e-03  1.28644373e-02 -1.20760067e-02  5.32581246e-03  2.49877068e-02  4.61110624e-05 -9.94929249e-05 -5.19745509e-03  3.84600100e-03  2.55921685e-02 -1.17436596e-03 -6.38690085e-03  1.30926324e-02  9.25366330e-03  5.74752866e-03  3.67009494e-03 -8.91258598e-03  4.00217369e-03  3.44281741e-03  9.58762693e-03 -2.27720639e-03 -2.32857570e-02  1.38511115e-03 -8.95528520e-04  4.61956041e-03  1.58334490e-03 -1.33953256e-02  9.33843276e-03 -1.18030649e-02 -1.69549270e-02 -7.09789496e-03  2.20391827e-02  4.45628540e-03  9.96538007e-03  1.65677878e-02 -7.59950671e-03  2.65692874e-03 -8.20766386e-03 -7.06074345e-03 -2.07987758e-03  5.14666043e-03 -2.76077065e-03 -1.95259660e-02 -4.92735231e-03  8.77612858e-03  1.00959862e-02 -1.50249958e-02 -1.11764727e-02  1.16610630e-02  7.64885400e-03  3.32668278e-03  3.01162971e-03 -1.58037759e-03 -1.41781947e-03  8.98981516e-03  1.75613990e-03  1.51191862e-02 -2.83710480e-03  1.07771752e-02  4.62977255e-03 -2.14315927e-02  8.39924651e-03 -5.06151012e-03 -7.09692200e-03 -4.08430280e-03  6.98304718e-03 -1.66823580e-02  1.03978790e-02 -3.80447526e-03 -8.81291288e-04 -7.01620637e-03 -3.37387766e-04  1.45387862e-03  2.47556191e-03 -5.43786858e-03  4.35696112e-03  9.28427696e-03 -2.98430857e-03 -1.33052969e-02 -5.22909427e-03  3.27453974e-03 -1.52650037e-02 -6.31197961e-03 -1.12691563e-02  2.51482293e-02 -9.90015215e-03 -6.63267545e-03  1.19510287e-02 -5.39413439e-03 -1.27368125e-04  1.43712242e-02 -6.93140865e-03  5.03178495e-03 -6.58613742e-03  1.14176539e-03  5.67263818e-03  5.86148271e-04  3.90761369e-03  1.96726647e-03 -5.55310410e-03 -6.06663451e-03 -8.73890181e-03 -2.43698109e-02  6.17038008e-03 -3.46167710e-03 -6.02825684e-03 -3.83132858e-03 -1.14010213e-02  1.05697507e-03 -1.25030496e-02  1.31673251e-02 -1.55891930e-03 -1.40483952e-03 -1.73462121e-03 -9.77388603e-03 -4.26941340e-04 -4.54794628e-03 -1.76137344e-02  7.31736702e-04  4.76255040e-03 -1.77207668e-03  7.87317303e-03  3.73680103e-03  4.42208364e-03 -6.03005272e-03  9.67661470e-03  1.30678449e-02  4.52407294e-03  3.43513604e-04  1.57824610e-02 -9.26215438e-04 -1.62585090e-03  6.94119523e-04 -6.83462853e-03 -5.10692202e-03 -3.03575637e-03  1.38966284e-03  1.88861779e-02  4.08470203e-03 -2.45477906e-02 -2.42524100e-02  9.13219373e-03 -1.64326158e-02 -7.67338944e-03 -1.64148907e-02  1.63434325e-02  5.82653699e-03 -8.66393099e-03 -1.04383050e-02 -2.39454473e-02  1.32276766e-02 -1.50543327e-02  4.05624122e-03  2.18779668e-02 -3.30929946e-04  8.58893936e-03  8.00503015e-03  7.86819680e-04 -6.07566445e-03  3.53654486e-03 -1.95318875e-02  1.86700427e-02  8.84436153e-03  7.88514732e-03]</code></pre><h4 id="6、建模和预测"><a href="#6、建模和预测" class="headerlink" title="6、建模和预测"></a>6、建模和预测</h4><h5 id="6、1-采用随机森林模型"><a href="#6、1-采用随机森林模型" class="headerlink" title="6、1 采用随机森林模型"></a>6、1 采用随机森林模型</h5><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.ensemble import RandomForestClassifierfrom sklearn.cross_validation import cross_val_scoreforest = RandomForestClassifier( n_estimators = 100, n_jobs=2)print("Fitting a random forest to labeled training data...")forest = forest.fit( train_vec, train['sentiment'] )print("随机森林分类器10折交叉验证得分: ", np.mean(cross_val_score(forest, train_vec, train['sentiment'], cv=10, scoring='roc_auc')))# 测试集result = forest.predict(test_vec)output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )output.to_csv( "./data/rf_word2vec.csv", index=False, quoting=3 )</code></pre><pre><code>随机森林分类器10折交叉验证得分:  0.91351312</code></pre><h5 id="6、2-采用逻辑回归模型"><a href="#6、2-采用逻辑回归模型" class="headerlink" title="6、2  采用逻辑回归模型"></a>6、2  采用逻辑回归模型</h5><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.linear_model import LogisticRegression as LRmodel = LR()                             # 逻辑回归模型model.fit(train_vec,train["sentiment"])print("10折交叉验证：")print(np.mean(cross_val_score(model,train_vec,train["sentiment"],scoring="roc_auc")))test_predicted=np.array(model.predict(test_vec))output=pd.DataFrame(data={"id":test["id"],"sentiment":test_predicted})# 写入文件output.to_csv("./data/LR_word2vec.csv",index=False,quoting=3)</code></pre><pre><code>10折交叉验证：0.927272555469929</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bag of Words Meets Bags of Popcorn(2)-tfidf</title>
      <link href="/2018/06/14/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(2)-tfidf%20(1)/"/>
      <url>/2018/06/14/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(2)-tfidf%20(1)/</url>
      
        <content type="html"><![CDATA[<p>本篇是kaggle之电影评论文本情感分类（Bag of Words Meets Bags of Popcorn）实现的第二篇，语言模型选择的是TFIDF<br>主要参考：<a href="https://www.kaggle.com/rajathmc/bag-of-words-meets-bags-of-popcorn" target="_blank" rel="noopener">https://www.kaggle.com/rajathmc/bag-of-words-meets-bags-of-popcorn</a><br><a href="https://www.cnblogs.com/lijingpeng/p/5787549.html" target="_blank" rel="noopener">https://www.cnblogs.com/lijingpeng/p/5787549.html</a><br>这两篇文章，部分地方有修改。<br><a id="more"></a></p><h4 id="0、主要思路"><a href="#0、主要思路" class="headerlink" title="0、主要思路"></a>0、主要思路</h4><p>（1）首先使用第一篇中数据清洗方法对训练集和测试集的数据进行清洗；前一篇<a href="http://bei.dreamcykj.com/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(1)-Bag%20of%20Words/#more">http://bei.dreamcykj.com/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(1)-Bag%20of%20Words/#more</a></p><p>（2）利用sklearn中文本特征提取方法将数据向量化，并对训练集和测试集提取tfidf特征；</p><p>（3）选择适当的分类算法，在训练集上训练，并在测试集上进行测试，保存预测结果。</p><pre class=" language-lang-python"><code class="language-lang-python">import pandas as pdfrom bs4 import BeautifulSoupimport refrom nltk.corpus import stopwords</code></pre><h4 id="1、读取数据"><a href="#1、读取数据" class="headerlink" title="1、读取数据"></a>1、读取数据</h4><pre class=" language-lang-python"><code class="language-lang-python">train=pd.read_csv("./data/labeledTrainData.tsv",header=0,delimiter="\t",quoting=3)test=pd.read_csv("./data/testData.tsv",header=0,delimiter="\t",quoting=3)</code></pre><h4 id="2、数据清洗"><a href="#2、数据清洗" class="headerlink" title="2、数据清洗"></a>2、数据清洗</h4><p>这里直接给出数据清洗方法，分析过程见<a href="http://bei.dreamcykj.com/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(1)-Bag%20of%20Words/#more">http://bei.dreamcykj.com/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(1)-Bag%20of%20Words/#more</a></p><pre class=" language-lang-python"><code class="language-lang-python">def review_to_words(raw_review):    # 去掉html标签    review_text=BeautifulSoup(raw_review).get_text()    # 去掉标点符号和非法字符    review_text=re.sub("[^a-zA-Z]"," ",review_text)    # 将字符全部转化为小写，并通过空格符进行分词处理    words=review_text.lower().split()    # 去停用词    stops=set(stopwords.words("english"))    meaningful_words=[w for w in words if w not in stops]    # 将剩下的词还原成str类型    cleaned_review=" ".join(meaningful_words)    return cleaned_review</code></pre><h4 id="3、提取特征"><a href="#3、提取特征" class="headerlink" title="3、提取特征"></a>3、提取特征</h4><h5 id="3、1-准备数据"><a href="#3、1-准备数据" class="headerlink" title="3、1 准备数据"></a>3、1 准备数据</h5><p>这一步和前一篇的类似，分别清洗训练集和测试集。</p><pre class=" language-lang-python"><code class="language-lang-python"># 获取数据的数量num_reviews=len(train)# 对数据进行清洗clean_train_reviews=[]print("Cleaning and parsing the training set movie reviews...\n")for i in range(0,num_reviews):    if(i+1)%1000==0:        print("Review %d of %d \n" % (i+1, num_reviews))    temp_review=review_to_words(train["review"][i])    clean_train_reviews.append(temp_review)# 对测试集数据做相同的处理num_reviews=len(test)clean_test_reviews=[]print("Cleaning and parsing the training set movie reviews...\n")for i in range(0,num_reviews):    if(i+1)%1000==0:        print("Review %d of %d \n"%(i+1,num_reviews))    temp_review=review_to_words(test["review"][i])    clean_test_reviews.append(temp_review)print(len(clean_train_reviews))print(len(clean_test_reviews))</code></pre><pre><code>Cleaning and parsing the training set movie reviews.../home/xiongzy/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I&#39;m using the best available HTML parser for this system (&quot;lxml&quot;). This usually isn&#39;t a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently. The code that caused this warning is on line 193 of the file /home/xiongzy/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this: BeautifulSoup(YOUR_MARKUP})   to this: BeautifulSoup(YOUR_MARKUP, &quot;lxml&quot;)  markup_type=markup_type))Review 1000 of 25000 Review 2000 of 25000 Review 3000 of 25000 Review 4000 of 25000 Review 5000 of 25000 Review 6000 of 25000 Review 7000 of 25000 Review 8000 of 25000 Review 9000 of 25000 Review 10000 of 25000 Review 11000 of 25000 Review 12000 of 25000 Review 13000 of 25000 Review 14000 of 25000 Review 15000 of 25000 Review 16000 of 25000 Review 17000 of 25000 Review 18000 of 25000 Review 19000 of 25000 Review 20000 of 25000 Review 21000 of 25000 Review 22000 of 25000 Review 23000 of 25000 Review 24000 of 25000 Review 25000 of 25000 Cleaning and parsing the training set movie reviews...Review 1000 of 25000 Review 2000 of 25000 Review 3000 of 25000 Review 4000 of 25000 Review 5000 of 25000 Review 6000 of 25000 Review 7000 of 25000 Review 8000 of 25000 Review 9000 of 25000 Review 10000 of 25000 Review 11000 of 25000 Review 12000 of 25000 Review 13000 of 25000 Review 14000 of 25000 Review 15000 of 25000 Review 16000 of 25000 Review 17000 of 25000 Review 18000 of 25000 Review 19000 of 25000 Review 20000 of 25000 Review 21000 of 25000 Review 22000 of 25000 Review 23000 of 25000 Review 24000 of 25000 Review 25000 of 25000 2500025000</code></pre><h5 id="3、2-数据向量化"><a href="#3、2-数据向量化" class="headerlink" title="3、2  数据向量化"></a>3、2  数据向量化</h5><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.feature_extraction.text import TfidfVectorizer as TFtfidf=TF(analyzer="word",tokenizer=None,preprocessor=None,stop_words=None,max_features=5000)# 合并训练和测试集以便进行tfidf向量化操作data_all=clean_train_reviews+clean_test_reviews# 数据向量化print("Creating the tfidf vector...\n")tfidf.fit(data_all)# 获取训练集的向量表示train_x=tfidf.transform(clean_train_reviews)train_x=train_x.toarray()# 获取测试集的向量表示test_x=tfidf.transform(clean_test_reviews)test_x=test_x.toarray()print(train_x.shape)print(test_x.shape)print("finished")</code></pre><pre><code>Creating the tfidf vector...(25000, 5000)(25000, 5000)finished</code></pre><h4 id="4、-建模和训练"><a href="#4、-建模和训练" class="headerlink" title="4、 建模和训练"></a>4、 建模和训练</h4><p>这里分别对朴素贝叶斯和逻辑回归进行了测试，发现逻辑回归的效果较好。</p><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.naive_bayes import MultinomialNB as MNBfrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.grid_search import GridSearchCV</code></pre><pre><code>/home/xiongzy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.  &quot;This module will be removed in 0.20.&quot;, DeprecationWarning)/home/xiongzy/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.  DeprecationWarning)</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># model=MNB(alpha=1.0, class_prior=None, fit_prior=True)     # 朴素贝叶斯模型model = LR()                             # 逻辑回归模型model.fit(train_x,train["sentiment"])</code></pre><pre><code>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,          intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,          penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,          verbose=0, warm_start=False)</code></pre><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.cross_validation import cross_val_score</code></pre><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npprint("10折交叉验证：")print(np.mean(cross_val_score(model,train_x,train["sentiment"],scoring="roc_auc")))</code></pre><pre><code>10折交叉验证：0.9498602042713286</code></pre><pre class=" language-lang-python"><code class="language-lang-python">test_predicted=np.array(model.predict(test_x))</code></pre><pre class=" language-lang-python"><code class="language-lang-python">output=pd.DataFrame(data={"id":test["id"],"sentiment":test_predicted})# 写入文件output.to_csv("./data/tfidf.csv",index=False,quoting=3)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bag of Words Meets Bags of Popcorn(1)-Bag of Words</title>
      <link href="/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(1)-Bag%20of%20Words/"/>
      <url>/2018/06/13/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn(1)-Bag%20of%20Words/</url>
      
        <content type="html"><![CDATA[<p>本篇是kaggle之电影评论文本情感分类（Bag of Words Meets Bags of Popcorn）的实现，主要参照Rajath Chidananda的《Bag of Words Meets Bags of Popcorn》，整体是按照他的流程来走的，对每一步都加上了注释，也对相应点给出了参考资料链接。<a id="more"></a></p><h2 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h2><h3 id="1-1-查看数据格式"><a href="#1-1-查看数据格式" class="headerlink" title="1.1 查看数据格式"></a>1.1 查看数据格式</h3><pre class=" language-lang-python"><code class="language-lang-python">import pandas as pd# 读取数据train=pd.read_csv("./data/labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)# 查看数据的大小train.shape</code></pre><pre><code>(25000, 3)</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 查看数据的字段train.columns</code></pre><pre><code>Index([&#39;id&#39;, &#39;sentiment&#39;, &#39;review&#39;], dtype=&#39;object&#39;)</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 查看数据格式# train.head()</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 查看完整的一条评论train['review'][0]</code></pre><pre><code>&#39;&quot;With all this stuff going down at the moment with MJ i\&#39;ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\&#39;s feeling towards the press and also the obvious message of drugs are bad m\&#39;kay.&lt;br /&gt;&lt;br /&gt;Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.&lt;br /&gt;&lt;br /&gt;The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\&#39;s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\&#39;s music.&lt;br /&gt;&lt;br /&gt;Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.&lt;br /&gt;&lt;br /&gt;Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\&#39;s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\&#39;ve gave this subject....hmmm well i don\&#39;t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.&quot;&#39;</code></pre><h3 id="1-2-数据清洗"><a href="#1-2-数据清洗" class="headerlink" title="1.2 数据清洗"></a>1.2 数据清洗</h3><p>观察这条数据，发现有html标签，需要从html标签文件中抽取出文本，有标点符号，有大小写，介词、冠词较多，那么接下来就对数据进行清洗，</p><h4 id="1-2-1-清洗1——去除html标签"><a href="#1-2-1-清洗1——去除html标签" class="headerlink" title="1.2.1 清洗1——去除html标签"></a>1.2.1 清洗1——去除html标签</h4><pre class=" language-lang-python"><code class="language-lang-python">from bs4 import BeautifulSoupexample1=BeautifulSoup(train['review'][0])</code></pre><pre><code>/home/xiongzy/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I&#39;m using the best available HTML parser for this system (&quot;lxml&quot;). This usually isn&#39;t a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.The code that caused this warning is on line 193 of the file /home/xiongzy/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this: BeautifulSoup(YOUR_MARKUP})to this: BeautifulSoup(YOUR_MARKUP, &quot;lxml&quot;)  markup_type=markup_type))</code></pre><p>example1就是bs对象，调用get_text()方法即可获取文本<br>我们首先再输出一下原始文本，然后输出第一次清洗后的文本</p><pre class=" language-lang-python"><code class="language-lang-python">train['review'][0]</code></pre><pre><code>&#39;&quot;With all this stuff going down at the moment with MJ i\&#39;ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\&#39;s feeling towards the press and also the obvious message of drugs are bad m\&#39;kay.&lt;br /&gt;&lt;br /&gt;Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.&lt;br /&gt;&lt;br /&gt;The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\&#39;s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\&#39;s music.&lt;br /&gt;&lt;br /&gt;Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.&lt;br /&gt;&lt;br /&gt;Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\&#39;s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\&#39;ve gave this subject....hmmm well i don\&#39;t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.&quot;&#39;</code></pre><pre class=" language-lang-python"><code class="language-lang-python">example1.get_text()</code></pre><pre><code>&#39;&quot;With all this stuff going down at the moment with MJ i\&#39;ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\&#39;s feeling towards the press and also the obvious message of drugs are bad m\&#39;kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\&#39;s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\&#39;s music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\&#39;s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\&#39;ve gave this subject....hmmm well i don\&#39;t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.&quot;&#39;</code></pre><h4 id="1-2-2-清洗2——去掉标点符号和非法字符"><a href="#1-2-2-清洗2——去掉标点符号和非法字符" class="headerlink" title="1.2.2 清洗2——去掉标点符号和非法字符"></a>1.2.2 清洗2——去掉标点符号和非法字符</h4><p>然后发现文本中还有一些标点符号啥的，可以考虑用正则表达式去掉</p><p>re.sub(pattern, repl, string, count=0, flags=0)</p><p>pattern：表示正则表达式中的模式字符串；</p><p>repl：被替换的字符串（既可以是字符串，也可以是函数）；</p><p>string：要被处理的，要被替换的字符串；</p><p>count：匹配的次数, 默认是全部替换</p><p>flags：具体用处不详</p><p>具体strip(),replace()和re.sub()用法:<a href="https://www.cnblogs.com/sshcy/p/8065113.html" target="_blank" rel="noopener">https://www.cnblogs.com/sshcy/p/8065113.html</a></p><pre class=" language-lang-python"><code class="language-lang-python">import re letters_only=re.sub("[^a-zA-Z]"," ",example1.get_text()) # 注意，这里是将非字母字符替换成空白字符print(letters_only)</code></pre><pre><code> With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  </code></pre><h4 id="1-2-3-清洗3——数据归一化"><a href="#1-2-3-清洗3——数据归一化" class="headerlink" title="1.2.3 清洗3——数据归一化"></a>1.2.3 清洗3——数据归一化</h4><p>看到评论中数据，每条评论记录中的单词大小写都存在，于是可以考虑把所以字符改为小写，然后再以空格为分割符进行切割,得到一条评论的一个列表，列表元素为这条评论的单词</p><pre class=" language-lang-python"><code class="language-lang-python">lower_case=letters_only.lower()words=lower_case.split() # split默认是以空格为分割符# print(words)</code></pre><h4 id="1-2-4-清洗4——去停用词"><a href="#1-2-4-清洗4——去停用词" class="headerlink" title="1.2.4 清洗4——去停用词"></a>1.2.4 清洗4——去停用词</h4><p>因为是英文数据，于是可以考虑直接使用nltk提供的英文停用词表，nltk里好像有9门语言的停用词表，目前不支持中文。</p><p>使用停用词表需要预先下载ntlk_data,我已经上传百度云:链接：<a href="https://pan.baidu.com/s/1E-eKAZg5aEBRTqSYG24Tng" target="_blank" rel="noopener">https://pan.baidu.com/s/1E-eKAZg5aEBRTqSYG24Tng</a> 密码：lrbe</p><pre class=" language-lang-python"><code class="language-lang-python"># 查看停用词表中的单词import nltk# from nltk.corpus import stopwords# print(stopwords.words("english"))# words = [w for w in words if not w in stopwords.words("english")]print(words)</code></pre><pre><code>[&#39;with&#39;, &#39;all&#39;, &#39;this&#39;, &#39;stuff&#39;, &#39;going&#39;, &#39;down&#39;, &#39;at&#39;, &#39;the&#39;, &#39;moment&#39;, &#39;with&#39;, &#39;mj&#39;, &#39;i&#39;, &#39;ve&#39;, &#39;started&#39;, &#39;listening&#39;, &#39;to&#39;, &#39;his&#39;, &#39;music&#39;, &#39;watching&#39;, &#39;the&#39;, &#39;odd&#39;, &#39;documentary&#39;, &#39;here&#39;, &#39;and&#39;, &#39;there&#39;, &#39;watched&#39;, &#39;the&#39;, &#39;wiz&#39;, &#39;and&#39;, &#39;watched&#39;, &#39;moonwalker&#39;, &#39;again&#39;, &#39;maybe&#39;, &#39;i&#39;, &#39;just&#39;, &#39;want&#39;, &#39;to&#39;, &#39;get&#39;, &#39;a&#39;, &#39;certain&#39;, &#39;insight&#39;, &#39;into&#39;, &#39;this&#39;, &#39;guy&#39;, &#39;who&#39;, &#39;i&#39;, &#39;thought&#39;, &#39;was&#39;, &#39;really&#39;, &#39;cool&#39;, &#39;in&#39;, &#39;the&#39;, &#39;eighties&#39;, &#39;just&#39;, &#39;to&#39;, &#39;maybe&#39;, &#39;make&#39;, &#39;up&#39;, &#39;my&#39;, &#39;mind&#39;, &#39;whether&#39;, &#39;he&#39;, &#39;is&#39;, &#39;guilty&#39;, &#39;or&#39;, &#39;innocent&#39;, &#39;moonwalker&#39;, &#39;is&#39;, &#39;part&#39;, &#39;biography&#39;, &#39;part&#39;, &#39;feature&#39;, &#39;film&#39;, &#39;which&#39;, &#39;i&#39;, &#39;remember&#39;, &#39;going&#39;, &#39;to&#39;, &#39;see&#39;, &#39;at&#39;, &#39;the&#39;, &#39;cinema&#39;, &#39;when&#39;, &#39;it&#39;, &#39;was&#39;, &#39;originally&#39;, &#39;released&#39;, &#39;some&#39;, &#39;of&#39;, &#39;it&#39;, &#39;has&#39;, &#39;subtle&#39;, &#39;messages&#39;, &#39;about&#39;, &#39;mj&#39;, &#39;s&#39;, &#39;feeling&#39;, &#39;towards&#39;, &#39;the&#39;, &#39;press&#39;, &#39;and&#39;, &#39;also&#39;, &#39;the&#39;, &#39;obvious&#39;, &#39;message&#39;, &#39;of&#39;, &#39;drugs&#39;, &#39;are&#39;, &#39;bad&#39;, &#39;m&#39;, &#39;kay&#39;, &#39;visually&#39;, &#39;impressive&#39;, &#39;but&#39;, &#39;of&#39;, &#39;course&#39;, &#39;this&#39;, &#39;is&#39;, &#39;all&#39;, &#39;about&#39;, &#39;michael&#39;, &#39;jackson&#39;, &#39;so&#39;, &#39;unless&#39;, &#39;you&#39;, &#39;remotely&#39;, &#39;like&#39;, &#39;mj&#39;, &#39;in&#39;, &#39;anyway&#39;, &#39;then&#39;, &#39;you&#39;, &#39;are&#39;, &#39;going&#39;, &#39;to&#39;, &#39;hate&#39;, &#39;this&#39;, &#39;and&#39;, &#39;find&#39;, &#39;it&#39;, &#39;boring&#39;, &#39;some&#39;, &#39;may&#39;, &#39;call&#39;, &#39;mj&#39;, &#39;an&#39;, &#39;egotist&#39;, &#39;for&#39;, &#39;consenting&#39;, &#39;to&#39;, &#39;the&#39;, &#39;making&#39;, &#39;of&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;but&#39;, &#39;mj&#39;, &#39;and&#39;, &#39;most&#39;, &#39;of&#39;, &#39;his&#39;, &#39;fans&#39;, &#39;would&#39;, &#39;say&#39;, &#39;that&#39;, &#39;he&#39;, &#39;made&#39;, &#39;it&#39;, &#39;for&#39;, &#39;the&#39;, &#39;fans&#39;, &#39;which&#39;, &#39;if&#39;, &#39;true&#39;, &#39;is&#39;, &#39;really&#39;, &#39;nice&#39;, &#39;of&#39;, &#39;him&#39;, &#39;the&#39;, &#39;actual&#39;, &#39;feature&#39;, &#39;film&#39;, &#39;bit&#39;, &#39;when&#39;, &#39;it&#39;, &#39;finally&#39;, &#39;starts&#39;, &#39;is&#39;, &#39;only&#39;, &#39;on&#39;, &#39;for&#39;, &#39;minutes&#39;, &#39;or&#39;, &#39;so&#39;, &#39;excluding&#39;, &#39;the&#39;, &#39;smooth&#39;, &#39;criminal&#39;, &#39;sequence&#39;, &#39;and&#39;, &#39;joe&#39;, &#39;pesci&#39;, &#39;is&#39;, &#39;convincing&#39;, &#39;as&#39;, &#39;a&#39;, &#39;psychopathic&#39;, &#39;all&#39;, &#39;powerful&#39;, &#39;drug&#39;, &#39;lord&#39;, &#39;why&#39;, &#39;he&#39;, &#39;wants&#39;, &#39;mj&#39;, &#39;dead&#39;, &#39;so&#39;, &#39;bad&#39;, &#39;is&#39;, &#39;beyond&#39;, &#39;me&#39;, &#39;because&#39;, &#39;mj&#39;, &#39;overheard&#39;, &#39;his&#39;, &#39;plans&#39;, &#39;nah&#39;, &#39;joe&#39;, &#39;pesci&#39;, &#39;s&#39;, &#39;character&#39;, &#39;ranted&#39;, &#39;that&#39;, &#39;he&#39;, &#39;wanted&#39;, &#39;people&#39;, &#39;to&#39;, &#39;know&#39;, &#39;it&#39;, &#39;is&#39;, &#39;he&#39;, &#39;who&#39;, &#39;is&#39;, &#39;supplying&#39;, &#39;drugs&#39;, &#39;etc&#39;, &#39;so&#39;, &#39;i&#39;, &#39;dunno&#39;, &#39;maybe&#39;, &#39;he&#39;, &#39;just&#39;, &#39;hates&#39;, &#39;mj&#39;, &#39;s&#39;, &#39;music&#39;, &#39;lots&#39;, &#39;of&#39;, &#39;cool&#39;, &#39;things&#39;, &#39;in&#39;, &#39;this&#39;, &#39;like&#39;, &#39;mj&#39;, &#39;turning&#39;, &#39;into&#39;, &#39;a&#39;, &#39;car&#39;, &#39;and&#39;, &#39;a&#39;, &#39;robot&#39;, &#39;and&#39;, &#39;the&#39;, &#39;whole&#39;, &#39;speed&#39;, &#39;demon&#39;, &#39;sequence&#39;, &#39;also&#39;, &#39;the&#39;, &#39;director&#39;, &#39;must&#39;, &#39;have&#39;, &#39;had&#39;, &#39;the&#39;, &#39;patience&#39;, &#39;of&#39;, &#39;a&#39;, &#39;saint&#39;, &#39;when&#39;, &#39;it&#39;, &#39;came&#39;, &#39;to&#39;, &#39;filming&#39;, &#39;the&#39;, &#39;kiddy&#39;, &#39;bad&#39;, &#39;sequence&#39;, &#39;as&#39;, &#39;usually&#39;, &#39;directors&#39;, &#39;hate&#39;, &#39;working&#39;, &#39;with&#39;, &#39;one&#39;, &#39;kid&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;a&#39;, &#39;whole&#39;, &#39;bunch&#39;, &#39;of&#39;, &#39;them&#39;, &#39;performing&#39;, &#39;a&#39;, &#39;complex&#39;, &#39;dance&#39;, &#39;scene&#39;, &#39;bottom&#39;, &#39;line&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;is&#39;, &#39;for&#39;, &#39;people&#39;, &#39;who&#39;, &#39;like&#39;, &#39;mj&#39;, &#39;on&#39;, &#39;one&#39;, &#39;level&#39;, &#39;or&#39;, &#39;another&#39;, &#39;which&#39;, &#39;i&#39;, &#39;think&#39;, &#39;is&#39;, &#39;most&#39;, &#39;people&#39;, &#39;if&#39;, &#39;not&#39;, &#39;then&#39;, &#39;stay&#39;, &#39;away&#39;, &#39;it&#39;, &#39;does&#39;, &#39;try&#39;, &#39;and&#39;, &#39;give&#39;, &#39;off&#39;, &#39;a&#39;, &#39;wholesome&#39;, &#39;message&#39;, &#39;and&#39;, &#39;ironically&#39;, &#39;mj&#39;, &#39;s&#39;, &#39;bestest&#39;, &#39;buddy&#39;, &#39;in&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;is&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;michael&#39;, &#39;jackson&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;one&#39;, &#39;of&#39;, &#39;the&#39;, &#39;most&#39;, &#39;talented&#39;, &#39;people&#39;, &#39;ever&#39;, &#39;to&#39;, &#39;grace&#39;, &#39;this&#39;, &#39;planet&#39;, &#39;but&#39;, &#39;is&#39;, &#39;he&#39;, &#39;guilty&#39;, &#39;well&#39;, &#39;with&#39;, &#39;all&#39;, &#39;the&#39;, &#39;attention&#39;, &#39;i&#39;, &#39;ve&#39;, &#39;gave&#39;, &#39;this&#39;, &#39;subject&#39;, &#39;hmmm&#39;, &#39;well&#39;, &#39;i&#39;, &#39;don&#39;, &#39;t&#39;, &#39;know&#39;, &#39;because&#39;, &#39;people&#39;, &#39;can&#39;, &#39;be&#39;, &#39;different&#39;, &#39;behind&#39;, &#39;closed&#39;, &#39;doors&#39;, &#39;i&#39;, &#39;know&#39;, &#39;this&#39;, &#39;for&#39;, &#39;a&#39;, &#39;fact&#39;, &#39;he&#39;, &#39;is&#39;, &#39;either&#39;, &#39;an&#39;, &#39;extremely&#39;, &#39;nice&#39;, &#39;but&#39;, &#39;stupid&#39;, &#39;guy&#39;, &#39;or&#39;, &#39;one&#39;, &#39;of&#39;, &#39;the&#39;, &#39;most&#39;, &#39;sickest&#39;, &#39;liars&#39;, &#39;i&#39;, &#39;hope&#39;, &#39;he&#39;, &#39;is&#39;, &#39;not&#39;, &#39;the&#39;, &#39;latter&#39;]</code></pre><h3 id="1-3-小结"><a href="#1-3-小结" class="headerlink" title="1.3 小结"></a>1.3 小结</h3><p>首先对数据的格式、基本信息进行了观察，主要如下：</p><p>（1）数据大小：(25000, 3)</p><p>（2）每条记录的字段：[‘id’, ‘sentiment’,’review’]</p><p>接着对数据进行了清洗：去除html标签、去标点符号、转化为小写、去停用词，可以把数据清理这个过程写成一个函数</p><pre class=" language-lang-python"><code class="language-lang-python"># 需要映入的库import pandas as pdfrom bs4 import BeautifulSoupimport re import nltkfrom nltk.corpus import stopwords</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 数据清理函数def review_to_words(raw_review):    # 通过BeautifulSoup抽取纯文本    review_text=BeautifulSoup(raw_review).get_text()    # 通过正则表达式去掉标点符号    letters_only=re.sub("[^a-zA-Z]"," ",review_text)    # 转化为小写，并进行分词    words=letters_only.lower().split()    # 去停用词    stops = set(stopwords.words("english"))    meaningful_words=[w for w in words if w not in stops]    # 将处理后的数据返回    cleaned_review=" ".join(meaningful_words)    return cleaned_review</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 对编写的函数进行测试clean_review = review_to_words(train["review"][0])print(clean_review)</code></pre><pre><code>stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter/home/xiongzy/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I&#39;m using the best available HTML parser for this system (&quot;lxml&quot;). This usually isn&#39;t a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.The code that caused this warning is on line 193 of the file /home/xiongzy/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this: BeautifulSoup(YOUR_MARKUP})to this: BeautifulSoup(YOUR_MARKUP, &quot;lxml&quot;)  markup_type=markup_type))</code></pre><h2 id="2、特征提取"><a href="#2、特征提取" class="headerlink" title="2、特征提取"></a>2、特征提取</h2><p>经过前面的分析，可以通过前面定义的方法来清洗数据。数据清洗好后，接下来就是提取每一条评论数据的特征，这里采用词带特征</p><h3 id="2-1-准备数据"><a href="#2-1-准备数据" class="headerlink" title="2.1 准备数据"></a>2.1 准备数据</h3><p>使用在准备工作中定义的方法来清洗数据</p><pre class=" language-lang-python"><code class="language-lang-python"># 获取数据的数量num_reviews=len(train)# 对数据进行清洗clean_train_reviews=[]print("Cleaning and parsing the training set movie reviews...\n")for i in range(0,num_reviews):    if((i+1)%1000 == 0):        print("Review %d of %d \n" % (i+1, num_reviews))    temp_review=review_to_words(train["review"][i])    clean_train_reviews.append(temp_review)clean_train_reviews[:5]</code></pre><pre><code>Cleaning and parsing the training set movie reviews.../home/xiongzy/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I&#39;m using the best available HTML parser for this system (&quot;lxml&quot;). This usually isn&#39;t a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.The code that caused this warning is on line 193 of the file /home/xiongzy/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this: BeautifulSoup(YOUR_MARKUP})to this: BeautifulSoup(YOUR_MARKUP, &quot;lxml&quot;)  markup_type=markup_type))Review 1000 of 25000 Review 2000 of 25000 Review 3000 of 25000 Review 4000 of 25000 Review 5000 of 25000 Review 6000 of 25000 Review 7000 of 25000 Review 8000 of 25000 Review 9000 of 25000 Review 10000 of 25000 Review 11000 of 25000 Review 12000 of 25000 Review 13000 of 25000 Review 14000 of 25000 Review 15000 of 25000 Review 16000 of 25000 Review 17000 of 25000 Review 18000 of 25000 Review 19000 of 25000 Review 20000 of 25000 Review 21000 of 25000 Review 22000 of 25000 Review 23000 of 25000 Review 24000 of 25000 Review 25000 of 25000 [&#39;stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter&#39;, &#39;classic war worlds timothy hines entertaining film obviously goes great effort lengths faithfully recreate h g wells classic book mr hines succeeds watched film appreciated fact standard predictable hollywood fare comes every year e g spielberg version tom cruise slightest resemblance book obviously everyone looks different things movie envision amateur critics look criticize everything others rate movie important bases like entertained people never agree critics enjoyed effort mr hines put faithful h g wells classic novel found entertaining made easy overlook critics perceive shortcomings&#39;, &#39;film starts manager nicholas bell giving welcome investors robert carradine primal park secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one nature fearsome predators sabretooth tiger smilodon scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific meanwhile youngsters enter restricted area security center attacked pack large pre historical animals deadlier bigger addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star stars astounding terrifyingly though convincing giant animals savagely stalking prey group run afoul fight one nature fearsome predators furthermore third sabretooth dangerous slow stalks victims movie delivers goods lots blood gore beheading hair raising chills full scares sabretooths appear mediocre special effects story provides exciting stirring entertainment results quite boring giant animals majority made computer generator seem totally lousy middling performances though players reacting appropriately becoming food actors give vigorously physical performances dodging beasts running bound leaps dangling walls packs ridiculous final deadly scene small kids realistic gory violent attack scenes films sabretooths smilodon following sabretooth james r hickox vanessa angel david keith john rhys davies much better bc roland emmerich steven strait cliff curtis camilla belle motion picture filled bloody moments badly directed george miller originality takes many elements previous films miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe rating average bottom barrel&#39;, &#39;must assumed praised film greatest filmed opera ever read somewhere either care opera care wagner care anything except desire appear cultured either representation wagner swan song movie strikes unmitigated disaster leaden reading score matched tricksy lugubrious realisation text questionable people ideas opera matter play especially one shakespeare allowed anywhere near theatre film studio syberberg fashionably without smallest justification wagner text decided parsifal bisexual integration title character latter stages transmutes kind beatnik babe though one continues sing high tenor actors film singers get double dose armin jordan conductor seen face heard voice amfortas also appears monstrously double exposure kind batonzilla conductor ate monsalvat playing good friday music way transcendant loveliness nature represented scattering shopworn flaccid crocuses stuck ill laid turf expedient baffles theatre sometimes piece imperfections thoughts think syberberg splice parsifal gurnemanz mountain pasture lush provided julie andrews sound music sound hard endure high voices trumpets particular possessing aural glare adds another sort fatigue impatience uninspired conducting paralytic unfolding ritual someone another review mentioned bayreuth recording knappertsbusch though tempi often slow jordan altogether lacks sense pulse feeling ebb flow music half century orchestral sound set modern pressings still superior film&#39;, &#39;superbly trashy wondrously unpretentious exploitation hooray pre credits opening sequences somewhat give false impression dealing serious harrowing drama need fear barely ten minutes later necks nonsensical chainsaw battles rough fist fights lurid dialogs gratuitous nudity bo ingrid two orphaned siblings unusually close even slightly perverted relationship imagine playfully ripping towel covers sister naked body stare unshaven genitals several whole minutes well bo sister judging dubbed laughter mind sick dude anyway kids fled russia parents nasty soldiers brutally slaughtered mommy daddy friendly smuggler took custody however even raised trained bo ingrid expert smugglers actual plot lifts years later facing ultimate quest mythical incredibly valuable white fire diamond coincidentally found mine things life ever made little sense plot narrative structure white fire sure lot fun watch time clue beating cause bet actors understood even less whatever violence magnificently grotesque every single plot twist pleasingly retarded script goes totally bonkers beyond repair suddenly reveal reason bo needs replacement ingrid fred williamson enters scene big cigar mouth sleazy black fingers local prostitutes bo principal opponent italian chick big breasts hideous accent preposterous catchy theme song plays least dozen times throughout film obligatory falling love montage loads attractions god brilliant experience original french title translates life survive uniquely appropriate makes much sense rest movie none&#39;]</code></pre><h3 id="2-2-提取数据特征"><a href="#2-2-提取数据特征" class="headerlink" title="2.2 提取数据特征"></a>2.2 提取数据特征</h3><p>这里借助于sklearn来提取数据的词带特征,Scikit Learn CountVectorizer 入门实例:<br><a href="https://blog.csdn.net/guotong1988/article/details/51567562" target="_blank" rel="noopener">https://blog.csdn.net/guotong1988/article/details/51567562</a></p><p>参数介绍：<br><a href="http://www.itkeyword.com/doc/4813494854317445586/TfidfVectorizer-sklearn-CountVectorizer" target="_blank" rel="noopener">http://www.itkeyword.com/doc/4813494854317445586/TfidfVectorizer-sklearn-CountVectorizer</a></p><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.feature_extraction.text import CountVectorizervectorizer=CountVectorizer(analyzer="word",tokenizer=None,preprocessor=None,stop_words=None,max_features=5000)print("Creating the bag of words...\n")# 将每一条评论的特征矩阵转化为数组train_data_features = vectorizer.fit_transform(clean_train_reviews)train_data_features = train_data_features.toarray()# 查看数据特征维度print(train_data_features.shape)</code></pre><pre><code>Creating the bag of words...(25000, 5000)</code></pre><p>也就是每条记录提取了5000个特征。</p><p>接下来可以查看特征的名字,因为我们创建的是词带特征，因此所有的特征组成词汇表</p><p>也可以对特征进行统计，统计每个特征出现的次数</p><pre class=" language-lang-python"><code class="language-lang-python">vocab=vectorizer.get_feature_names()print(vocab)</code></pre><pre><code>[&#39;abandoned&#39;, &#39;abc&#39;, &#39;abilities&#39;, &#39;ability&#39;, &#39;able&#39;, &#39;abraham&#39;, &#39;absence&#39;, &#39;absent&#39;, &#39;absolute&#39;, &#39;absolutely&#39;, &#39;absurd&#39;, &#39;abuse&#39;, &#39;abusive&#39;, &#39;abysmal&#39;, &#39;academy&#39;, &#39;accent&#39;, &#39;accents&#39;, &#39;accept&#39;, &#39;acceptable&#39;, &#39;accepted&#39;, &#39;access&#39;, &#39;accident&#39;, &#39;accidentally&#39;, &#39;accompanied&#39;, &#39;accomplished&#39;, &#39;according&#39;, &#39;account&#39;, &#39;accuracy&#39;, &#39;accurate&#39;, &#39;accused&#39;, &#39;achieve&#39;, &#39;achieved&#39;, &#39;achievement&#39;, &#39;acid&#39;, &#39;across&#39;, &#39;act&#39;, &#39;acted&#39;, &#39;acting&#39;, &#39;action&#39;, &#39;actions&#39;, &#39;activities&#39;, &#39;actor&#39;, &#39;actors&#39;, &#39;actress&#39;, &#39;actresses&#39;, &#39;acts&#39;, &#39;actual&#39;, &#39;actually&#39;, &#39;ad&#39;, &#39;adam&#39;, &#39;adams&#39;, &#39;adaptation&#39;, &#39;adaptations&#39;, &#39;adapted&#39;, &#39;add&#39;, &#39;added&#39;, &#39;adding&#39;, &#39;addition&#39;, &#39;adds&#39;, &#39;adequate&#39;, &#39;admire&#39;, &#39;admit&#39;, &#39;admittedly&#39;, &#39;adorable&#39;, &#39;adult&#39;, &#39;adults&#39;, &#39;advance&#39;, &#39;advanced&#39;, &#39;advantage&#39;, &#39;adventure&#39;, &#39;adventures&#39;, &#39;advertising&#39;, &#39;advice&#39;, &#39;advise&#39;, &#39;affair&#39;, &#39;affect&#39;, &#39;affected&#39;, &#39;afford&#39;, &#39;aforementioned&#39;, &#39;afraid&#39;, &#39;africa&#39;, &#39;african&#39;, &#39;afternoon&#39;, &#39;afterwards&#39;, &#39;age&#39;, &#39;aged&#39;, &#39;agent&#39;, &#39;agents&#39;, &#39;ages&#39;, &#39;aging&#39;, &#39;ago&#39;, &#39;agree&#39;, &#39;agreed&#39;, &#39;agrees&#39;, &#39;ah&#39;, &#39;ahead&#39;, &#39;aid&#39;, &#39;aids&#39;, &#39;aim&#39;, &#39;aimed&#39;, &#39;air&#39;, &#39;aired&#39;, &#39;airplane&#39;, &#39;airport&#39;, &#39;aka&#39;, &#39;akshay&#39;, &#39;al&#39;, &#39;alan&#39;, &#39;alas&#39;, &#39;albeit&#39;, &#39;albert&#39;, &#39;album&#39;, &#39;alcohol&#39;, &#39;alcoholic&#39;, &#39;alec&#39;, &#39;alert&#39;, &#39;alex&#39;, &#39;alexander&#39;, &#39;alfred&#39;, &#39;alice&#39;, &#39;alicia&#39;, &#39;alien&#39;, &#39;aliens&#39;, &#39;alike&#39;, &#39;alison&#39;, &#39;alive&#39;, &#39;allen&#39;, &#39;allow&#39;, &#39;allowed&#39;, &#39;allowing&#39;, &#39;allows&#39;, &#39;almost&#39;, &#39;alone&#39;, &#39;along&#39;, &#39;alongside&#39;, &#39;already&#39;, &#39;alright&#39;, &#39;also&#39;, &#39;alternate&#39;, &#39;alternative&#39;, &#39;although&#39;, &#39;altman&#39;, &#39;altogether&#39;, &#39;always&#39;, &#39;amanda&#39;, &#39;amateur&#39;, &#39;amateurish&#39;, &#39;amazed&#39;, &#39;amazing&#39;, &#39;amazingly&#39;, &#39;ambiguous&#39;, &#39;ambitious&#39;, &#39;america&#39;, &#39;american&#39;, &#39;americans&#39;, &#39;amitabh&#39;, &#39;among&#39;, &#39;amongst&#39;, &#39;amount&#39;, &#39;amounts&#39;, &#39;amused&#39;, &#39;amusing&#39;, &#39;amy&#39;, &#39;analysis&#39;, &#39;ancient&#39;, &#39;anderson&#39;, &#39;andre&#39;, &#39;andrew&#39;, &#39;andrews&#39;, &#39;andy&#39;, &#39;angel&#39;, &#39;angela&#39;, &#39;angeles&#39;, &#39;angels&#39;, &#39;anger&#39;, &#39;angle&#39;, &#39;angles&#39;, &#39;angry&#39;, &#39;animal&#39;, &#39;animals&#39;, &#39;animated&#39;, &#39;animation&#39;, &#39;anime&#39;, &#39;ann&#39;, &#39;anna&#39;, &#39;anne&#39;, &#39;annie&#39;, &#39;annoyed&#39;, &#39;annoying&#39;, &#39;another&#39;, &#39;answer&#39;, &#39;answers&#39;, &#39;anthony&#39;, &#39;anti&#39;, &#39;antics&#39;, &#39;antonioni&#39;, &#39;antwone&#39;, &#39;anybody&#39;, &#39;anymore&#39;, &#39;anyone&#39;, &#39;anything&#39;, &#39;anyway&#39;, &#39;anyways&#39;, &#39;anywhere&#39;, &#39;apart&#39;, &#39;apartment&#39;, &#39;ape&#39;, &#39;apes&#39;, &#39;appalling&#39;, &#39;apparent&#39;, &#39;apparently&#39;, &#39;appeal&#39;, &#39;appealing&#39;, &#39;appear&#39;, &#39;appearance&#39;, &#39;appearances&#39;, &#39;appeared&#39;, &#39;appearing&#39;, &#39;appears&#39;, &#39;appreciate&#39;, &#39;appreciated&#39;, &#39;appreciation&#39;, &#39;approach&#39;, &#39;appropriate&#39;, &#39;april&#39;, &#39;area&#39;, &#39;areas&#39;, &#39;arguably&#39;, &#39;argue&#39;, &#39;argument&#39;, &#39;arm&#39;, &#39;armed&#39;, &#39;arms&#39;, &#39;army&#39;, &#39;arnold&#39;, &#39;around&#39;, &#39;arrested&#39;, &#39;arrival&#39;, &#39;arrive&#39;, &#39;arrived&#39;, &#39;arrives&#39;, &#39;arrogant&#39;, &#39;art&#39;, &#39;arthur&#39;, &#39;artificial&#39;, &#39;artist&#39;, &#39;artistic&#39;, &#39;artists&#39;, &#39;arts&#39;, &#39;ashamed&#39;, &#39;ashley&#39;, &#39;asian&#39;, &#39;aside&#39;, &#39;ask&#39;, &#39;asked&#39;, &#39;asking&#39;, &#39;asks&#39;, &#39;asleep&#39;, &#39;aspect&#39;, &#39;aspects&#39;, &#39;ass&#39;, &#39;assassin&#39;, &#39;assault&#39;, &#39;assigned&#39;, &#39;assistant&#39;, &#39;associated&#39;, &#39;assume&#39;, &#39;assumed&#39;, &#39;astaire&#39;, &#39;astonishing&#39;, &#39;atlantis&#39;, &#39;atmosphere&#39;, &#39;atmospheric&#39;, &#39;atrocious&#39;, &#39;attached&#39;, &#39;attack&#39;, &#39;attacked&#39;, &#39;attacks&#39;, &#39;attempt&#39;, &#39;attempted&#39;, &#39;attempting&#39;, &#39;attempts&#39;, &#39;attend&#39;, &#39;attention&#39;, &#39;attitude&#39;, &#39;attitudes&#39;, &#39;attorney&#39;, &#39;attracted&#39;, &#39;attraction&#39;, &#39;attractive&#39;, &#39;audience&#39;, &#39;audiences&#39;, &#39;audio&#39;, &#39;aunt&#39;, &#39;austen&#39;, &#39;austin&#39;, &#39;australia&#39;, &#39;australian&#39;, &#39;authentic&#39;, &#39;author&#39;, &#39;authority&#39;, &#39;available&#39;, &#39;average&#39;, &#39;avoid&#39;, &#39;avoided&#39;, &#39;awake&#39;, &#39;award&#39;, &#39;awards&#39;, &#39;aware&#39;, &#39;away&#39;, &#39;awe&#39;, &#39;awesome&#39;, &#39;awful&#39;, &#39;awfully&#39;, &#39;awhile&#39;, &#39;awkward&#39;, &#39;babe&#39;, &#39;baby&#39;, &#39;bacall&#39;, &#39;back&#39;, &#39;backdrop&#39;, &#39;background&#39;, &#39;backgrounds&#39;, &#39;bad&#39;, &#39;badly&#39;, &#39;bag&#39;, &#39;baker&#39;, &#39;bakshi&#39;, &#39;balance&#39;, &#39;baldwin&#39;, &#39;ball&#39;, &#39;ballet&#39;, &#39;balls&#39;, &#39;band&#39;, &#39;bands&#39;, &#39;bang&#39;, &#39;bank&#39;, &#39;banned&#39;, &#39;bar&#39;, &#39;barbara&#39;, &#39;bare&#39;, &#39;barely&#39;, &#39;bargain&#39;, &#39;barry&#39;, &#39;barrymore&#39;, &#39;base&#39;, &#39;baseball&#39;, &#39;based&#39;, &#39;basement&#39;, &#39;basic&#39;, &#39;basically&#39;, &#39;basis&#39;, &#39;basketball&#39;, &#39;bat&#39;, &#39;bath&#39;, &#39;bathroom&#39;, &#39;batman&#39;, &#39;battle&#39;, &#39;battles&#39;, &#39;bay&#39;, &#39;bbc&#39;, &#39;beach&#39;, &#39;bear&#39;, &#39;bears&#39;, &#39;beast&#39;, &#39;beat&#39;, &#39;beaten&#39;, &#39;beating&#39;, &#39;beats&#39;, &#39;beatty&#39;, &#39;beautiful&#39;, &#39;beautifully&#39;, &#39;beauty&#39;, &#39;became&#39;, &#39;become&#39;, &#39;becomes&#39;, &#39;becoming&#39;, &#39;bed&#39;, &#39;bedroom&#39;, &#39;beer&#39;, &#39;began&#39;, &#39;begin&#39;, &#39;beginning&#39;, &#39;begins&#39;, &#39;behave&#39;, &#39;behavior&#39;, &#39;behind&#39;, &#39;beings&#39;, &#39;bela&#39;, &#39;belief&#39;, &#39;beliefs&#39;, &#39;believable&#39;, &#39;believe&#39;, &#39;believed&#39;, &#39;believes&#39;, &#39;believing&#39;, &#39;bell&#39;, &#39;belong&#39;, &#39;belongs&#39;, &#39;beloved&#39;, &#39;belushi&#39;, &#39;ben&#39;, &#39;beneath&#39;, &#39;benefit&#39;, &#39;bergman&#39;, &#39;berlin&#39;, &#39;besides&#39;, &#39;best&#39;, &#39;bet&#39;, &#39;bette&#39;, &#39;better&#39;, &#39;bettie&#39;, &#39;betty&#39;, &#39;beyond&#39;, &#39;bible&#39;, &#39;big&#39;, &#39;bigger&#39;, &#39;biggest&#39;, &#39;biko&#39;, &#39;bill&#39;, &#39;billed&#39;, &#39;billy&#39;, &#39;bin&#39;, &#39;biography&#39;, &#39;bird&#39;, &#39;birds&#39;, &#39;birth&#39;, &#39;birthday&#39;, &#39;bit&#39;, &#39;bite&#39;, &#39;bits&#39;, &#39;bitter&#39;, &#39;bizarre&#39;, &#39;black&#39;, &#39;blade&#39;, &#39;blah&#39;, &#39;blair&#39;, &#39;blake&#39;, &#39;blame&#39;, &#39;bland&#39;, &#39;blank&#39;, &#39;blast&#39;, &#39;blatant&#39;, &#39;bleak&#39;, &#39;blend&#39;, &#39;blew&#39;, &#39;blind&#39;, &#39;blob&#39;, &#39;block&#39;, &#39;blockbuster&#39;, &#39;blond&#39;, &#39;blonde&#39;, &#39;blood&#39;, &#39;bloody&#39;, &#39;blow&#39;, &#39;blowing&#39;, &#39;blown&#39;, &#39;blows&#39;, &#39;blue&#39;, &#39;blues&#39;, &#39;blunt&#39;, &#39;bo&#39;, &#39;board&#39;, &#39;boat&#39;, &#39;bob&#39;, &#39;bobby&#39;, &#39;bodies&#39;, &#39;body&#39;, &#39;bold&#39;, &#39;boll&#39;, &#39;bollywood&#39;, &#39;bomb&#39;, &#39;bond&#39;, &#39;bone&#39;, &#39;bonus&#39;, &#39;book&#39;, &#39;books&#39;, &#39;boom&#39;, &#39;boot&#39;, &#39;border&#39;, &#39;bore&#39;, &#39;bored&#39;, &#39;boredom&#39;, &#39;boring&#39;, &#39;born&#39;, &#39;borrowed&#39;, &#39;boss&#39;, &#39;bother&#39;, &#39;bothered&#39;, &#39;bottle&#39;, &#39;bottom&#39;, &#39;bought&#39;, &#39;bound&#39;, &#39;bourne&#39;, &#39;box&#39;, &#39;boxing&#39;, &#39;boy&#39;, &#39;boyfriend&#39;, &#39;boyle&#39;, &#39;boys&#39;, &#39;brad&#39;, &#39;brady&#39;, &#39;brain&#39;, &#39;brains&#39;, &#39;branagh&#39;, &#39;brand&#39;, &#39;brando&#39;, &#39;brave&#39;, &#39;brazil&#39;, &#39;break&#39;, &#39;breaking&#39;, &#39;breaks&#39;, &#39;breasts&#39;, &#39;breath&#39;, &#39;breathtaking&#39;, &#39;brenda&#39;, &#39;brian&#39;, &#39;bride&#39;, &#39;bridge&#39;, &#39;brief&#39;, &#39;briefly&#39;, &#39;bright&#39;, &#39;brilliance&#39;, &#39;brilliant&#39;, &#39;brilliantly&#39;, &#39;bring&#39;, &#39;bringing&#39;, &#39;brings&#39;, &#39;britain&#39;, &#39;british&#39;, &#39;broad&#39;, &#39;broadcast&#39;, &#39;broadway&#39;, &#39;broke&#39;, &#39;broken&#39;, &#39;brooklyn&#39;, &#39;brooks&#39;, &#39;brosnan&#39;, &#39;brother&#39;, &#39;brothers&#39;, &#39;brought&#39;, &#39;brown&#39;, &#39;bruce&#39;, &#39;brutal&#39;, &#39;brutality&#39;, &#39;brutally&#39;, &#39;buck&#39;, &#39;bucks&#39;, &#39;bud&#39;, &#39;buddies&#39;, &#39;buddy&#39;, &#39;budget&#39;, &#39;buff&#39;, &#39;buffalo&#39;, &#39;buffs&#39;, &#39;bug&#39;, &#39;bugs&#39;, &#39;build&#39;, &#39;building&#39;, &#39;buildings&#39;, &#39;builds&#39;, &#39;built&#39;, &#39;bull&#39;, &#39;bullet&#39;, &#39;bullets&#39;, &#39;bumbling&#39;, &#39;bunch&#39;, &#39;bunny&#39;, &#39;buried&#39;, &#39;burn&#39;, &#39;burned&#39;, &#39;burning&#39;, &#39;burns&#39;, &#39;burt&#39;, &#39;burton&#39;, &#39;bus&#39;, &#39;bush&#39;, &#39;business&#39;, &#39;businessman&#39;, &#39;buster&#39;, &#39;busy&#39;, &#39;butler&#39;, &#39;butt&#39;, &#39;button&#39;, &#39;buy&#39;, &#39;buying&#39;, &#39;cabin&#39;, &#39;cable&#39;, &#39;cage&#39;, &#39;cagney&#39;, &#39;caine&#39;, &#39;cake&#39;, &#39;caliber&#39;, &#39;california&#39;, &#39;call&#39;, &#39;called&#39;, &#39;calling&#39;, &#39;calls&#39;, &#39;calm&#39;, &#39;came&#39;, &#39;cameo&#39;, &#39;cameos&#39;, &#39;camera&#39;, &#39;cameras&#39;, &#39;cameron&#39;, &#39;camp&#39;, &#39;campbell&#39;, &#39;campy&#39;, &#39;canada&#39;, &#39;canadian&#39;, &#39;candy&#39;, &#39;cannibal&#39;, &#39;cannot&#39;, &#39;cant&#39;, &#39;capable&#39;, &#39;capital&#39;, &#39;captain&#39;, &#39;captivating&#39;, &#39;capture&#39;, &#39;captured&#39;, &#39;captures&#39;, &#39;capturing&#39;, &#39;car&#39;, &#39;card&#39;, &#39;cardboard&#39;, &#39;cards&#39;, &#39;care&#39;, &#39;cared&#39;, &#39;career&#39;, &#39;careers&#39;, &#39;careful&#39;, &#39;carefully&#39;, &#39;carell&#39;, &#39;cares&#39;, &#39;caring&#39;, &#39;carl&#39;, &#39;carla&#39;, &#39;carol&#39;, &#39;carpenter&#39;, &#39;carradine&#39;, &#39;carrey&#39;, &#39;carrie&#39;, &#39;carried&#39;, &#39;carries&#39;, &#39;carry&#39;, &#39;carrying&#39;, &#39;cars&#39;, &#39;carter&#39;, &#39;cartoon&#39;, &#39;cartoons&#39;, &#39;cary&#39;, &#39;case&#39;, &#39;cases&#39;, &#39;cash&#39;, &#39;cassidy&#39;, &#39;cast&#39;, &#39;casting&#39;, &#39;castle&#39;, &#39;cat&#39;, &#39;catch&#39;, &#39;catches&#39;, &#39;catching&#39;, &#39;catchy&#39;, &#39;category&#39;, &#39;catherine&#39;, &#39;catholic&#39;, &#39;cats&#39;, &#39;caught&#39;, &#39;cause&#39;, &#39;caused&#39;, &#39;causes&#39;, &#39;causing&#39;, &#39;cave&#39;, &#39;cd&#39;, &#39;celebrity&#39;, &#39;cell&#39;, &#39;celluloid&#39;, &#39;center&#39;, &#39;centered&#39;, &#39;centers&#39;, &#39;central&#39;, &#39;century&#39;, &#39;certain&#39;, &#39;certainly&#39;, &#39;cg&#39;, &#39;cgi&#39;, &#39;chain&#39;, &#39;chair&#39;, &#39;challenge&#39;, &#39;challenging&#39;, &#39;championship&#39;, &#39;chan&#39;, &#39;chance&#39;, &#39;chances&#39;, &#39;change&#39;, &#39;changed&#39;, &#39;changes&#39;, &#39;changing&#39;, &#39;channel&#39;, &#39;channels&#39;, &#39;chaos&#39;, &#39;chaplin&#39;, &#39;chapter&#39;, &#39;character&#39;, &#39;characterization&#39;, &#39;characters&#39;, &#39;charge&#39;, &#39;charisma&#39;, &#39;charismatic&#39;, &#39;charles&#39;, &#39;charlie&#39;, &#39;charlotte&#39;, &#39;charm&#39;, &#39;charming&#39;, &#39;chase&#39;, &#39;chased&#39;, &#39;chases&#39;, &#39;chasing&#39;, &#39;che&#39;, &#39;cheap&#39;, &#39;cheated&#39;, &#39;cheating&#39;, &#39;check&#39;, &#39;checked&#39;, &#39;checking&#39;, &#39;cheek&#39;, &#39;cheese&#39;, &#39;cheesy&#39;, &#39;chemistry&#39;, &#39;chess&#39;, &#39;chest&#39;, &#39;chicago&#39;, &#39;chick&#39;, &#39;chicken&#39;, &#39;chicks&#39;, &#39;chief&#39;, &#39;child&#39;, &#39;childhood&#39;, &#39;childish&#39;, &#39;children&#39;, &#39;chilling&#39;, &#39;china&#39;, &#39;chinese&#39;, &#39;choice&#39;, &#39;choices&#39;, &#39;choose&#39;, &#39;chooses&#39;, &#39;choreographed&#39;, &#39;choreography&#39;, &#39;chorus&#39;, &#39;chose&#39;, &#39;chosen&#39;, &#39;chris&#39;, &#39;christ&#39;, &#39;christian&#39;, &#39;christianity&#39;, &#39;christians&#39;, &#39;christmas&#39;, &#39;christopher&#39;, &#39;christy&#39;, &#39;chuck&#39;, &#39;church&#39;, &#39;cia&#39;, &#39;cinderella&#39;, &#39;cinema&#39;, &#39;cinematic&#39;, &#39;cinematographer&#39;, &#39;cinematography&#39;, &#39;circle&#39;, &#39;circumstances&#39;, &#39;cities&#39;, &#39;citizen&#39;, &#39;city&#39;, &#39;civil&#39;, &#39;civilization&#39;, &#39;claim&#39;, &#39;claimed&#39;, &#39;claims&#39;, &#39;claire&#39;, &#39;clark&#39;, &#39;class&#39;, &#39;classes&#39;, &#39;classic&#39;, &#39;classical&#39;, &#39;classics&#39;, &#39;claus&#39;, &#39;clean&#39;, &#39;clear&#39;, &#39;clearly&#39;, &#39;clever&#39;, &#39;cleverly&#39;, &#39;clich&#39;, &#39;cliche&#39;, &#39;cliff&#39;, &#39;climactic&#39;, &#39;climax&#39;, &#39;clint&#39;, &#39;clip&#39;, &#39;clips&#39;, &#39;clock&#39;, &#39;close&#39;, &#39;closed&#39;, &#39;closely&#39;, &#39;closer&#39;, &#39;closest&#39;, &#39;closet&#39;, &#39;closing&#39;, &#39;clothes&#39;, &#39;clothing&#39;, &#39;clown&#39;, &#39;club&#39;, &#39;clue&#39;, &#39;clues&#39;, &#39;clumsy&#39;, &#39;co&#39;, &#39;coach&#39;, &#39;coast&#39;, &#39;code&#39;, &#39;coffee&#39;, &#39;coherent&#39;, &#39;cold&#39;, &#39;cole&#39;, &#39;collection&#39;, &#39;college&#39;, &#39;colonel&#39;, &#39;color&#39;, &#39;colorful&#39;, &#39;colors&#39;, &#39;colour&#39;, &#39;columbo&#39;, &#39;com&#39;, &#39;combat&#39;, &#39;combination&#39;, &#39;combine&#39;, &#39;combined&#39;, &#39;come&#39;, &#39;comedian&#39;, &#39;comedic&#39;, &#39;comedies&#39;, &#39;comedy&#39;, &#39;comes&#39;, &#39;comfort&#39;, &#39;comfortable&#39;, &#39;comic&#39;, &#39;comical&#39;, &#39;comics&#39;, &#39;coming&#39;, &#39;command&#39;, &#39;comment&#39;, &#39;commentary&#39;, &#39;commented&#39;, &#39;comments&#39;, &#39;commercial&#39;, &#39;commercials&#39;, &#39;commit&#39;, &#39;committed&#39;, &#39;common&#39;, &#39;communist&#39;, &#39;community&#39;, &#39;companies&#39;, &#39;companion&#39;, &#39;company&#39;, &#39;compare&#39;, &#39;compared&#39;, &#39;comparing&#39;, &#39;comparison&#39;, &#39;compassion&#39;, &#39;compelled&#39;, &#39;compelling&#39;, &#39;competent&#39;, &#39;competition&#39;, &#39;complain&#39;, &#39;complaint&#39;, &#39;complete&#39;, &#39;completely&#39;, &#39;complex&#39;, &#39;complexity&#39;, &#39;complicated&#39;, &#39;composed&#39;, &#39;composer&#39;, &#39;computer&#39;, &#39;con&#39;, &#39;conceived&#39;, &#39;concept&#39;, &#39;concern&#39;, &#39;concerned&#39;, &#39;concerning&#39;, &#39;concerns&#39;, &#39;concert&#39;, &#39;conclusion&#39;, &#39;condition&#39;, &#39;conditions&#39;, &#39;confess&#39;, &#39;confidence&#39;, &#39;conflict&#39;, &#39;conflicts&#39;, &#39;confrontation&#39;, &#39;confused&#39;, &#39;confusing&#39;, &#39;confusion&#39;, &#39;connect&#39;, &#39;connected&#39;, &#39;connection&#39;, &#39;connery&#39;, &#39;conscious&#39;, &#39;consequences&#39;, &#39;conservative&#39;, &#39;consider&#39;, &#39;considerable&#39;, &#39;considered&#39;, &#39;considering&#39;, &#39;consistent&#39;, &#39;consistently&#39;, &#39;consists&#39;, &#39;conspiracy&#39;, &#39;constant&#39;, &#39;constantly&#39;, &#39;constructed&#39;, &#39;construction&#39;, &#39;contact&#39;, &#39;contain&#39;, &#39;contained&#39;, &#39;contains&#39;, &#39;contemporary&#39;, &#39;content&#39;, &#39;contest&#39;, &#39;context&#39;, &#39;continue&#39;, &#39;continued&#39;, &#39;continues&#39;, &#39;continuity&#39;, &#39;contract&#39;, &#39;contrary&#39;, &#39;contrast&#39;, &#39;contrived&#39;, &#39;control&#39;, &#39;controversial&#39;, &#39;conventional&#39;, &#39;conversation&#39;, &#39;conversations&#39;, &#39;convey&#39;, &#39;convince&#39;, &#39;convinced&#39;, &#39;convincing&#39;, &#39;convincingly&#39;, &#39;convoluted&#39;, &#39;cook&#39;, &#39;cool&#39;, &#39;cooper&#39;, &#39;cop&#39;, &#39;copies&#39;, &#39;cops&#39;, &#39;copy&#39;, &#39;core&#39;, &#39;corner&#39;, &#39;corny&#39;, &#39;corporate&#39;, &#39;corpse&#39;, &#39;correct&#39;, &#39;correctly&#39;, &#39;corrupt&#39;, &#39;corruption&#39;, &#39;cost&#39;, &#39;costs&#39;, &#39;costume&#39;, &#39;costumes&#39;, &#39;could&#39;, &#39;count&#39;, &#39;counter&#39;, &#39;countless&#39;, &#39;countries&#39;, &#39;country&#39;, &#39;countryside&#39;, &#39;couple&#39;, &#39;couples&#39;, &#39;courage&#39;, &#39;course&#39;, &#39;court&#39;, &#39;cousin&#39;, &#39;cover&#39;, &#39;covered&#39;, &#39;covers&#39;, &#39;cowboy&#39;, &#39;cox&#39;, &#39;crack&#39;, &#39;cracking&#39;, &#39;craft&#39;, &#39;crafted&#39;, &#39;craig&#39;, &#39;crap&#39;, &#39;crappy&#39;, &#39;crash&#39;, &#39;craven&#39;, &#39;crawford&#39;, &#39;crazed&#39;, &#39;crazy&#39;, &#39;cream&#39;, &#39;create&#39;, &#39;created&#39;, &#39;creates&#39;, &#39;creating&#39;, &#39;creation&#39;, &#39;creative&#39;, &#39;creativity&#39;, &#39;creator&#39;, &#39;creators&#39;, &#39;creature&#39;, &#39;creatures&#39;, &#39;credibility&#39;, &#39;credible&#39;, &#39;credit&#39;, &#39;credits&#39;, &#39;creep&#39;, &#39;creepy&#39;, &#39;crew&#39;, &#39;cried&#39;, &#39;crime&#39;, &#39;crimes&#39;, &#39;criminal&#39;, &#39;criminals&#39;, &#39;cringe&#39;, &#39;crisis&#39;, &#39;critic&#39;, &#39;critical&#39;, &#39;criticism&#39;, &#39;critics&#39;, &#39;crocodile&#39;, &#39;cross&#39;, &#39;crowd&#39;, &#39;crucial&#39;, &#39;crude&#39;, &#39;cruel&#39;, &#39;cruise&#39;, &#39;crush&#39;, &#39;cry&#39;, &#39;crying&#39;, &#39;crystal&#39;, &#39;cuba&#39;, &#39;cube&#39;, &#39;cult&#39;, &#39;cultural&#39;, &#39;culture&#39;, &#39;cup&#39;, &#39;cure&#39;, &#39;curiosity&#39;, &#39;curious&#39;, &#39;current&#39;, &#39;currently&#39;, &#39;curse&#39;, &#39;curtis&#39;, &#39;cusack&#39;, &#39;cut&#39;, &#39;cute&#39;, &#39;cuts&#39;, &#39;cutting&#39;, &#39;cynical&#39;, &#39;da&#39;, &#39;dad&#39;, &#39;daddy&#39;, &#39;daily&#39;, &#39;dalton&#39;, &#39;damage&#39;, &#39;damme&#39;, &#39;damn&#39;, &#39;damon&#39;, &#39;dan&#39;, &#39;dana&#39;, &#39;dance&#39;, &#39;dancer&#39;, &#39;dancers&#39;, &#39;dances&#39;, &#39;dancing&#39;, &#39;danes&#39;, &#39;danger&#39;, &#39;dangerous&#39;, &#39;daniel&#39;, &#39;danny&#39;, &#39;dare&#39;, &#39;daring&#39;, &#39;dark&#39;, &#39;darker&#39;, &#39;darkness&#39;, &#39;darren&#39;, &#39;date&#39;, &#39;dated&#39;, &#39;dating&#39;, &#39;daughter&#39;, &#39;daughters&#39;, &#39;dave&#39;, &#39;david&#39;, &#39;davies&#39;, &#39;davis&#39;, &#39;dawn&#39;, &#39;dawson&#39;, &#39;day&#39;, &#39;days&#39;, &#39;de&#39;, &#39;dead&#39;, &#39;deadly&#39;, &#39;deaf&#39;, &#39;deal&#39;, &#39;dealing&#39;, &#39;deals&#39;, &#39;dealt&#39;, &#39;dean&#39;, &#39;dear&#39;, &#39;death&#39;, &#39;deaths&#39;, &#39;debut&#39;, &#39;decade&#39;, &#39;decades&#39;, &#39;deceased&#39;, &#39;decent&#39;, &#39;decide&#39;, &#39;decided&#39;, &#39;decides&#39;, &#39;decision&#39;, &#39;decisions&#39;, &#39;dedicated&#39;, &#39;dee&#39;, &#39;deep&#39;, &#39;deeper&#39;, &#39;deeply&#39;, &#39;defeat&#39;, &#39;defend&#39;, &#39;defense&#39;, &#39;defined&#39;, &#39;definite&#39;, &#39;definitely&#39;, &#39;definition&#39;, &#39;degree&#39;, &#39;del&#39;, &#39;deliberately&#39;, &#39;delight&#39;, &#39;delightful&#39;, &#39;deliver&#39;, &#39;delivered&#39;, &#39;delivering&#39;, &#39;delivers&#39;, &#39;delivery&#39;, &#39;demand&#39;, &#39;demands&#39;, &#39;demented&#39;, &#39;demise&#39;, &#39;demon&#39;, &#39;demons&#39;, &#39;deniro&#39;, &#39;dennis&#39;, &#39;dentist&#39;, &#39;denzel&#39;, &#39;department&#39;, &#39;depicted&#39;, &#39;depicting&#39;, &#39;depiction&#39;, &#39;depicts&#39;, &#39;depressed&#39;, &#39;depressing&#39;, &#39;depression&#39;, &#39;depth&#39;, &#39;der&#39;, &#39;derek&#39;, &#39;descent&#39;, &#39;describe&#39;, &#39;described&#39;, &#39;describes&#39;, &#39;description&#39;, &#39;desert&#39;, &#39;deserve&#39;, &#39;deserved&#39;, &#39;deserves&#39;, &#39;design&#39;, &#39;designed&#39;, &#39;designs&#39;, &#39;desire&#39;, &#39;desired&#39;, &#39;despair&#39;, &#39;desperate&#39;, &#39;desperately&#39;, &#39;desperation&#39;, &#39;despite&#39;, &#39;destiny&#39;, &#39;destroy&#39;, &#39;destroyed&#39;, &#39;destroying&#39;, &#39;destruction&#39;, &#39;detail&#39;, &#39;detailed&#39;, &#39;details&#39;, &#39;detective&#39;, &#39;determined&#39;, &#39;develop&#39;, &#39;developed&#39;, &#39;developing&#39;, &#39;development&#39;, &#39;develops&#39;, &#39;device&#39;, &#39;devil&#39;, &#39;devoid&#39;, &#39;devoted&#39;, &#39;dialog&#39;, &#39;dialogs&#39;, &#39;dialogue&#39;, &#39;dialogues&#39;, &#39;diamond&#39;, &#39;diana&#39;, &#39;diane&#39;, &#39;dick&#39;, &#39;dickens&#39;, &#39;die&#39;, &#39;died&#39;, &#39;dies&#39;, &#39;difference&#39;, &#39;differences&#39;, &#39;different&#39;, &#39;difficult&#39;, &#39;dig&#39;, &#39;digital&#39;, &#39;dignity&#39;, &#39;dimension&#39;, &#39;dimensional&#39;, &#39;din&#39;, &#39;dinner&#39;, &#39;dinosaur&#39;, &#39;dinosaurs&#39;, &#39;dire&#39;, &#39;direct&#39;, &#39;directed&#39;, &#39;directing&#39;, &#39;direction&#39;, &#39;directions&#39;, &#39;directly&#39;, &#39;director&#39;, &#39;directorial&#39;, &#39;directors&#39;, &#39;directs&#39;, &#39;dirty&#39;, &#39;disagree&#39;, &#39;disappear&#39;, &#39;disappeared&#39;, &#39;disappoint&#39;, &#39;disappointed&#39;, &#39;disappointing&#39;, &#39;disappointment&#39;, &#39;disaster&#39;, &#39;disbelief&#39;, &#39;disc&#39;, &#39;discover&#39;, &#39;discovered&#39;, &#39;discovers&#39;, &#39;discovery&#39;, &#39;discuss&#39;, &#39;discussion&#39;, &#39;disease&#39;, &#39;disgusting&#39;, &#39;disjointed&#39;, &#39;dislike&#39;, &#39;disliked&#39;, &#39;disney&#39;, &#39;display&#39;, &#39;displayed&#39;, &#39;displays&#39;, &#39;distance&#39;, &#39;distant&#39;, &#39;distinct&#39;, &#39;distracting&#39;, &#39;distribution&#39;, &#39;disturbed&#39;, &#39;disturbing&#39;, &#39;divorce&#39;, &#39;dixon&#39;, &#39;doc&#39;, &#39;doctor&#39;, &#39;documentaries&#39;, &#39;documentary&#39;, &#39;dog&#39;, &#39;dogs&#39;, &#39;doll&#39;, &#39;dollar&#39;, &#39;dollars&#39;, &#39;dolls&#39;, &#39;dolph&#39;, &#39;domestic&#39;, &#39;domino&#39;, &#39;donald&#39;, &#39;done&#39;, &#39;donna&#39;, &#39;doo&#39;, &#39;doom&#39;, &#39;doomed&#39;, &#39;door&#39;, &#39;doors&#39;, &#39;dorothy&#39;, &#39;double&#39;, &#39;doubt&#39;, &#39;doubts&#39;, &#39;douglas&#39;, &#39;downey&#39;, &#39;downhill&#39;, &#39;downright&#39;, &#39;dozen&#39;, &#39;dozens&#39;, &#39;dr&#39;, &#39;dracula&#39;, &#39;drag&#39;, &#39;dragged&#39;, &#39;dragon&#39;, &#39;drags&#39;, &#39;drake&#39;, &#39;drama&#39;, &#39;dramas&#39;, &#39;dramatic&#39;, &#39;draw&#39;, &#39;drawing&#39;, &#39;drawn&#39;, &#39;draws&#39;, &#39;dreadful&#39;, &#39;dream&#39;, &#39;dreams&#39;, &#39;dreary&#39;, &#39;dreck&#39;, &#39;dress&#39;, &#39;dressed&#39;, &#39;dressing&#39;, &#39;drew&#39;, &#39;drink&#39;, &#39;drinking&#39;, &#39;drive&#39;, &#39;drivel&#39;, &#39;driven&#39;, &#39;driver&#39;, &#39;drives&#39;, &#39;driving&#39;, &#39;drop&#39;, &#39;dropped&#39;, &#39;dropping&#39;, &#39;drops&#39;, &#39;drug&#39;, &#39;drugs&#39;, &#39;drunk&#39;, &#39;drunken&#39;, &#39;dry&#39;, &#39;dub&#39;, &#39;dubbed&#39;, &#39;dubbing&#39;, &#39;dud&#39;, &#39;dude&#39;, &#39;due&#39;, &#39;duke&#39;, &#39;dull&#39;, &#39;dumb&#39;, &#39;duo&#39;, &#39;dust&#39;, &#39;dutch&#39;, &#39;duty&#39;, &#39;dvd&#39;, &#39;dying&#39;, &#39;dynamic&#39;, &#39;eager&#39;, &#39;ear&#39;, &#39;earl&#39;, &#39;earlier&#39;, &#39;early&#39;, &#39;earned&#39;, &#39;ears&#39;, &#39;earth&#39;, &#39;ease&#39;, &#39;easier&#39;, &#39;easily&#39;, &#39;east&#39;, &#39;eastern&#39;, &#39;eastwood&#39;, &#39;easy&#39;, &#39;eat&#39;, &#39;eaten&#39;, &#39;eating&#39;, &#39;eccentric&#39;, &#39;ed&#39;, &#39;eddie&#39;, &#39;edgar&#39;, &#39;edge&#39;, &#39;edgy&#39;, &#39;edie&#39;, &#39;edited&#39;, &#39;editing&#39;, &#39;edition&#39;, &#39;editor&#39;, &#39;education&#39;, &#39;educational&#39;, &#39;edward&#39;, &#39;eerie&#39;, &#39;effect&#39;, &#39;effective&#39;, &#39;effectively&#39;, &#39;effects&#39;, &#39;effort&#39;, &#39;efforts&#39;, &#39;ego&#39;, &#39;eight&#39;, &#39;eighties&#39;, &#39;either&#39;, &#39;elaborate&#39;, &#39;elderly&#39;, &#39;elegant&#39;, &#39;element&#39;, &#39;elements&#39;, &#39;elephant&#39;, &#39;elizabeth&#39;, &#39;ellen&#39;, &#39;elm&#39;, &#39;else&#39;, &#39;elsewhere&#39;, &#39;elvira&#39;, &#39;elvis&#39;, &#39;em&#39;, &#39;embarrassed&#39;, &#39;embarrassing&#39;, &#39;embarrassment&#39;, &#39;emily&#39;, &#39;emma&#39;, &#39;emotion&#39;, &#39;emotional&#39;, &#39;emotionally&#39;, &#39;emotions&#39;, &#39;empathy&#39;, &#39;emperor&#39;, &#39;emphasis&#39;, &#39;empire&#39;, &#39;empty&#39;, &#39;en&#39;, &#39;encounter&#39;, &#39;encounters&#39;, &#39;end&#39;, &#39;endearing&#39;, &#39;ended&#39;, &#39;ending&#39;, &#39;endings&#39;, &#39;endless&#39;, &#39;ends&#39;, &#39;endure&#39;, &#39;enemies&#39;, &#39;enemy&#39;, &#39;energy&#39;, &#39;engage&#39;, &#39;engaged&#39;, &#39;engaging&#39;, &#39;england&#39;, &#39;english&#39;, &#39;enjoy&#39;, &#39;enjoyable&#39;, &#39;enjoyed&#39;, &#39;enjoying&#39;, &#39;enjoyment&#39;, &#39;enjoys&#39;, &#39;enormous&#39;, &#39;enough&#39;, &#39;ensemble&#39;, &#39;ensues&#39;, &#39;enter&#39;, &#39;enterprise&#39;, &#39;enters&#39;, &#39;entertain&#39;, &#39;entertained&#39;, &#39;entertaining&#39;, &#39;entertainment&#39;, &#39;enthusiasm&#39;, &#39;entire&#39;, &#39;entirely&#39;, &#39;entry&#39;, &#39;environment&#39;, &#39;epic&#39;, &#39;episode&#39;, &#39;episodes&#39;, &#39;equal&#39;, &#39;equally&#39;, &#39;equipment&#39;, &#39;equivalent&#39;, &#39;er&#39;, &#39;era&#39;, &#39;eric&#39;, &#39;erotic&#39;, &#39;errors&#39;, &#39;escape&#39;, &#39;escaped&#39;, &#39;escapes&#39;, &#39;especially&#39;, &#39;essence&#39;, &#39;essential&#39;, &#39;essentially&#39;, &#39;established&#39;, &#39;estate&#39;, &#39;et&#39;, &#39;etc&#39;, &#39;ethan&#39;, &#39;eugene&#39;, &#39;europe&#39;, &#39;european&#39;, &#39;eva&#39;, &#39;eve&#39;, &#39;even&#39;, &#39;evening&#39;, &#39;event&#39;, &#39;events&#39;, &#39;eventually&#39;, &#39;ever&#39;, &#39;every&#39;, &#39;everybody&#39;, &#39;everyday&#39;, &#39;everyone&#39;, &#39;everything&#39;, &#39;everywhere&#39;, &#39;evidence&#39;, &#39;evident&#39;, &#39;evil&#39;, &#39;ex&#39;, &#39;exact&#39;, &#39;exactly&#39;, &#39;exaggerated&#39;, &#39;examination&#39;, &#39;example&#39;, &#39;examples&#39;, &#39;excellent&#39;, &#39;except&#39;, &#39;exception&#39;, &#39;exceptional&#39;, &#39;exceptionally&#39;, &#39;excessive&#39;, &#39;exchange&#39;, &#39;excited&#39;, &#39;excitement&#39;, &#39;exciting&#39;, &#39;excuse&#39;, &#39;executed&#39;, &#39;execution&#39;, &#39;executive&#39;, &#39;exercise&#39;, &#39;exist&#39;, &#39;existed&#39;, &#39;existence&#39;, &#39;existent&#39;, &#39;exists&#39;, &#39;exotic&#39;, &#39;expect&#39;, &#39;expectations&#39;, &#39;expected&#39;, &#39;expecting&#39;, &#39;expedition&#39;, &#39;expensive&#39;, &#39;experience&#39;, &#39;experienced&#39;, &#39;experiences&#39;, &#39;experiment&#39;, &#39;experimental&#39;, &#39;experiments&#39;, &#39;expert&#39;, &#39;explain&#39;, &#39;explained&#39;, &#39;explaining&#39;, &#39;explains&#39;, &#39;explanation&#39;, &#39;explicit&#39;, &#39;exploitation&#39;, &#39;exploration&#39;, &#39;explore&#39;, &#39;explored&#39;, &#39;explosion&#39;, &#39;explosions&#39;, &#39;exposed&#39;, &#39;exposure&#39;, &#39;express&#39;, &#39;expressed&#39;, &#39;expression&#39;, &#39;expressions&#39;, &#39;extended&#39;, &#39;extent&#39;, &#39;extra&#39;, &#39;extraordinary&#39;, &#39;extras&#39;, &#39;extreme&#39;, &#39;extremely&#39;, &#39;eye&#39;, &#39;eyed&#39;, &#39;eyes&#39;, &#39;eyre&#39;, &#39;fabulous&#39;, &#39;face&#39;, &#39;faced&#39;, &#39;faces&#39;, &#39;facial&#39;, &#39;facing&#39;, &#39;fact&#39;, &#39;factor&#39;, &#39;factory&#39;, &#39;facts&#39;, &#39;fail&#39;, &#39;failed&#39;, &#39;failing&#39;, &#39;fails&#39;, &#39;failure&#39;, &#39;fair&#39;, &#39;fairly&#39;, &#39;fairy&#39;, &#39;faith&#39;, &#39;faithful&#39;, &#39;fake&#39;, &#39;falk&#39;, &#39;fall&#39;, &#39;fallen&#39;, &#39;falling&#39;, &#39;falls&#39;, &#39;false&#39;, &#39;fame&#39;, &#39;familiar&#39;, &#39;families&#39;, &#39;family&#39;, &#39;famous&#39;, &#39;fan&#39;, &#39;fancy&#39;, &#39;fans&#39;, &#39;fantastic&#39;, &#39;fantasy&#39;, &#39;far&#39;, &#39;farce&#39;, &#39;fare&#39;, &#39;farm&#39;, &#39;farrell&#39;, &#39;fascinated&#39;, &#39;fascinating&#39;, &#39;fashion&#39;, &#39;fashioned&#39;, &#39;fast&#39;, &#39;faster&#39;, &#39;fat&#39;, &#39;fatal&#39;, &#39;fate&#39;, &#39;father&#39;, &#39;fault&#39;, &#39;faults&#39;, &#39;favor&#39;, &#39;favorite&#39;, &#39;favorites&#39;, &#39;favourite&#39;, &#39;fay&#39;, &#39;fbi&#39;, &#39;fear&#39;, &#39;fears&#39;, &#39;feature&#39;, &#39;featured&#39;, &#39;features&#39;, &#39;featuring&#39;, &#39;fed&#39;, &#39;feed&#39;, &#39;feel&#39;, &#39;feeling&#39;, &#39;feelings&#39;, &#39;feels&#39;, &#39;feet&#39;, &#39;felix&#39;, &#39;fell&#39;, &#39;fellow&#39;, &#39;felt&#39;, &#39;female&#39;, &#39;feminist&#39;, &#39;femme&#39;, &#39;fest&#39;, &#39;festival&#39;, &#39;fetched&#39;, &#39;fever&#39;, &#39;fi&#39;, &#39;fianc&#39;, &#39;fiction&#39;, &#39;fictional&#39;, &#39;fido&#39;, &#39;field&#39;, &#39;fields&#39;, &#39;fifteen&#39;, &#39;fight&#39;, &#39;fighter&#39;, &#39;fighting&#39;, &#39;fights&#39;, &#39;figure&#39;, &#39;figured&#39;, &#39;figures&#39;, &#39;files&#39;, &#39;fill&#39;, &#39;filled&#39;, &#39;film&#39;, &#39;filmed&#39;, &#39;filming&#39;, &#39;filmmaker&#39;, &#39;filmmakers&#39;, &#39;films&#39;, &#39;final&#39;, &#39;finale&#39;, &#39;finally&#39;, &#39;financial&#39;, &#39;find&#39;, &#39;finding&#39;, &#39;finds&#39;, &#39;fine&#39;, &#39;finest&#39;, &#39;finger&#39;, &#39;finish&#39;, &#39;finished&#39;, &#39;fire&#39;, &#39;fired&#39;, &#39;firm&#39;, &#39;first&#39;, &#39;firstly&#39;, &#39;fish&#39;, &#39;fisher&#39;, &#39;fishing&#39;, &#39;fit&#39;, &#39;fits&#39;, &#39;fitting&#39;, &#39;five&#39;, &#39;fix&#39;, &#39;flair&#39;, &#39;flash&#39;, &#39;flashback&#39;, &#39;flashbacks&#39;, &#39;flat&#39;, &#39;flaw&#39;, &#39;flawed&#39;, &#39;flawless&#39;, &#39;flaws&#39;, &#39;flesh&#39;, &#39;flick&#39;, &#39;flicks&#39;, &#39;flies&#39;, &#39;flight&#39;, &#39;floating&#39;, &#39;floor&#39;, &#39;flop&#39;, &#39;florida&#39;, &#39;flow&#39;, &#39;fly&#39;, &#39;flying&#39;, &#39;flynn&#39;, &#39;focus&#39;, &#39;focused&#39;, &#39;focuses&#39;, &#39;focusing&#39;, &#39;folk&#39;, &#39;folks&#39;, &#39;follow&#39;, &#39;followed&#39;, &#39;following&#39;, &#39;follows&#39;, &#39;fond&#39;, &#39;fonda&#39;, &#39;food&#39;, &#39;fool&#39;, &#39;fooled&#39;, &#39;foot&#39;, &#39;footage&#39;, &#39;football&#39;, &#39;forbidden&#39;, &#39;force&#39;, &#39;forced&#39;, &#39;forces&#39;, &#39;ford&#39;, &#39;foreign&#39;, &#39;forest&#39;, &#39;forever&#39;, &#39;forget&#39;, &#39;forgettable&#39;, &#39;forgive&#39;, &#39;forgot&#39;, &#39;forgotten&#39;, &#39;form&#39;, &#39;format&#39;, &#39;former&#39;, &#39;forms&#39;, &#39;formula&#39;, &#39;formulaic&#39;, &#39;forth&#39;, &#39;fortunately&#39;, &#39;fortune&#39;, &#39;forty&#39;, &#39;forward&#39;, &#39;foster&#39;, &#39;fought&#39;, &#39;foul&#39;, &#39;found&#39;, &#39;four&#39;, &#39;fourth&#39;, &#39;fox&#39;, &#39;frame&#39;, &#39;france&#39;, &#39;franchise&#39;, &#39;francis&#39;, &#39;francisco&#39;, &#39;franco&#39;, &#39;frank&#39;, &#39;frankenstein&#39;, &#39;frankie&#39;, &#39;frankly&#39;, &#39;freak&#39;, &#39;fred&#39;, &#39;freddy&#39;, &#39;free&#39;, &#39;freedom&#39;, &#39;freeman&#39;, &#39;french&#39;, &#39;frequent&#39;, &#39;frequently&#39;, &#39;fresh&#39;, &#39;friday&#39;, &#39;friend&#39;, &#39;friendly&#39;, &#39;friends&#39;, &#39;friendship&#39;, &#39;frightening&#39;, &#39;front&#39;, &#39;frustrated&#39;, &#39;frustrating&#39;, &#39;frustration&#39;, &#39;fu&#39;, &#39;fulci&#39;, &#39;full&#39;, &#39;fuller&#39;, &#39;fully&#39;, &#39;fun&#39;, &#39;funeral&#39;, &#39;funnier&#39;, &#39;funniest&#39;, &#39;funny&#39;, &#39;furious&#39;, &#39;furthermore&#39;, &#39;fury&#39;, &#39;future&#39;, &#39;futuristic&#39;, &#39;fx&#39;, &#39;gabriel&#39;, &#39;gadget&#39;, &#39;gag&#39;, &#39;gags&#39;, &#39;gain&#39;, &#39;game&#39;, &#39;games&#39;, &#39;gandhi&#39;, &#39;gang&#39;, &#39;gangster&#39;, &#39;gangsters&#39;, &#39;garbage&#39;, &#39;garbo&#39;, &#39;garden&#39;, &#39;gary&#39;, &#39;gas&#39;, &#39;gave&#39;, &#39;gay&#39;, &#39;gem&#39;, &#39;gender&#39;, &#39;gene&#39;, &#39;general&#39;, &#39;generally&#39;, &#39;generated&#39;, &#39;generation&#39;, &#39;generations&#39;, &#39;generic&#39;, &#39;generous&#39;, &#39;genius&#39;, &#39;genre&#39;, &#39;genres&#39;, &#39;gentle&#39;, &#39;gentleman&#39;, &#39;genuine&#39;, &#39;genuinely&#39;, &#39;george&#39;, &#39;gerard&#39;, &#39;german&#39;, &#39;germans&#39;, &#39;germany&#39;, &#39;get&#39;, &#39;gets&#39;, &#39;getting&#39;, &#39;ghost&#39;, &#39;ghosts&#39;, &#39;giallo&#39;, &#39;giant&#39;, &#39;gift&#39;, &#39;gifted&#39;, &#39;ginger&#39;, &#39;girl&#39;, &#39;girlfriend&#39;, &#39;girls&#39;, &#39;give&#39;, &#39;given&#39;, &#39;gives&#39;, &#39;giving&#39;, &#39;glad&#39;, &#39;glass&#39;, &#39;glasses&#39;, &#39;glenn&#39;, &#39;glimpse&#39;, &#39;global&#39;, &#39;glorious&#39;, &#39;glory&#39;, &#39;glover&#39;, &#39;go&#39;, &#39;goal&#39;, &#39;god&#39;, &#39;godfather&#39;, &#39;godzilla&#39;, &#39;goes&#39;, &#39;going&#39;, &#39;gold&#39;, &#39;goldberg&#39;, &#39;golden&#39;, &#39;gone&#39;, &#39;gonna&#39;, &#39;good&#39;, &#39;goodness&#39;, &#39;goofy&#39;, &#39;gordon&#39;, &#39;gore&#39;, &#39;gorgeous&#39;, &#39;gory&#39;, &#39;got&#39;, &#39;gothic&#39;, &#39;gotta&#39;, &#39;gotten&#39;, &#39;government&#39;, &#39;grab&#39;, &#39;grabs&#39;, &#39;grace&#39;, &#39;grade&#39;, &#39;gradually&#39;, &#39;graham&#39;, &#39;grand&#39;, &#39;grandfather&#39;, &#39;grandmother&#39;, &#39;grant&#39;, &#39;granted&#39;, &#39;graphic&#39;, &#39;graphics&#39;, &#39;grasp&#39;, &#39;gratuitous&#39;, &#39;grave&#39;, &#39;gray&#39;, &#39;grayson&#39;, &#39;great&#39;, &#39;greater&#39;, &#39;greatest&#39;, &#39;greatly&#39;, &#39;greatness&#39;, &#39;greed&#39;, &#39;greedy&#39;, &#39;greek&#39;, &#39;green&#39;, &#39;greg&#39;, &#39;gregory&#39;, &#39;grew&#39;, &#39;grey&#39;, &#39;grief&#39;, &#39;griffith&#39;, &#39;grim&#39;, &#39;grinch&#39;, &#39;gripping&#39;, &#39;gritty&#39;, &#39;gross&#39;, &#39;ground&#39;, &#39;group&#39;, &#39;groups&#39;, &#39;grow&#39;, &#39;growing&#39;, &#39;grown&#39;, &#39;grows&#39;, &#39;gruesome&#39;, &#39;guarantee&#39;, &#39;guard&#39;, &#39;guess&#39;, &#39;guessed&#39;, &#39;guessing&#39;, &#39;guest&#39;, &#39;guide&#39;, &#39;guilt&#39;, &#39;guilty&#39;, &#39;gun&#39;, &#39;gundam&#39;, &#39;guns&#39;, &#39;guts&#39;, &#39;guy&#39;, &#39;guys&#39;, &#39;ha&#39;, &#39;hair&#39;, &#39;hal&#39;, &#39;half&#39;, &#39;halfway&#39;, &#39;hall&#39;, &#39;halloween&#39;, &#39;ham&#39;, &#39;hamilton&#39;, &#39;hamlet&#39;, &#39;hammer&#39;, &#39;hand&#39;, &#39;handed&#39;, &#39;handful&#39;, &#39;handle&#39;, &#39;handled&#39;, &#39;hands&#39;, &#39;handsome&#39;, &#39;hang&#39;, &#39;hanging&#39;, &#39;hank&#39;, &#39;hanks&#39;, &#39;happen&#39;, &#39;happened&#39;, &#39;happening&#39;, &#39;happens&#39;, &#39;happily&#39;, &#39;happiness&#39;, &#39;happy&#39;, &#39;hard&#39;, &#39;hardcore&#39;, &#39;harder&#39;, &#39;hardly&#39;, &#39;hardy&#39;, &#39;harm&#39;, &#39;harris&#39;, &#39;harry&#39;, &#39;harsh&#39;, &#39;hart&#39;, &#39;hartley&#39;, &#39;harvey&#39;, &#39;hat&#39;, &#39;hate&#39;, &#39;hated&#39;, &#39;hates&#39;, &#39;hatred&#39;, &#39;haunted&#39;, &#39;haunting&#39;, &#39;hawke&#39;, &#39;hbo&#39;, &#39;head&#39;, &#39;headed&#39;, &#39;heads&#39;, &#39;health&#39;, &#39;hear&#39;, &#39;heard&#39;, &#39;hearing&#39;, &#39;heart&#39;, &#39;hearted&#39;, &#39;hearts&#39;, &#39;heat&#39;, &#39;heaven&#39;, &#39;heavily&#39;, &#39;heavy&#39;, &#39;heck&#39;, &#39;heights&#39;, &#39;held&#39;, &#39;helen&#39;, &#39;helicopter&#39;, &#39;hell&#39;, &#39;hello&#39;, &#39;help&#39;, &#39;helped&#39;, &#39;helping&#39;, &#39;helps&#39;, &#39;hence&#39;, &#39;henry&#39;, &#39;hero&#39;, &#39;heroes&#39;, &#39;heroic&#39;, &#39;heroine&#39;, &#39;heston&#39;, &#39;hey&#39;, &#39;hidden&#39;, &#39;hide&#39;, &#39;hideous&#39;, &#39;hiding&#39;, &#39;high&#39;, &#39;higher&#39;, &#39;highest&#39;, &#39;highlight&#39;, &#39;highlights&#39;, &#39;highly&#39;, &#39;hilarious&#39;, &#39;hilariously&#39;, &#39;hill&#39;, &#39;hills&#39;, &#39;hint&#39;, &#39;hints&#39;, &#39;hip&#39;, &#39;hippie&#39;, &#39;hire&#39;, &#39;hired&#39;, &#39;historical&#39;, &#39;historically&#39;, &#39;history&#39;, &#39;hit&#39;, &#39;hitchcock&#39;, &#39;hitler&#39;, &#39;hits&#39;, &#39;hitting&#39;, &#39;ho&#39;, &#39;hoffman&#39;, &#39;hold&#39;, &#39;holding&#39;, &#39;holds&#39;, &#39;hole&#39;, &#39;holes&#39;, &#39;holiday&#39;, &#39;hollow&#39;, &#39;holly&#39;, &#39;hollywood&#39;, &#39;holmes&#39;, &#39;holy&#39;, &#39;homage&#39;, &#39;home&#39;, &#39;homeless&#39;, &#39;homer&#39;, &#39;homosexual&#39;, &#39;honest&#39;, &#39;honestly&#39;, &#39;honesty&#39;, &#39;hong&#39;, &#39;honor&#39;, &#39;hood&#39;, &#39;hook&#39;, &#39;hooked&#39;, &#39;hop&#39;, &#39;hope&#39;, &#39;hoped&#39;, &#39;hopefully&#39;, &#39;hopeless&#39;, &#39;hopes&#39;, &#39;hoping&#39;, &#39;hopper&#39;, &#39;horrendous&#39;, &#39;horrible&#39;, &#39;horribly&#39;, &#39;horrid&#39;, &#39;horrific&#39;, &#39;horrifying&#39;, &#39;horror&#39;, &#39;horrors&#39;, &#39;horse&#39;, &#39;horses&#39;, &#39;hospital&#39;, &#39;host&#39;, &#39;hot&#39;, &#39;hotel&#39;, &#39;hour&#39;, &#39;hours&#39;, &#39;house&#39;, &#39;household&#39;, &#39;houses&#39;, &#39;howard&#39;, &#39;however&#39;, &#39;hudson&#39;, &#39;huge&#39;, &#39;hugh&#39;, &#39;huh&#39;, &#39;human&#39;, &#39;humanity&#39;, &#39;humans&#39;, &#39;humble&#39;, &#39;humor&#39;, &#39;humorous&#39;, &#39;humour&#39;, &#39;hundred&#39;, &#39;hundreds&#39;, &#39;hung&#39;, &#39;hunt&#39;, &#39;hunter&#39;, &#39;hunters&#39;, &#39;hunting&#39;, &#39;hurt&#39;, &#39;hurts&#39;, &#39;husband&#39;, &#39;husbands&#39;, &#39;hyde&#39;, &#39;hype&#39;, &#39;hysterical&#39;, &#39;ian&#39;, &#39;ice&#39;, &#39;icon&#39;, &#39;idea&#39;, &#39;ideal&#39;, &#39;ideas&#39;, &#39;identify&#39;, &#39;identity&#39;, &#39;idiot&#39;, &#39;idiotic&#39;, &#39;idiots&#39;, &#39;ignorant&#39;, &#39;ignore&#39;, &#39;ignored&#39;, &#39;ii&#39;, &#39;iii&#39;, &#39;ill&#39;, &#39;illegal&#39;, &#39;illness&#39;, &#39;illogical&#39;, &#39;im&#39;, &#39;image&#39;, &#39;imagery&#39;, &#39;images&#39;, &#39;imagination&#39;, &#39;imaginative&#39;, &#39;imagine&#39;, &#39;imagined&#39;, &#39;imdb&#39;, &#39;imitation&#39;, &#39;immediate&#39;, &#39;immediately&#39;, &#39;immensely&#39;, &#39;impact&#39;, &#39;implausible&#39;, &#39;importance&#39;, &#39;important&#39;, &#39;importantly&#39;, &#39;impossible&#39;, &#39;impress&#39;, &#39;impressed&#39;, &#39;impression&#39;, &#39;impressive&#39;, &#39;improve&#39;, &#39;improved&#39;, &#39;improvement&#39;, &#39;inability&#39;, &#39;inane&#39;, &#39;inappropriate&#39;, &#39;incident&#39;, &#39;include&#39;, &#39;included&#39;, &#39;includes&#39;, &#39;including&#39;, &#39;incoherent&#39;, &#39;incompetent&#39;, &#39;incomprehensible&#39;, &#39;increasingly&#39;, &#39;incredible&#39;, &#39;incredibly&#39;, &#39;indeed&#39;, &#39;independent&#39;, &#39;india&#39;, &#39;indian&#39;, &#39;indians&#39;, &#39;indie&#39;, &#39;individual&#39;, &#39;individuals&#39;, &#39;inducing&#39;, &#39;indulgent&#39;, &#39;industry&#39;, &#39;inept&#39;, &#39;inevitable&#39;, &#39;inevitably&#39;, &#39;infamous&#39;, &#39;inferior&#39;, &#39;influence&#39;, &#39;influenced&#39;, &#39;information&#39;, &#39;ingredients&#39;, &#39;initial&#39;, &#39;initially&#39;, &#39;inner&#39;, &#39;innocence&#39;, &#39;innocent&#39;, &#39;innovative&#39;, &#39;insane&#39;, &#39;inside&#39;, &#39;insight&#39;, &#39;inspector&#39;, &#39;inspiration&#39;, &#39;inspired&#39;, &#39;inspiring&#39;, &#39;installment&#39;, &#39;instance&#39;, &#39;instant&#39;, &#39;instantly&#39;, &#39;instead&#39;, &#39;instinct&#39;, &#39;insult&#39;, &#39;insulting&#39;, &#39;integrity&#39;, &#39;intellectual&#39;, &#39;intelligence&#39;, &#39;intelligent&#39;, &#39;intended&#39;, &#39;intense&#39;, &#39;intensity&#39;, &#39;intent&#39;, &#39;intention&#39;, &#39;intentionally&#39;, &#39;intentions&#39;, &#39;interaction&#39;, &#39;interest&#39;, &#39;interested&#39;, &#39;interesting&#39;, &#39;interests&#39;, &#39;international&#39;, &#39;internet&#39;, &#39;interpretation&#39;, &#39;interview&#39;, &#39;interviews&#39;, &#39;intimate&#39;, &#39;intrigue&#39;, &#39;intrigued&#39;, &#39;intriguing&#39;, &#39;introduce&#39;, &#39;introduced&#39;, &#39;introduces&#39;, &#39;introduction&#39;, &#39;invasion&#39;, &#39;invented&#39;, &#39;inventive&#39;, &#39;investigate&#39;, &#39;investigation&#39;, &#39;invisible&#39;, &#39;involve&#39;, &#39;involved&#39;, &#39;involvement&#39;, &#39;involves&#39;, &#39;involving&#39;, &#39;iran&#39;, &#39;iraq&#39;, &#39;ireland&#39;, &#39;irish&#39;, &#39;iron&#39;, &#39;ironic&#39;, &#39;ironically&#39;, &#39;irony&#39;, &#39;irrelevant&#39;, &#39;irritating&#39;, &#39;island&#39;, &#39;isolated&#39;, &#39;israel&#39;, &#39;issue&#39;, &#39;issues&#39;, &#39;italian&#39;, &#39;italy&#39;, &#39;jack&#39;, &#39;jackie&#39;, &#39;jackson&#39;, &#39;jail&#39;, &#39;jake&#39;, &#39;james&#39;, &#39;jamie&#39;, &#39;jane&#39;, &#39;japan&#39;, &#39;japanese&#39;, &#39;jason&#39;, &#39;jaw&#39;, &#39;jaws&#39;, &#39;jay&#39;, &#39;jazz&#39;, &#39;jealous&#39;, &#39;jean&#39;, &#39;jeff&#39;, &#39;jeffrey&#39;, &#39;jennifer&#39;, &#39;jenny&#39;, &#39;jeremy&#39;, &#39;jerk&#39;, &#39;jerry&#39;, &#39;jesse&#39;, &#39;jessica&#39;, &#39;jesus&#39;, &#39;jet&#39;, &#39;jewish&#39;, &#39;jim&#39;, &#39;jimmy&#39;, &#39;joan&#39;, &#39;job&#39;, &#39;jobs&#39;, &#39;joe&#39;, &#39;joel&#39;, &#39;joey&#39;, &#39;john&#39;, &#39;johnny&#39;, &#39;johnson&#39;, &#39;join&#39;, &#39;joined&#39;, &#39;joke&#39;, &#39;jokes&#39;, &#39;jon&#39;, &#39;jonathan&#39;, &#39;jones&#39;, &#39;joseph&#39;, &#39;josh&#39;, &#39;journalist&#39;, &#39;journey&#39;, &#39;joy&#39;, &#39;jr&#39;, &#39;judge&#39;, &#39;judging&#39;, &#39;judy&#39;, &#39;julia&#39;, &#39;julian&#39;, &#39;julie&#39;, &#39;jump&#39;, &#39;jumped&#39;, &#39;jumping&#39;, &#39;jumps&#39;, &#39;june&#39;, &#39;jungle&#39;, &#39;junior&#39;, &#39;junk&#39;, &#39;justice&#39;, &#39;justify&#39;, &#39;justin&#39;, &#39;juvenile&#39;, &#39;kane&#39;, &#39;kansas&#39;, &#39;kapoor&#39;, &#39;karen&#39;, &#39;karloff&#39;, &#39;kate&#39;, &#39;kay&#39;, &#39;keaton&#39;, &#39;keep&#39;, &#39;keeping&#39;, &#39;keeps&#39;, &#39;keith&#39;, &#39;kelly&#39;, &#39;ken&#39;, &#39;kennedy&#39;, &#39;kenneth&#39;, &#39;kept&#39;, &#39;kevin&#39;, &#39;key&#39;, &#39;khan&#39;, &#39;kick&#39;, &#39;kicked&#39;, &#39;kicking&#39;, &#39;kicks&#39;, &#39;kid&#39;, &#39;kidding&#39;, &#39;kidnapped&#39;, &#39;kids&#39;, &#39;kill&#39;, &#39;killed&#39;, &#39;killer&#39;, &#39;killers&#39;, &#39;killing&#39;, &#39;killings&#39;, &#39;kills&#39;, &#39;kim&#39;, &#39;kind&#39;, &#39;kinda&#39;, &#39;kinds&#39;, &#39;king&#39;, &#39;kingdom&#39;, &#39;kirk&#39;, &#39;kiss&#39;, &#39;kissing&#39;, &#39;kitchen&#39;, &#39;knew&#39;, &#39;knife&#39;, &#39;knock&#39;, &#39;know&#39;, &#39;knowing&#39;, &#39;knowledge&#39;, &#39;known&#39;, &#39;knows&#39;, &#39;kolchak&#39;, &#39;kong&#39;, &#39;korean&#39;, &#39;kubrick&#39;, &#39;kudos&#39;, &#39;kumar&#39;, &#39;kung&#39;, &#39;kurosawa&#39;, &#39;kurt&#39;, &#39;kyle&#39;, &#39;la&#39;, &#39;lab&#39;, &#39;lack&#39;, &#39;lacked&#39;, &#39;lacking&#39;, &#39;lackluster&#39;, &#39;lacks&#39;, &#39;ladies&#39;, &#39;lady&#39;, &#39;laid&#39;, &#39;lake&#39;, &#39;lame&#39;, &#39;land&#39;, &#39;landing&#39;, &#39;landscape&#39;, &#39;landscapes&#39;, &#39;lane&#39;, &#39;language&#39;, &#39;large&#39;, &#39;largely&#39;, &#39;larger&#39;, &#39;larry&#39;, &#39;last&#39;, &#39;lasted&#39;, &#39;late&#39;, &#39;lately&#39;, &#39;later&#39;, &#39;latest&#39;, &#39;latin&#39;, &#39;latter&#39;, &#39;laugh&#39;, &#39;laughable&#39;, &#39;laughably&#39;, &#39;laughed&#39;, &#39;laughing&#39;, &#39;laughs&#39;, &#39;laughter&#39;, &#39;laura&#39;, &#39;laurel&#39;, &#39;law&#39;, &#39;lawrence&#39;, &#39;laws&#39;, &#39;lawyer&#39;, &#39;lay&#39;, &#39;lazy&#39;, &#39;le&#39;, &#39;lead&#39;, &#39;leader&#39;, &#39;leading&#39;, &#39;leads&#39;, &#39;league&#39;, &#39;learn&#39;, &#39;learned&#39;, &#39;learning&#39;, &#39;learns&#39;, &#39;least&#39;, &#39;leave&#39;, &#39;leaves&#39;, &#39;leaving&#39;, &#39;led&#39;, &#39;lee&#39;, &#39;left&#39;, &#39;leg&#39;, &#39;legacy&#39;, &#39;legal&#39;, &#39;legend&#39;, &#39;legendary&#39;, &#39;legs&#39;, &#39;lemmon&#39;, &#39;lena&#39;, &#39;length&#39;, &#39;lengthy&#39;, &#39;leo&#39;, &#39;leon&#39;, &#39;leonard&#39;, &#39;les&#39;, &#39;lesbian&#39;, &#39;leslie&#39;, &#39;less&#39;, &#39;lesser&#39;, &#39;lesson&#39;, &#39;lessons&#39;, &#39;let&#39;, &#39;lets&#39;, &#39;letter&#39;, &#39;letters&#39;, &#39;letting&#39;, &#39;level&#39;, &#39;levels&#39;, &#39;lewis&#39;, &#39;li&#39;, &#39;liberal&#39;, &#39;library&#39;, &#39;lie&#39;, &#39;lies&#39;, &#39;life&#39;, &#39;lifestyle&#39;, &#39;lifetime&#39;, &#39;light&#39;, &#39;lighting&#39;, &#39;lights&#39;, &#39;likable&#39;, &#39;like&#39;, &#39;liked&#39;, &#39;likely&#39;, &#39;likes&#39;, &#39;likewise&#39;, &#39;liking&#39;, &#39;lily&#39;, &#39;limited&#39;, &#39;limits&#39;, &#39;lincoln&#39;, &#39;linda&#39;, &#39;line&#39;, &#39;liners&#39;, &#39;lines&#39;, &#39;link&#39;, &#39;lion&#39;, &#39;lips&#39;, &#39;lisa&#39;, &#39;list&#39;, &#39;listed&#39;, &#39;listen&#39;, &#39;listening&#39;, &#39;lit&#39;, &#39;literally&#39;, &#39;literature&#39;, &#39;little&#39;, &#39;live&#39;, &#39;lived&#39;, &#39;lively&#39;, &#39;lives&#39;, &#39;living&#39;, &#39;lloyd&#39;, &#39;load&#39;, &#39;loaded&#39;, &#39;loads&#39;, &#39;local&#39;, &#39;location&#39;, &#39;locations&#39;, &#39;locked&#39;, &#39;logan&#39;, &#39;logic&#39;, &#39;logical&#39;, &#39;lol&#39;, &#39;london&#39;, &#39;lone&#39;, &#39;lonely&#39;, &#39;long&#39;, &#39;longer&#39;, &#39;look&#39;, &#39;looked&#39;, &#39;looking&#39;, &#39;looks&#39;, &#39;loose&#39;, &#39;loosely&#39;, &#39;lord&#39;, &#39;los&#39;, &#39;lose&#39;, &#39;loser&#39;, &#39;losers&#39;, &#39;loses&#39;, &#39;losing&#39;, &#39;loss&#39;, &#39;lost&#39;, &#39;lot&#39;, &#39;lots&#39;, &#39;lou&#39;, &#39;loud&#39;, &#39;louis&#39;, &#39;louise&#39;, &#39;lousy&#39;, &#39;lovable&#39;, &#39;love&#39;, &#39;loved&#39;, &#39;lovely&#39;, &#39;lover&#39;, &#39;lovers&#39;, &#39;loves&#39;, &#39;loving&#39;, &#39;low&#39;, &#39;lower&#39;, &#39;lowest&#39;, &#39;loyal&#39;, &#39;loyalty&#39;, &#39;lucas&#39;, &#39;luck&#39;, &#39;luckily&#39;, &#39;lucky&#39;, &#39;lucy&#39;, &#39;ludicrous&#39;, &#39;lugosi&#39;, &#39;luke&#39;, &#39;lumet&#39;, &#39;lundgren&#39;, &#39;lust&#39;, &#39;lying&#39;, &#39;lynch&#39;, &#39;lyrics&#39;, &#39;macarthur&#39;, &#39;machine&#39;, &#39;machines&#39;, &#39;macy&#39;, &#39;mad&#39;, &#39;made&#39;, &#39;madness&#39;, &#39;madonna&#39;, &#39;mafia&#39;, &#39;magazine&#39;, &#39;maggie&#39;, &#39;magic&#39;, &#39;magical&#39;, &#39;magnificent&#39;, &#39;maid&#39;, &#39;mail&#39;, &#39;main&#39;, &#39;mainly&#39;, &#39;mainstream&#39;, &#39;maintain&#39;, &#39;major&#39;, &#39;majority&#39;, &#39;make&#39;, &#39;maker&#39;, &#39;makers&#39;, &#39;makes&#39;, &#39;makeup&#39;, &#39;making&#39;, &#39;male&#39;, &#39;mall&#39;, &#39;malone&#39;, &#39;man&#39;, &#39;manage&#39;, &#39;managed&#39;, &#39;manager&#39;, &#39;manages&#39;, &#39;manhattan&#39;, &#39;maniac&#39;, &#39;manipulative&#39;, &#39;mankind&#39;, &#39;mann&#39;, &#39;manner&#39;, &#39;mansion&#39;, &#39;many&#39;, &#39;map&#39;, &#39;marc&#39;, &#39;march&#39;, &#39;margaret&#39;, &#39;maria&#39;, &#39;marie&#39;, &#39;mario&#39;, &#39;marion&#39;, &#39;mark&#39;, &#39;market&#39;, &#39;marketing&#39;, &#39;marks&#39;, &#39;marriage&#39;, &#39;married&#39;, &#39;marry&#39;, &#39;mars&#39;, &#39;marshall&#39;, &#39;martial&#39;, &#39;martin&#39;, &#39;marty&#39;, &#39;marvelous&#39;, &#39;mary&#39;, &#39;mask&#39;, &#39;masks&#39;, &#39;mass&#39;, &#39;massacre&#39;, &#39;masses&#39;, &#39;massive&#39;, &#39;master&#39;, &#39;masterful&#39;, &#39;masterpiece&#39;, &#39;masterpieces&#39;, &#39;masters&#39;, &#39;match&#39;, &#39;matched&#39;, &#39;matches&#39;, &#39;mate&#39;, &#39;material&#39;, &#39;matrix&#39;, &#39;matt&#39;, &#39;matter&#39;, &#39;matters&#39;, &#39;matthau&#39;, &#39;matthew&#39;, &#39;mature&#39;, &#39;max&#39;, &#39;may&#39;, &#39;maybe&#39;, &#39;mayor&#39;, &#39;mclaglen&#39;, &#39;mean&#39;, &#39;meaning&#39;, &#39;meaningful&#39;, &#39;meaningless&#39;, &#39;means&#39;, &#39;meant&#39;, &#39;meanwhile&#39;, &#39;measure&#39;, &#39;meat&#39;, &#39;mechanical&#39;, &#39;media&#39;, &#39;medical&#39;, &#39;mediocre&#39;, &#39;medium&#39;, &#39;meet&#39;, &#39;meeting&#39;, &#39;meets&#39;, &#39;mel&#39;, &#39;melodrama&#39;, &#39;melodramatic&#39;, &#39;melting&#39;, &#39;member&#39;, &#39;members&#39;, &#39;memorable&#39;, &#39;memories&#39;, &#39;memory&#39;, &#39;men&#39;, &#39;menace&#39;, &#39;menacing&#39;, &#39;mental&#39;, &#39;mentally&#39;, &#39;mention&#39;, &#39;mentioned&#39;, &#39;mentioning&#39;, &#39;mentions&#39;, &#39;mere&#39;, &#39;merely&#39;, &#39;merit&#39;, &#39;merits&#39;, &#39;meryl&#39;, &#39;mess&#39;, &#39;message&#39;, &#39;messages&#39;, &#39;messed&#39;, &#39;met&#39;, &#39;metal&#39;, &#39;metaphor&#39;, &#39;method&#39;, &#39;methods&#39;, &#39;mexican&#39;, &#39;mexico&#39;, &#39;mgm&#39;, &#39;michael&#39;, &#39;michelle&#39;, &#39;mickey&#39;, &#39;mid&#39;, &#39;middle&#39;, &#39;midnight&#39;, &#39;might&#39;, &#39;mighty&#39;, &#39;miike&#39;, &#39;mike&#39;, &#39;mild&#39;, &#39;mildly&#39;, &#39;mildred&#39;, &#39;mile&#39;, &#39;miles&#39;, &#39;military&#39;, &#39;milk&#39;, &#39;mill&#39;, &#39;miller&#39;, &#39;million&#39;, &#39;millionaire&#39;, &#39;millions&#39;, &#39;min&#39;, &#39;mind&#39;, &#39;minded&#39;, &#39;mindless&#39;, &#39;minds&#39;, &#39;mine&#39;, &#39;mini&#39;, &#39;minimal&#39;, &#39;minimum&#39;, &#39;minor&#39;, &#39;minute&#39;, &#39;minutes&#39;, &#39;miracle&#39;, &#39;mirror&#39;, &#39;miscast&#39;, &#39;miserable&#39;, &#39;miserably&#39;, &#39;misery&#39;, &#39;miss&#39;, &#39;missed&#39;, &#39;misses&#39;, &#39;missing&#39;, &#39;mission&#39;, &#39;mistake&#39;, &#39;mistaken&#39;, &#39;mistakes&#39;, &#39;mistress&#39;, &#39;mitchell&#39;, &#39;mix&#39;, &#39;mixed&#39;, &#39;mixture&#39;, &#39;miyazaki&#39;, &#39;mm&#39;, &#39;mob&#39;, &#39;model&#39;, &#39;models&#39;, &#39;modern&#39;, &#39;modesty&#39;, &#39;molly&#39;, &#39;mom&#39;, &#39;moment&#39;, &#39;moments&#39;, &#39;mon&#39;, &#39;money&#39;, &#39;monk&#39;, &#39;monkey&#39;, &#39;monkeys&#39;, &#39;monster&#39;, &#39;monsters&#39;, &#39;montage&#39;, &#39;montana&#39;, &#39;month&#39;, &#39;months&#39;, &#39;mood&#39;, &#39;moody&#39;, &#39;moon&#39;, &#39;moore&#39;, &#39;moral&#39;, &#39;morality&#39;, &#39;morgan&#39;, &#39;morning&#39;, &#39;moronic&#39;, &#39;morris&#39;, &#39;mostly&#39;, &#39;mother&#39;, &#39;motion&#39;, &#39;motivation&#39;, &#39;motivations&#39;, &#39;motives&#39;, &#39;mountain&#39;, &#39;mountains&#39;, &#39;mouse&#39;, &#39;mouth&#39;, &#39;move&#39;, &#39;moved&#39;, &#39;movement&#39;, &#39;movements&#39;, &#39;moves&#39;, &#39;movie&#39;, &#39;movies&#39;, &#39;moving&#39;, &#39;mr&#39;, &#39;mrs&#39;, &#39;ms&#39;, &#39;mst&#39;, &#39;mtv&#39;, &#39;much&#39;, &#39;multi&#39;, &#39;multiple&#39;, &#39;mummy&#39;, &#39;mundane&#39;, &#39;murder&#39;, &#39;murdered&#39;, &#39;murderer&#39;, &#39;murderous&#39;, &#39;murders&#39;, &#39;murphy&#39;, &#39;murray&#39;, &#39;museum&#39;, &#39;music&#39;, &#39;musical&#39;, &#39;musicals&#39;, &#39;muslim&#39;, &#39;must&#39;, &#39;myers&#39;, &#39;mysteries&#39;, &#39;mysterious&#39;, &#39;mystery&#39;, &#39;nail&#39;, &#39;naive&#39;, &#39;naked&#39;, &#39;name&#39;, &#39;named&#39;, &#39;namely&#39;, &#39;names&#39;, &#39;nancy&#39;, &#39;narration&#39;, &#39;narrative&#39;, &#39;narrator&#39;, &#39;nasty&#39;, &#39;nathan&#39;, &#39;nation&#39;, &#39;national&#39;, &#39;native&#39;, &#39;natural&#39;, &#39;naturally&#39;, &#39;nature&#39;, &#39;navy&#39;, &#39;nazi&#39;, &#39;nazis&#39;, &#39;nd&#39;, &#39;near&#39;, &#39;nearby&#39;, &#39;nearly&#39;, &#39;neat&#39;, &#39;necessarily&#39;, &#39;necessary&#39;, &#39;neck&#39;, &#39;ned&#39;, &#39;need&#39;, &#39;needed&#39;, &#39;needless&#39;, &#39;needs&#39;, &#39;negative&#39;, &#39;neighbor&#39;, &#39;neighborhood&#39;, &#39;neighbors&#39;, &#39;neil&#39;, &#39;neither&#39;, &#39;nelson&#39;, &#39;neo&#39;, &#39;nephew&#39;, &#39;nerd&#39;, &#39;nervous&#39;, &#39;network&#39;, &#39;never&#39;, &#39;nevertheless&#39;, &#39;new&#39;, &#39;newly&#39;, &#39;newman&#39;, &#39;news&#39;, &#39;newspaper&#39;, &#39;next&#39;, &#39;nice&#39;, &#39;nicely&#39;, &#39;nicholas&#39;, &#39;nicholson&#39;, &#39;nick&#39;, &#39;nicole&#39;, &#39;night&#39;, &#39;nightmare&#39;, &#39;nightmares&#39;, &#39;nights&#39;, &#39;nine&#39;, &#39;ninja&#39;, &#39;niro&#39;, &#39;noble&#39;, &#39;nobody&#39;, &#39;noir&#39;, &#39;noise&#39;, &#39;nominated&#39;, &#39;nomination&#39;, &#39;non&#39;, &#39;none&#39;, &#39;nonetheless&#39;, &#39;nonsense&#39;, &#39;nonsensical&#39;, &#39;normal&#39;, &#39;normally&#39;, &#39;norman&#39;, &#39;north&#39;, &#39;nose&#39;, &#39;nostalgia&#39;, &#39;nostalgic&#39;, &#39;notable&#39;, &#39;notably&#39;, &#39;notch&#39;, &#39;note&#39;, &#39;noted&#39;, &#39;notes&#39;, &#39;nothing&#39;, &#39;notice&#39;, &#39;noticed&#39;, &#39;notion&#39;, &#39;notorious&#39;, &#39;novak&#39;, &#39;novel&#39;, &#39;novels&#39;, &#39;nowadays&#39;, &#39;nowhere&#39;, &#39;nuclear&#39;, &#39;nude&#39;, &#39;nudity&#39;, &#39;number&#39;, &#39;numbers&#39;, &#39;numerous&#39;, &#39;nurse&#39;, &#39;nuts&#39;, &#39;nyc&#39;, &#39;object&#39;, &#39;objective&#39;, &#39;obnoxious&#39;, &#39;obscure&#39;, &#39;obsessed&#39;, &#39;obsession&#39;, &#39;obvious&#39;, &#39;obviously&#39;, &#39;occasion&#39;, &#39;occasional&#39;, &#39;occasionally&#39;, &#39;occur&#39;, &#39;occurred&#39;, &#39;occurs&#39;, &#39;ocean&#39;, &#39;odd&#39;, &#39;oddly&#39;, &#39;odds&#39;, &#39;offended&#39;, &#39;offensive&#39;, &#39;offer&#39;, &#39;offered&#39;, &#39;offering&#39;, &#39;offers&#39;, &#39;office&#39;, &#39;officer&#39;, &#39;officers&#39;, &#39;official&#39;, &#39;often&#39;, &#39;oh&#39;, &#39;oil&#39;, &#39;ok&#39;, &#39;okay&#39;, &#39;old&#39;, &#39;older&#39;, &#39;oliver&#39;, &#39;olivier&#39;, &#39;ollie&#39;, &#39;omen&#39;, &#39;one&#39;, &#39;ones&#39;, &#39;online&#39;, &#39;onto&#39;, &#39;open&#39;, &#39;opened&#39;, &#39;opening&#39;, &#39;opens&#39;, &#39;opera&#39;, &#39;operation&#39;, &#39;opinion&#39;, &#39;opinions&#39;, &#39;opportunities&#39;, &#39;opportunity&#39;, &#39;opposed&#39;, &#39;opposite&#39;, &#39;orange&#39;, &#39;order&#39;, &#39;ordered&#39;, &#39;orders&#39;, &#39;ordinary&#39;, &#39;original&#39;, &#39;originality&#39;, &#39;originally&#39;, &#39;orleans&#39;, &#39;orson&#39;, &#39;oscar&#39;, &#39;oscars&#39;, &#39;othello&#39;, &#39;others&#39;, &#39;otherwise&#39;, &#39;ought&#39;, &#39;outcome&#39;, &#39;outer&#39;, &#39;outfit&#39;, &#39;outrageous&#39;, &#39;outside&#39;, &#39;outstanding&#39;, &#39;overacting&#39;, &#39;overall&#39;, &#39;overcome&#39;, &#39;overdone&#39;, &#39;overlook&#39;, &#39;overlooked&#39;, &#39;overly&#39;, &#39;overrated&#39;, &#39;overwhelming&#39;, &#39;owen&#39;, &#39;owner&#39;, &#39;oz&#39;, &#39;pace&#39;, &#39;paced&#39;, &#39;pacing&#39;, &#39;pacino&#39;, &#39;pack&#39;, &#39;package&#39;, &#39;packed&#39;, &#39;page&#39;, &#39;paid&#39;, &#39;pain&#39;, &#39;painful&#39;, &#39;painfully&#39;, &#39;paint&#39;, &#39;painted&#39;, &#39;painting&#39;, &#39;pair&#39;, &#39;pal&#39;, &#39;palace&#39;, &#39;palance&#39;, &#39;palma&#39;, &#39;paltrow&#39;, &#39;pamela&#39;, &#39;pan&#39;, &#39;panic&#39;, &#39;pants&#39;, &#39;paper&#39;, &#39;par&#39;, &#39;parallel&#39;, &#39;paranoia&#39;, &#39;parent&#39;, &#39;parents&#39;, &#39;paris&#39;, &#39;park&#39;, &#39;parker&#39;, &#39;parody&#39;, &#39;part&#39;, &#39;particular&#39;, &#39;particularly&#39;, &#39;parties&#39;, &#39;partly&#39;, &#39;partner&#39;, &#39;parts&#39;, &#39;party&#39;, &#39;pass&#39;, &#39;passable&#39;, &#39;passed&#39;, &#39;passes&#39;, &#39;passing&#39;, &#39;passion&#39;, &#39;passionate&#39;, &#39;past&#39;, &#39;pat&#39;, &#39;path&#39;, &#39;pathetic&#39;, &#39;patience&#39;, &#39;patient&#39;, &#39;patients&#39;, &#39;patrick&#39;, &#39;paul&#39;, &#39;paulie&#39;, &#39;pay&#39;, &#39;paying&#39;, &#39;pays&#39;, &#39;peace&#39;, &#39;peak&#39;, &#39;pearl&#39;, &#39;people&#39;, &#39;peoples&#39;, &#39;per&#39;, &#39;perfect&#39;, &#39;perfection&#39;, &#39;perfectly&#39;, &#39;perform&#39;, &#39;performance&#39;, &#39;performances&#39;, &#39;performed&#39;, &#39;performer&#39;, &#39;performers&#39;, &#39;performing&#39;, &#39;performs&#39;, &#39;perhaps&#39;, &#39;period&#39;, &#39;perry&#39;, &#39;person&#39;, &#39;persona&#39;, &#39;personal&#39;, &#39;personalities&#39;, &#39;personality&#39;, &#39;personally&#39;, &#39;persons&#39;, &#39;perspective&#39;, &#39;pet&#39;, &#39;pete&#39;, &#39;peter&#39;, &#39;peters&#39;, &#39;petty&#39;, &#39;pg&#39;, &#39;phantom&#39;, &#39;phil&#39;, &#39;philip&#39;, &#39;philosophical&#39;, &#39;philosophy&#39;, &#39;phone&#39;, &#39;phony&#39;, &#39;photo&#39;, &#39;photographed&#39;, &#39;photographer&#39;, &#39;photography&#39;, &#39;photos&#39;, &#39;physical&#39;, &#39;physically&#39;, &#39;piano&#39;, &#39;pick&#39;, &#39;picked&#39;, &#39;picking&#39;, &#39;picks&#39;, &#39;picture&#39;, &#39;pictures&#39;, &#39;pie&#39;, &#39;piece&#39;, &#39;pieces&#39;, &#39;pierce&#39;, &#39;pig&#39;, &#39;pile&#39;, &#39;pilot&#39;, &#39;pin&#39;, &#39;pink&#39;, &#39;pit&#39;, &#39;pitch&#39;, &#39;pitt&#39;, &#39;pity&#39;, &#39;place&#39;, &#39;placed&#39;, &#39;places&#39;, &#39;plague&#39;, &#39;plain&#39;, &#39;plan&#39;, &#39;plane&#39;, &#39;planet&#39;, &#39;planned&#39;, &#39;planning&#39;, &#39;plans&#39;, &#39;plant&#39;, &#39;plastic&#39;, &#39;plausible&#39;, &#39;play&#39;, &#39;played&#39;, &#39;player&#39;, &#39;players&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;pleasant&#39;, &#39;pleasantly&#39;, &#39;please&#39;, &#39;pleased&#39;, &#39;pleasure&#39;, &#39;plenty&#39;, &#39;plight&#39;, &#39;plot&#39;, &#39;plots&#39;, &#39;plus&#39;, &#39;poem&#39;, &#39;poetic&#39;, &#39;poetry&#39;, &#39;poignant&#39;, &#39;point&#39;, &#39;pointed&#39;, &#39;pointless&#39;, &#39;points&#39;, &#39;pokemon&#39;, &#39;polanski&#39;, &#39;police&#39;, &#39;polished&#39;, &#39;political&#39;, &#39;politically&#39;, &#39;politics&#39;, &#39;pool&#39;, &#39;poor&#39;, &#39;poorly&#39;, &#39;pop&#39;, &#39;popcorn&#39;, &#39;pops&#39;, &#39;popular&#39;, &#39;popularity&#39;, &#39;population&#39;, &#39;porn&#39;, &#39;porno&#39;, &#39;portion&#39;, &#39;portrait&#39;, &#39;portray&#39;, &#39;portrayal&#39;, &#39;portrayed&#39;, &#39;portraying&#39;, &#39;portrays&#39;, &#39;position&#39;, &#39;positive&#39;, &#39;possessed&#39;, &#39;possibilities&#39;, &#39;possibility&#39;, &#39;possible&#39;, &#39;possibly&#39;, &#39;post&#39;, &#39;poster&#39;, &#39;pot&#39;, &#39;potential&#39;, &#39;potentially&#39;, &#39;poverty&#39;, &#39;powell&#39;, &#39;power&#39;, &#39;powerful&#39;, &#39;powers&#39;, &#39;practically&#39;, &#39;practice&#39;, &#39;praise&#39;, &#39;pre&#39;, &#39;precious&#39;, &#39;predictable&#39;, &#39;prefer&#39;, &#39;pregnant&#39;, &#39;premiere&#39;, &#39;premise&#39;, &#39;prepared&#39;, &#39;prequel&#39;, &#39;presence&#39;, &#39;present&#39;, &#39;presentation&#39;, &#39;presented&#39;, &#39;presents&#39;, &#39;president&#39;, &#39;press&#39;, &#39;pressure&#39;, &#39;preston&#39;, &#39;presumably&#39;, &#39;pretend&#39;, &#39;pretending&#39;, &#39;pretentious&#39;, &#39;pretty&#39;, &#39;prevent&#39;, &#39;preview&#39;, &#39;previous&#39;, &#39;previously&#39;, &#39;prey&#39;, &#39;price&#39;, &#39;priceless&#39;, &#39;pride&#39;, &#39;priest&#39;, &#39;primarily&#39;, &#39;primary&#39;, &#39;prime&#39;, &#39;prince&#39;, &#39;princess&#39;, &#39;principal&#39;, &#39;print&#39;, &#39;prior&#39;, &#39;prison&#39;, &#39;prisoner&#39;, &#39;prisoners&#39;, &#39;private&#39;, &#39;prize&#39;, &#39;pro&#39;, &#39;probably&#39;, &#39;problem&#39;, &#39;problems&#39;, &#39;proceedings&#39;, &#39;proceeds&#39;, &#39;process&#39;, &#39;produce&#39;, &#39;produced&#39;, &#39;producer&#39;, &#39;producers&#39;, &#39;producing&#39;, &#39;product&#39;, &#39;production&#39;, &#39;productions&#39;, &#39;professional&#39;, &#39;professor&#39;, &#39;profound&#39;, &#39;program&#39;, &#39;progress&#39;, &#39;progresses&#39;, &#39;project&#39;, &#39;projects&#39;, &#39;prom&#39;, &#39;promise&#39;, &#39;promised&#39;, &#39;promises&#39;, &#39;promising&#39;, &#39;proof&#39;, &#39;propaganda&#39;, &#39;proper&#39;, &#39;properly&#39;, &#39;property&#39;, &#39;props&#39;, &#39;prostitute&#39;, &#39;protagonist&#39;, &#39;protagonists&#39;, &#39;protect&#39;, &#39;proud&#39;, &#39;prove&#39;, &#39;proved&#39;, &#39;proves&#39;, &#39;provide&#39;, &#39;provided&#39;, &#39;provides&#39;, &#39;providing&#39;, &#39;provoking&#39;, &#39;pseudo&#39;, &#39;psychiatrist&#39;, &#39;psychic&#39;, &#39;psycho&#39;, &#39;psychological&#39;, &#39;psychotic&#39;, &#39;public&#39;, &#39;pull&#39;, &#39;pulled&#39;, &#39;pulling&#39;, &#39;pulls&#39;, &#39;pulp&#39;, &#39;punch&#39;, &#39;punishment&#39;, &#39;punk&#39;, &#39;puppet&#39;, &#39;purchase&#39;, &#39;purchased&#39;, &#39;pure&#39;, &#39;purely&#39;, &#39;purple&#39;, &#39;purpose&#39;, &#39;purposes&#39;, &#39;pursuit&#39;, &#39;push&#39;, &#39;pushed&#39;, &#39;pushing&#39;, &#39;put&#39;, &#39;puts&#39;, &#39;putting&#39;, &#39;qualities&#39;, &#39;quality&#39;, &#39;queen&#39;, &#39;quest&#39;, &#39;question&#39;, &#39;questionable&#39;, &#39;questions&#39;, &#39;quick&#39;, &#39;quickly&#39;, &#39;quiet&#39;, &#39;quinn&#39;, &#39;quirky&#39;, &#39;quit&#39;, &#39;quite&#39;, &#39;quote&#39;, &#39;quotes&#39;, &#39;rabbit&#39;, &#39;race&#39;, &#39;rachel&#39;, &#39;racial&#39;, &#39;racism&#39;, &#39;racist&#39;, &#39;radio&#39;, &#39;rage&#39;, &#39;rain&#39;, &#39;raines&#39;, &#39;raise&#39;, &#39;raised&#39;, &#39;raising&#39;, &#39;ralph&#39;, &#39;rambo&#39;, &#39;ramones&#39;, &#39;ran&#39;, &#39;random&#39;, &#39;randomly&#39;, &#39;randy&#39;, &#39;range&#39;, &#39;rangers&#39;, &#39;rank&#39;, &#39;ranks&#39;, &#39;rap&#39;, &#39;rape&#39;, &#39;raped&#39;, &#39;rare&#39;, &#39;rarely&#39;, &#39;rat&#39;, &#39;rate&#39;, &#39;rated&#39;, &#39;rather&#39;, &#39;rating&#39;, &#39;ratings&#39;, &#39;rats&#39;, &#39;rave&#39;, &#39;raw&#39;, &#39;ray&#39;, &#39;raymond&#39;, &#39;rd&#39;, &#39;reach&#39;, &#39;reached&#39;, &#39;reaches&#39;, &#39;reaching&#39;, &#39;react&#39;, &#39;reaction&#39;, &#39;reactions&#39;, &#39;read&#39;, &#39;reader&#39;, &#39;reading&#39;, &#39;reads&#39;, &#39;ready&#39;, &#39;real&#39;, &#39;realise&#39;, &#39;realism&#39;, &#39;realistic&#39;, &#39;reality&#39;, &#39;realize&#39;, &#39;realized&#39;, &#39;realizes&#39;, &#39;realizing&#39;, &#39;really&#39;, &#39;reason&#39;, &#39;reasonable&#39;, &#39;reasonably&#39;, &#39;reasons&#39;, &#39;rebel&#39;, &#39;recall&#39;, &#39;receive&#39;, &#39;received&#39;, &#39;receives&#39;, &#39;recent&#39;, &#39;recently&#39;, &#39;recognition&#39;, &#39;recognize&#39;, &#39;recognized&#39;, &#39;recommend&#39;, &#39;recommended&#39;, &#39;record&#39;, &#39;recorded&#39;, &#39;recording&#39;, &#39;red&#39;, &#39;redeeming&#39;, &#39;redemption&#39;, &#39;reduced&#39;, &#39;reed&#39;, &#39;reel&#39;, &#39;reference&#39;, &#39;references&#39;, &#39;reflect&#39;, &#39;reflection&#39;, &#39;refreshing&#39;, &#39;refused&#39;, &#39;refuses&#39;, &#39;regard&#39;, &#39;regarding&#39;, &#39;regardless&#39;, &#39;regret&#39;, &#39;regular&#39;, &#39;reid&#39;, &#39;relate&#39;, &#39;related&#39;, &#39;relation&#39;, &#39;relations&#39;, &#39;relationship&#39;, &#39;relationships&#39;, &#39;relative&#39;, &#39;relatively&#39;, &#39;relatives&#39;, &#39;release&#39;, &#39;released&#39;, &#39;relevant&#39;, &#39;relief&#39;, &#39;relies&#39;, &#39;religion&#39;, &#39;religious&#39;, &#39;remain&#39;, &#39;remained&#39;, &#39;remaining&#39;, &#39;remains&#39;, &#39;remake&#39;, &#39;remarkable&#39;, &#39;remarkably&#39;, &#39;remarks&#39;, &#39;remember&#39;, &#39;remembered&#39;, &#39;remind&#39;, &#39;reminded&#39;, &#39;reminds&#39;, &#39;reminiscent&#39;, &#39;remote&#39;, &#39;remotely&#39;, &#39;removed&#39;, &#39;rendition&#39;, &#39;rent&#39;, &#39;rental&#39;, &#39;rented&#39;, &#39;renting&#39;, &#39;repeat&#39;, &#39;repeated&#39;, &#39;repeatedly&#39;, &#39;repetitive&#39;, &#39;replaced&#39;, &#39;report&#39;, &#39;reporter&#39;, &#39;represent&#39;, &#39;represented&#39;, &#39;represents&#39;, &#39;reputation&#39;, &#39;required&#39;, &#39;requires&#39;, &#39;rescue&#39;, &#39;research&#39;, &#39;resemblance&#39;, &#39;resemble&#39;, &#39;resembles&#39;, &#39;resident&#39;, &#39;resist&#39;, &#39;resolution&#39;, &#39;resort&#39;, &#39;resources&#39;, &#39;respect&#39;, &#39;respected&#39;, &#39;respectively&#39;, &#39;response&#39;, &#39;responsibility&#39;, &#39;responsible&#39;, &#39;rest&#39;, &#39;restaurant&#39;, &#39;restored&#39;, &#39;result&#39;, &#39;resulting&#39;, &#39;results&#39;, &#39;retarded&#39;, &#39;retired&#39;, &#39;return&#39;, &#39;returned&#39;, &#39;returning&#39;, &#39;returns&#39;, &#39;reunion&#39;, &#39;reveal&#39;, &#39;revealed&#39;, &#39;revealing&#39;, &#39;reveals&#39;, &#39;revelation&#39;, &#39;revenge&#39;, &#39;review&#39;, &#39;reviewer&#39;, &#39;reviewers&#39;, &#39;reviews&#39;, &#39;revolution&#39;, &#39;revolutionary&#39;, &#39;revolves&#39;, &#39;rex&#39;, &#39;reynolds&#39;, &#39;rich&#39;, &#39;richard&#39;, &#39;richards&#39;, &#39;richardson&#39;, &#39;rick&#39;, &#39;rid&#39;, &#39;ridden&#39;, &#39;ride&#39;, &#39;ridiculous&#39;, &#39;ridiculously&#39;, &#39;riding&#39;, &#39;right&#39;, &#39;rights&#39;, &#39;ring&#39;, &#39;rings&#39;, &#39;rip&#39;, &#39;ripped&#39;, &#39;rise&#39;, &#39;rises&#39;, &#39;rising&#39;, &#39;risk&#39;, &#39;rita&#39;, &#39;ritter&#39;, &#39;rival&#39;, &#39;river&#39;, &#39;riveting&#39;, &#39;road&#39;, &#39;rob&#39;, &#39;robbery&#39;, &#39;robbins&#39;, &#39;robert&#39;, &#39;roberts&#39;, &#39;robin&#39;, &#39;robinson&#39;, &#39;robot&#39;, &#39;robots&#39;, &#39;rochester&#39;, &#39;rock&#39;, &#39;rocket&#39;, &#39;rocks&#39;, &#39;rocky&#39;, &#39;roger&#39;, &#39;rogers&#39;, &#39;role&#39;, &#39;roles&#39;, &#39;roll&#39;, &#39;rolled&#39;, &#39;rolling&#39;, &#39;roman&#39;, &#39;romance&#39;, &#39;romantic&#39;, &#39;romero&#39;, &#39;romp&#39;, &#39;ron&#39;, &#39;room&#39;, &#39;rooms&#39;, &#39;rooney&#39;, &#39;root&#39;, &#39;roots&#39;, &#39;rose&#39;, &#39;ross&#39;, &#39;roth&#39;, &#39;rotten&#39;, &#39;rough&#39;, &#39;round&#39;, &#39;routine&#39;, &#39;row&#39;, &#39;roy&#39;, &#39;royal&#39;, &#39;rubber&#39;, &#39;rubbish&#39;, &#39;ruby&#39;, &#39;ruin&#39;, &#39;ruined&#39;, &#39;ruins&#39;, &#39;rukh&#39;, &#39;rule&#39;, &#39;rules&#39;, &#39;run&#39;, &#39;running&#39;, &#39;runs&#39;, &#39;rural&#39;, &#39;rush&#39;, &#39;rushed&#39;, &#39;russell&#39;, &#39;russia&#39;, &#39;russian&#39;, &#39;ruth&#39;, &#39;ruthless&#39;, &#39;ryan&#39;, &#39;sabrina&#39;, &#39;sacrifice&#39;, &#39;sad&#39;, &#39;sadistic&#39;, &#39;sadly&#39;, &#39;sadness&#39;, &#39;safe&#39;, &#39;safety&#39;, &#39;saga&#39;, &#39;said&#39;, &#39;sake&#39;, &#39;sally&#39;, &#39;sam&#39;, &#39;samurai&#39;, &#39;san&#39;, &#39;sandler&#39;, &#39;sandra&#39;, &#39;santa&#39;, &#39;sappy&#39;, &#39;sarah&#39;, &#39;sat&#39;, &#39;satan&#39;, &#39;satire&#39;, &#39;satisfied&#39;, &#39;satisfy&#39;, &#39;satisfying&#39;, &#39;saturday&#39;, &#39;savage&#39;, &#39;save&#39;, &#39;saved&#39;, &#39;saves&#39;, &#39;saving&#39;, &#39;saw&#39;, &#39;say&#39;, &#39;saying&#39;, &#39;says&#39;, &#39;scale&#39;, &#39;scare&#39;, &#39;scarecrow&#39;, &#39;scared&#39;, &#39;scares&#39;, &#39;scary&#39;, &#39;scenario&#39;, &#39;scene&#39;, &#39;scenery&#39;, &#39;scenes&#39;, &#39;scheme&#39;, &#39;school&#39;, &#39;sci&#39;, &#39;science&#39;, &#39;scientific&#39;, &#39;scientist&#39;, &#39;scientists&#39;, &#39;scooby&#39;, &#39;scope&#39;, &#39;score&#39;, &#39;scores&#39;, &#39;scotland&#39;, &#39;scott&#39;, &#39;scottish&#39;, &#39;scream&#39;, &#39;screaming&#39;, &#39;screams&#39;, &#39;screen&#39;, &#39;screening&#39;, &#39;screenplay&#39;, &#39;screenwriter&#39;, &#39;script&#39;, &#39;scripted&#39;, &#39;scripts&#39;, &#39;scrooge&#39;, &#39;sea&#39;, &#39;seagal&#39;, &#39;sean&#39;, &#39;search&#39;, &#39;searching&#39;, &#39;season&#39;, &#39;seasons&#39;, &#39;seat&#39;, &#39;second&#39;, &#39;secondly&#39;, &#39;seconds&#39;, &#39;secret&#39;, &#39;secretary&#39;, &#39;secretly&#39;, &#39;secrets&#39;, &#39;section&#39;, &#39;security&#39;, &#39;see&#39;, &#39;seed&#39;, &#39;seeing&#39;, &#39;seek&#39;, &#39;seeking&#39;, &#39;seeks&#39;, &#39;seem&#39;, &#39;seemed&#39;, &#39;seemingly&#39;, &#39;seems&#39;, &#39;seen&#39;, &#39;sees&#39;, &#39;segment&#39;, &#39;segments&#39;, &#39;seldom&#39;, &#39;self&#39;, &#39;selfish&#39;, &#39;sell&#39;, &#39;sellers&#39;, &#39;selling&#39;, &#39;semi&#39;, &#39;send&#39;, &#39;sends&#39;, &#39;sense&#39;, &#39;senseless&#39;, &#39;senses&#39;, &#39;sensitive&#39;, &#39;sent&#39;, &#39;sentence&#39;, &#39;sentimental&#39;, &#39;separate&#39;, &#39;september&#39;, &#39;sequel&#39;, &#39;sequels&#39;, &#39;sequence&#39;, &#39;sequences&#39;, &#39;serial&#39;, &#39;series&#39;, &#39;serious&#39;, &#39;seriously&#39;, &#39;serve&#39;, &#39;served&#39;, &#39;serves&#39;, &#39;service&#39;, &#39;serving&#39;, &#39;set&#39;, &#39;sets&#39;, &#39;setting&#39;, &#39;settings&#39;, &#39;settle&#39;, &#39;seven&#39;, &#39;seventies&#39;, &#39;several&#39;, &#39;severe&#39;, &#39;sex&#39;, &#39;sexual&#39;, &#39;sexuality&#39;, &#39;sexually&#39;, &#39;sexy&#39;, &#39;sh&#39;, &#39;shadow&#39;, &#39;shadows&#39;, &#39;shake&#39;, &#39;shakespeare&#39;, &#39;shaky&#39;, &#39;shall&#39;, &#39;shallow&#39;, &#39;shame&#39;, &#39;shanghai&#39;, &#39;shape&#39;, &#39;share&#39;, &#39;shark&#39;, &#39;sharp&#39;, &#39;shaw&#39;, &#39;shed&#39;, &#39;sheer&#39;, &#39;shelf&#39;, &#39;shelley&#39;, &#39;sheriff&#39;, &#39;shine&#39;, &#39;shines&#39;, &#39;shining&#39;, &#39;ship&#39;, &#39;ships&#39;, &#39;shirley&#39;, &#39;shirt&#39;, &#39;shock&#39;, &#39;shocked&#39;, &#39;shocking&#39;, &#39;shoes&#39;, &#39;shoot&#39;, &#39;shooting&#39;, &#39;shoots&#39;, &#39;shop&#39;, &#39;short&#39;, &#39;shortly&#39;, &#39;shorts&#39;, &#39;shot&#39;, &#39;shots&#39;, &#39;show&#39;, &#39;showcase&#39;, &#39;showdown&#39;, &#39;showed&#39;, &#39;shower&#39;, &#39;showing&#39;, &#39;shown&#39;, &#39;shows&#39;, &#39;shut&#39;, &#39;shy&#39;, &#39;sick&#39;, &#39;sid&#39;, &#39;side&#39;, &#39;sidekick&#39;, &#39;sides&#39;, &#39;sidney&#39;, &#39;sight&#39;, &#39;sign&#39;, &#39;signed&#39;, &#39;significance&#39;, &#39;significant&#39;, &#39;signs&#39;, &#39;silence&#39;, &#39;silent&#39;, &#39;silly&#39;, &#39;silver&#39;, &#39;similar&#39;, &#39;similarities&#39;, &#39;similarly&#39;, &#39;simmons&#39;, &#39;simon&#39;, &#39;simple&#39;, &#39;simplicity&#39;, &#39;simplistic&#39;, &#39;simply&#39;, &#39;simpson&#39;, &#39;sin&#39;, &#39;sinatra&#39;, &#39;since&#39;, &#39;sincere&#39;, &#39;sing&#39;, &#39;singer&#39;, &#39;singers&#39;, &#39;singing&#39;, &#39;single&#39;, &#39;sings&#39;, &#39;sinister&#39;, &#39;sink&#39;, &#39;sir&#39;, &#39;sirk&#39;, &#39;sissy&#39;, &#39;sister&#39;, &#39;sisters&#39;, &#39;sit&#39;, &#39;sitcom&#39;, &#39;site&#39;, &#39;sits&#39;, &#39;sitting&#39;, &#39;situation&#39;, &#39;situations&#39;, &#39;six&#39;, &#39;sixties&#39;, &#39;size&#39;, &#39;skill&#39;, &#39;skills&#39;, &#39;skin&#39;, &#39;skip&#39;, &#39;skull&#39;, &#39;sky&#39;, &#39;slap&#39;, &#39;slapstick&#39;, &#39;slasher&#39;, &#39;slaughter&#39;, &#39;slave&#39;, &#39;sleazy&#39;, &#39;sleep&#39;, &#39;sleeping&#39;, &#39;slice&#39;, &#39;slick&#39;, &#39;slight&#39;, &#39;slightest&#39;, &#39;slightly&#39;, &#39;sloppy&#39;, &#39;slow&#39;, &#39;slowly&#39;, &#39;small&#39;, &#39;smaller&#39;, &#39;smart&#39;, &#39;smile&#39;, &#39;smiling&#39;, &#39;smith&#39;, &#39;smoke&#39;, &#39;smoking&#39;, &#39;smooth&#39;, &#39;snake&#39;, &#39;sneak&#39;, &#39;snl&#39;, &#39;snow&#39;, &#39;soap&#39;, &#39;soccer&#39;, &#39;social&#39;, &#39;society&#39;, &#39;soderbergh&#39;, &#39;soft&#39;, &#39;sold&#39;, &#39;soldier&#39;, &#39;soldiers&#39;, &#39;sole&#39;, &#39;solely&#39;, &#39;solid&#39;, &#39;solo&#39;, &#39;solution&#39;, &#39;solve&#39;, &#39;somebody&#39;, &#39;somehow&#39;, &#39;someone&#39;, &#39;something&#39;, &#39;sometimes&#39;, &#39;somewhat&#39;, &#39;somewhere&#39;, &#39;son&#39;, &#39;song&#39;, &#39;songs&#39;, &#39;sons&#39;, &#39;soon&#39;, &#39;sophisticated&#39;, &#39;sopranos&#39;, &#39;sorry&#39;, &#39;sort&#39;, &#39;sorts&#39;, &#39;soul&#39;, &#39;souls&#39;, &#39;sound&#39;, &#39;sounded&#39;, &#39;sounding&#39;, &#39;sounds&#39;, &#39;soundtrack&#39;, &#39;source&#39;, &#39;south&#39;, &#39;southern&#39;, &#39;soviet&#39;, &#39;space&#39;, &#39;spacey&#39;, &#39;spain&#39;, &#39;spanish&#39;, &#39;spare&#39;, &#39;spark&#39;, &#39;speak&#39;, &#39;speaking&#39;, &#39;speaks&#39;, &#39;special&#39;, &#39;specially&#39;, &#39;species&#39;, &#39;specific&#39;, &#39;specifically&#39;, &#39;spectacular&#39;, &#39;speech&#39;, &#39;speed&#39;, &#39;spell&#39;, &#39;spend&#39;, &#39;spending&#39;, &#39;spends&#39;, &#39;spent&#39;, &#39;spider&#39;, &#39;spielberg&#39;, &#39;spike&#39;, &#39;spin&#39;, &#39;spirit&#39;, &#39;spirited&#39;, &#39;spirits&#39;, &#39;spiritual&#39;, &#39;spite&#39;, &#39;splatter&#39;, &#39;splendid&#39;, &#39;split&#39;, &#39;spock&#39;, &#39;spoil&#39;, &#39;spoiled&#39;, &#39;spoiler&#39;, &#39;spoilers&#39;, &#39;spoke&#39;, &#39;spoken&#39;, &#39;spoof&#39;, &#39;spooky&#39;, &#39;sport&#39;, &#39;sports&#39;, &#39;spot&#39;, &#39;spots&#39;, &#39;spread&#39;, &#39;spring&#39;, &#39;spy&#39;, &#39;square&#39;, &#39;st&#39;, &#39;stack&#39;, &#39;staff&#39;, &#39;stage&#39;, &#39;staged&#39;, &#39;stale&#39;, &#39;stallone&#39;, &#39;stan&#39;, &#39;stand&#39;, &#39;standard&#39;, &#39;standards&#39;, &#39;standing&#39;, &#39;stands&#39;, &#39;stanley&#39;, &#39;stanwyck&#39;, &#39;star&#39;, &#39;stargate&#39;, &#39;staring&#39;, &#39;starred&#39;, &#39;starring&#39;, &#39;stars&#39;, &#39;start&#39;, &#39;started&#39;, &#39;starting&#39;, &#39;starts&#39;, &#39;state&#39;, &#39;stated&#39;, &#39;statement&#39;, &#39;states&#39;, &#39;station&#39;, &#39;status&#39;, &#39;stay&#39;, &#39;stayed&#39;, &#39;staying&#39;, &#39;stays&#39;, &#39;steal&#39;, &#39;stealing&#39;, &#39;steals&#39;, &#39;steel&#39;, &#39;stellar&#39;, &#39;step&#39;, &#39;stephen&#39;, &#39;steps&#39;, &#39;stereotype&#39;, &#39;stereotypes&#39;, &#39;stereotypical&#39;, &#39;steve&#39;, &#39;steven&#39;, &#39;stevens&#39;, &#39;stewart&#39;, &#39;stick&#39;, &#39;sticks&#39;, &#39;stiff&#39;, &#39;still&#39;, &#39;stiller&#39;, &#39;stilted&#39;, &#39;stinker&#39;, &#39;stinks&#39;, &#39;stock&#39;, &#39;stole&#39;, &#39;stolen&#39;, &#39;stomach&#39;, &#39;stone&#39;, &#39;stood&#39;, &#39;stooges&#39;, &#39;stop&#39;, &#39;stopped&#39;, &#39;stops&#39;, &#39;store&#39;, &#39;stories&#39;, &#39;storm&#39;, &#39;story&#39;, &#39;storyline&#39;, &#39;storytelling&#39;, &#39;straight&#39;, &#39;strange&#39;, &#39;strangely&#39;, &#39;stranger&#39;, &#39;strangers&#39;, &#39;streep&#39;, &#39;street&#39;, &#39;streets&#39;, &#39;streisand&#39;, &#39;strength&#39;, &#39;stress&#39;, &#39;stretch&#39;, &#39;stretched&#39;, &#39;strictly&#39;, &#39;strike&#39;, &#39;strikes&#39;, &#39;striking&#39;, &#39;string&#39;, &#39;strip&#39;, &#39;strong&#39;, &#39;stronger&#39;, &#39;strongly&#39;, &#39;struck&#39;, &#39;structure&#39;, &#39;struggle&#39;, &#39;struggles&#39;, &#39;struggling&#39;, &#39;stuart&#39;, &#39;stuck&#39;, &#39;student&#39;, &#39;students&#39;, &#39;studio&#39;, &#39;studios&#39;, &#39;study&#39;, &#39;stuff&#39;, &#39;stumbled&#39;, &#39;stunned&#39;, &#39;stunning&#39;, &#39;stunt&#39;, &#39;stunts&#39;, &#39;stupid&#39;, &#39;stupidity&#39;, &#39;style&#39;, &#39;styles&#39;, &#39;stylish&#39;, &#39;sub&#39;, &#39;subject&#39;, &#39;subjected&#39;, &#39;subjects&#39;, &#39;subplot&#39;, &#39;subplots&#39;, &#39;subsequent&#39;, &#39;substance&#39;, &#39;subtitles&#39;, &#39;subtle&#39;, &#39;subtlety&#39;, &#39;succeed&#39;, &#39;succeeded&#39;, &#39;succeeds&#39;, &#39;success&#39;, &#39;successful&#39;, &#39;successfully&#39;, &#39;suck&#39;, &#39;sucked&#39;, &#39;sucks&#39;, &#39;sudden&#39;, &#39;suddenly&#39;, &#39;sue&#39;, &#39;suffer&#39;, &#39;suffered&#39;, &#39;suffering&#39;, &#39;suffers&#39;, &#39;suffice&#39;, &#39;suggest&#39;, &#39;suggested&#39;, &#39;suggests&#39;, &#39;suicide&#39;, &#39;suit&#39;, &#39;suitable&#39;, &#39;suited&#39;, &#39;suits&#39;, &#39;sullivan&#39;, &#39;sum&#39;, &#39;summary&#39;, &#39;summer&#39;, &#39;sun&#39;, &#39;sunday&#39;, &#39;sunshine&#39;, &#39;super&#39;, &#39;superb&#39;, &#39;superbly&#39;, &#39;superficial&#39;, &#39;superhero&#39;, &#39;superior&#39;, &#39;superman&#39;, &#39;supernatural&#39;, &#39;support&#39;, &#39;supported&#39;, &#39;supporting&#39;, &#39;suppose&#39;, &#39;supposed&#39;, &#39;supposedly&#39;, &#39;sure&#39;, &#39;surely&#39;, &#39;surface&#39;, &#39;surfing&#39;, &#39;surprise&#39;, &#39;surprised&#39;, &#39;surprises&#39;, &#39;surprising&#39;, &#39;surprisingly&#39;, &#39;surreal&#39;, &#39;surrounded&#39;, &#39;surrounding&#39;, &#39;survival&#39;, &#39;survive&#39;, &#39;survived&#39;, &#39;surviving&#39;, &#39;survivor&#39;, &#39;survivors&#39;, &#39;susan&#39;, &#39;suspect&#39;, &#39;suspects&#39;, &#39;suspend&#39;, &#39;suspense&#39;, &#39;suspenseful&#39;, &#39;suspicious&#39;, &#39;sutherland&#39;, &#39;swear&#39;, &#39;swedish&#39;, &#39;sweet&#39;, &#39;swim&#39;, &#39;swimming&#39;, &#39;switch&#39;, &#39;sword&#39;, &#39;symbolism&#39;, &#39;sympathetic&#39;, &#39;sympathy&#39;, &#39;synopsis&#39;, &#39;system&#39;, &#39;table&#39;, &#39;tacky&#39;, &#39;tad&#39;, &#39;tag&#39;, &#39;take&#39;, &#39;taken&#39;, &#39;takes&#39;, &#39;taking&#39;, &#39;tale&#39;, &#39;talent&#39;, &#39;talented&#39;, &#39;talents&#39;, &#39;tales&#39;, &#39;talk&#39;, &#39;talked&#39;, &#39;talking&#39;, &#39;talks&#39;, &#39;tall&#39;, &#39;tame&#39;, &#39;tank&#39;, &#39;tap&#39;, &#39;tape&#39;, &#39;tarantino&#39;, &#39;target&#39;, &#39;tarzan&#39;, &#39;task&#39;, &#39;taste&#39;, &#39;taught&#39;, &#39;taxi&#39;, &#39;taylor&#39;, &#39;tea&#39;, &#39;teach&#39;, &#39;teacher&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;tear&#39;, &#39;tears&#39;, &#39;tech&#39;, &#39;technical&#39;, &#39;technically&#39;, &#39;technicolor&#39;, &#39;technique&#39;, &#39;techniques&#39;, &#39;technology&#39;, &#39;ted&#39;, &#39;tedious&#39;, &#39;teen&#39;, &#39;teenage&#39;, &#39;teenager&#39;, &#39;teenagers&#39;, &#39;teens&#39;, &#39;teeth&#39;, &#39;television&#39;, &#39;tell&#39;, &#39;telling&#39;, &#39;tells&#39;, &#39;temple&#39;, &#39;ten&#39;, &#39;tend&#39;, &#39;tender&#39;, &#39;tends&#39;, &#39;tense&#39;, &#39;tension&#39;, &#39;term&#39;, &#39;terms&#39;, &#39;terrible&#39;, &#39;terribly&#39;, &#39;terrific&#39;, &#39;terrifying&#39;, &#39;territory&#39;, &#39;terror&#39;, &#39;terrorist&#39;, &#39;terrorists&#39;, &#39;terry&#39;, &#39;test&#39;, &#39;testament&#39;, &#39;texas&#39;, &#39;text&#39;, &#39;th&#39;, &#39;thank&#39;, &#39;thankfully&#39;, &#39;thanks&#39;, &#39;thats&#39;, &#39;theater&#39;, &#39;theaters&#39;, &#39;theatre&#39;, &#39;theatrical&#39;, &#39;theme&#39;, &#39;themes&#39;, &#39;theory&#39;, &#39;therefore&#39;, &#39;thick&#39;, &#39;thief&#39;, &#39;thin&#39;, &#39;thing&#39;, &#39;things&#39;, &#39;think&#39;, &#39;thinking&#39;, &#39;thinks&#39;, &#39;third&#39;, &#39;thirty&#39;, &#39;thomas&#39;, &#39;thompson&#39;, &#39;thoroughly&#39;, &#39;though&#39;, &#39;thought&#39;, &#39;thoughtful&#39;, &#39;thoughts&#39;, &#39;thousand&#39;, &#39;thousands&#39;, &#39;threat&#39;, &#39;threatening&#39;, &#39;three&#39;, &#39;threw&#39;, &#39;thrill&#39;, &#39;thriller&#39;, &#39;thrillers&#39;, &#39;thrilling&#39;, &#39;thrills&#39;, &#39;throat&#39;, &#39;throughout&#39;, &#39;throw&#39;, &#39;throwing&#39;, &#39;thrown&#39;, &#39;throws&#39;, &#39;thru&#39;, &#39;thugs&#39;, &#39;thumbs&#39;, &#39;thus&#39;, &#39;ticket&#39;, &#39;tie&#39;, &#39;tied&#39;, &#39;ties&#39;, &#39;tiger&#39;, &#39;tight&#39;, &#39;till&#39;, &#39;tim&#39;, &#39;time&#39;, &#39;timeless&#39;, &#39;times&#39;, &#39;timing&#39;, &#39;timon&#39;, &#39;timothy&#39;, &#39;tiny&#39;, &#39;tired&#39;, &#39;tiresome&#39;, &#39;titanic&#39;, &#39;title&#39;, &#39;titled&#39;, &#39;titles&#39;, &#39;today&#39;, &#39;todd&#39;, &#39;together&#39;, &#39;toilet&#39;, &#39;told&#39;, &#39;tom&#39;, &#39;tomatoes&#39;, &#39;tommy&#39;, &#39;tone&#39;, &#39;tongue&#39;, &#39;tonight&#39;, &#39;tons&#39;, &#39;tony&#39;, &#39;took&#39;, &#39;tooth&#39;, &#39;top&#39;, &#39;topic&#39;, &#39;topless&#39;, &#39;torn&#39;, &#39;torture&#39;, &#39;tortured&#39;, &#39;total&#39;, &#39;totally&#39;, &#39;touch&#39;, &#39;touched&#39;, &#39;touches&#39;, &#39;touching&#39;, &#39;tough&#39;, &#39;tour&#39;, &#39;toward&#39;, &#39;towards&#39;, &#39;town&#39;, &#39;toy&#39;, &#39;toys&#39;, &#39;track&#39;, &#39;tracks&#39;, &#39;tracy&#39;, &#39;trade&#39;, &#39;trademark&#39;, &#39;tradition&#39;, &#39;traditional&#39;, &#39;tragedy&#39;, &#39;tragic&#39;, &#39;trail&#39;, &#39;trailer&#39;, &#39;trailers&#39;, &#39;train&#39;, &#39;trained&#39;, &#39;training&#39;, &#39;transfer&#39;, &#39;transformation&#39;, &#39;transition&#39;, &#39;translation&#39;, &#39;trap&#39;, &#39;trapped&#39;, &#39;trash&#39;, &#39;trashy&#39;, &#39;travel&#39;, &#39;traveling&#39;, &#39;travels&#39;, &#39;travesty&#39;, &#39;treasure&#39;, &#39;treat&#39;, &#39;treated&#39;, &#39;treatment&#39;, &#39;treats&#39;, &#39;tree&#39;, &#39;trees&#39;, &#39;trek&#39;, &#39;tremendous&#39;, &#39;trial&#39;, &#39;tribe&#39;, &#39;tribute&#39;, &#39;trick&#39;, &#39;tricks&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;trilogy&#39;, &#39;trio&#39;, &#39;trip&#39;, &#39;trite&#39;, &#39;triumph&#39;, &#39;troops&#39;, &#39;trouble&#39;, &#39;troubled&#39;, &#39;troubles&#39;, &#39;truck&#39;, &#39;true&#39;, &#39;truly&#39;, &#39;trust&#39;, &#39;truth&#39;, &#39;try&#39;, &#39;trying&#39;, &#39;tube&#39;, &#39;tune&#39;, &#39;tunes&#39;, &#39;turkey&#39;, &#39;turn&#39;, &#39;turned&#39;, &#39;turner&#39;, &#39;turning&#39;, &#39;turns&#39;, &#39;tv&#39;, &#39;twelve&#39;, &#39;twenty&#39;, &#39;twice&#39;, &#39;twilight&#39;, &#39;twin&#39;, &#39;twins&#39;, &#39;twist&#39;, &#39;twisted&#39;, &#39;twists&#39;, &#39;two&#39;, &#39;type&#39;, &#39;types&#39;, &#39;typical&#39;, &#39;typically&#39;, &#39;ugly&#39;, &#39;uk&#39;, &#39;ultimate&#39;, &#39;ultimately&#39;, &#39;ultra&#39;, &#39;un&#39;, &#39;unable&#39;, &#39;unaware&#39;, &#39;unbearable&#39;, &#39;unbelievable&#39;, &#39;unbelievably&#39;, &#39;uncle&#39;, &#39;uncomfortable&#39;, &#39;unconvincing&#39;, &#39;underground&#39;, &#39;underlying&#39;, &#39;underrated&#39;, &#39;understand&#39;, &#39;understandable&#39;, &#39;understanding&#39;, &#39;understated&#39;, &#39;understood&#39;, &#39;undoubtedly&#39;, &#39;uneven&#39;, &#39;unexpected&#39;, &#39;unexpectedly&#39;, &#39;unfair&#39;, &#39;unfolds&#39;, &#39;unforgettable&#39;, &#39;unfortunate&#39;, &#39;unfortunately&#39;, &#39;unfunny&#39;, &#39;unhappy&#39;, &#39;uninspired&#39;, &#39;unintentional&#39;, &#39;unintentionally&#39;, &#39;uninteresting&#39;, &#39;union&#39;, &#39;unique&#39;, &#39;unit&#39;, &#39;united&#39;, &#39;universal&#39;, &#39;universe&#39;, &#39;university&#39;, &#39;unknown&#39;, &#39;unless&#39;, &#39;unlike&#39;, &#39;unlikely&#39;, &#39;unnecessary&#39;, &#39;unoriginal&#39;, &#39;unpleasant&#39;, &#39;unpredictable&#39;, &#39;unreal&#39;, &#39;unrealistic&#39;, &#39;unseen&#39;, &#39;unsettling&#39;, &#39;unusual&#39;, &#39;unwatchable&#39;, &#39;uplifting&#39;, &#39;upon&#39;, &#39;upper&#39;, &#39;ups&#39;, &#39;upset&#39;, &#39;urban&#39;, &#39;urge&#39;, &#39;us&#39;, &#39;usa&#39;, &#39;use&#39;, &#39;used&#39;, &#39;useful&#39;, &#39;useless&#39;, &#39;user&#39;, &#39;uses&#39;, &#39;using&#39;, &#39;ustinov&#39;, &#39;usual&#39;, &#39;usually&#39;, &#39;utter&#39;, &#39;utterly&#39;, &#39;uwe&#39;, &#39;vacation&#39;, &#39;vague&#39;, &#39;vaguely&#39;, &#39;valentine&#39;, &#39;valley&#39;, &#39;valuable&#39;, &#39;value&#39;, &#39;values&#39;, &#39;vampire&#39;, &#39;vampires&#39;, &#39;van&#39;, &#39;vance&#39;, &#39;variety&#39;, &#39;various&#39;, &#39;vast&#39;, &#39;vegas&#39;, &#39;vehicle&#39;, &#39;vengeance&#39;, &#39;verhoeven&#39;, &#39;version&#39;, &#39;versions&#39;, &#39;versus&#39;, &#39;veteran&#39;, &#39;vhs&#39;, &#39;via&#39;, &#39;vice&#39;, &#39;vicious&#39;, &#39;victim&#39;, &#39;victims&#39;, &#39;victor&#39;, &#39;victoria&#39;, &#39;victory&#39;, &#39;video&#39;, &#39;videos&#39;, &#39;vietnam&#39;, &#39;view&#39;, &#39;viewed&#39;, &#39;viewer&#39;, &#39;viewers&#39;, &#39;viewing&#39;, &#39;viewings&#39;, &#39;views&#39;, &#39;village&#39;, &#39;villain&#39;, &#39;villains&#39;, &#39;vincent&#39;, &#39;violence&#39;, &#39;violent&#39;, &#39;virgin&#39;, &#39;virginia&#39;, &#39;virtually&#39;, &#39;virus&#39;, &#39;visible&#39;, &#39;vision&#39;, &#39;visit&#39;, &#39;visits&#39;, &#39;visual&#39;, &#39;visually&#39;, &#39;visuals&#39;, &#39;vivid&#39;, &#39;voice&#39;, &#39;voiced&#39;, &#39;voices&#39;, &#39;voight&#39;, &#39;von&#39;, &#39;vote&#39;, &#39;vs&#39;, &#39;vulnerable&#39;, &#39;wacky&#39;, &#39;wait&#39;, &#39;waited&#39;, &#39;waiting&#39;, &#39;waitress&#39;, &#39;wake&#39;, &#39;walk&#39;, &#39;walked&#39;, &#39;walken&#39;, &#39;walker&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;wall&#39;, &#39;wallace&#39;, &#39;walls&#39;, &#39;walsh&#39;, &#39;walter&#39;, &#39;wandering&#39;, &#39;wang&#39;, &#39;wanna&#39;, &#39;wannabe&#39;, &#39;want&#39;, &#39;wanted&#39;, &#39;wanting&#39;, &#39;wants&#39;, &#39;war&#39;, &#39;ward&#39;, &#39;warm&#39;, &#39;warming&#39;, &#39;warmth&#39;, &#39;warn&#39;, &#39;warned&#39;, &#39;warner&#39;, &#39;warning&#39;, &#39;warren&#39;, &#39;warrior&#39;, &#39;warriors&#39;, &#39;wars&#39;, &#39;washington&#39;, &#39;waste&#39;, &#39;wasted&#39;, &#39;wasting&#39;, &#39;watch&#39;, &#39;watchable&#39;, &#39;watched&#39;, &#39;watches&#39;, &#39;watching&#39;, &#39;water&#39;, &#39;waters&#39;, &#39;watson&#39;, &#39;wave&#39;, &#39;waves&#39;, &#39;way&#39;, &#39;wayne&#39;, &#39;ways&#39;, &#39;weak&#39;, &#39;weakest&#39;, &#39;wealth&#39;, &#39;wealthy&#39;, &#39;weapon&#39;, &#39;weapons&#39;, &#39;wear&#39;, &#39;wearing&#39;, &#39;wears&#39;, &#39;web&#39;, &#39;website&#39;, &#39;wedding&#39;, &#39;week&#39;, &#39;weekend&#39;, &#39;weeks&#39;, &#39;weight&#39;, &#39;weird&#39;, &#39;welcome&#39;, &#39;well&#39;, &#39;welles&#39;, &#39;wells&#39;, &#39;wendy&#39;, &#39;went&#39;, &#39;werewolf&#39;, &#39;wes&#39;, &#39;west&#39;, &#39;western&#39;, &#39;westerns&#39;, &#39;wet&#39;, &#39;whale&#39;, &#39;whatever&#39;, &#39;whats&#39;, &#39;whatsoever&#39;, &#39;whenever&#39;, &#39;whereas&#39;, &#39;whether&#39;, &#39;whilst&#39;, &#39;white&#39;, &#39;whoever&#39;, &#39;whole&#39;, &#39;wholly&#39;, &#39;whoopi&#39;, &#39;whose&#39;, &#39;wicked&#39;, &#39;wide&#39;, &#39;widely&#39;, &#39;widmark&#39;, &#39;widow&#39;, &#39;wife&#39;, &#39;wild&#39;, &#39;wilderness&#39;, &#39;william&#39;, &#39;williams&#39;, &#39;willie&#39;, &#39;willing&#39;, &#39;willis&#39;, &#39;wilson&#39;, &#39;win&#39;, &#39;wind&#39;, &#39;window&#39;, &#39;winds&#39;, &#39;wing&#39;, &#39;winner&#39;, &#39;winning&#39;, &#39;wins&#39;, &#39;winter&#39;, &#39;winters&#39;, &#39;wisdom&#39;, &#39;wise&#39;, &#39;wish&#39;, &#39;wished&#39;, &#39;wishes&#39;, &#39;wishing&#39;, &#39;wit&#39;, &#39;witch&#39;, &#39;witches&#39;, &#39;within&#39;, &#39;without&#39;, &#39;witness&#39;, &#39;witnessed&#39;, &#39;witnesses&#39;, &#39;witty&#39;, &#39;wives&#39;, &#39;wizard&#39;, &#39;wolf&#39;, &#39;woman&#39;, &#39;women&#39;, &#39;wonder&#39;, &#39;wondered&#39;, &#39;wonderful&#39;, &#39;wonderfully&#39;, &#39;wondering&#39;, &#39;wonders&#39;, &#39;wont&#39;, &#39;woo&#39;, &#39;wood&#39;, &#39;wooden&#39;, &#39;woods&#39;, &#39;woody&#39;, &#39;word&#39;, &#39;words&#39;, &#39;wore&#39;, &#39;work&#39;, &#39;worked&#39;, &#39;worker&#39;, &#39;workers&#39;, &#39;working&#39;, &#39;works&#39;, &#39;world&#39;, &#39;worlds&#39;, &#39;worn&#39;, &#39;worried&#39;, &#39;worry&#39;, &#39;worse&#39;, &#39;worst&#39;, &#39;worth&#39;, &#39;worthless&#39;, &#39;worthwhile&#39;, &#39;worthy&#39;, &#39;would&#39;, &#39;wound&#39;, &#39;wounded&#39;, &#39;wow&#39;, &#39;wrap&#39;, &#39;wrapped&#39;, &#39;wreck&#39;, &#39;wrestling&#39;, &#39;wretched&#39;, &#39;write&#39;, &#39;writer&#39;, &#39;writers&#39;, &#39;writes&#39;, &#39;writing&#39;, &#39;written&#39;, &#39;wrong&#39;, &#39;wrote&#39;, &#39;ww&#39;, &#39;wwii&#39;, &#39;ya&#39;, &#39;yard&#39;, &#39;yeah&#39;, &#39;year&#39;, &#39;years&#39;, &#39;yelling&#39;, &#39;yellow&#39;, &#39;yes&#39;, &#39;yesterday&#39;, &#39;yet&#39;, &#39;york&#39;, &#39;young&#39;, &#39;younger&#39;, &#39;youth&#39;, &#39;zero&#39;, &#39;zizek&#39;, &#39;zombie&#39;, &#39;zombies&#39;, &#39;zone&#39;]</code></pre><h2 id="3-建模并测试"><a href="#3-建模并测试" class="headerlink" title="3 建模并测试"></a>3 建模并测试</h2><h3 id="3-1-建立模型"><a href="#3-1-建立模型" class="headerlink" title="3.1 建立模型"></a>3.1 建立模型</h3><p>这里选择随机森林模型，参数介绍以及调参方法见：Sklearn-RandomForest随机森林<a href="https://blog.csdn.net/cherdw/article/details/54971771" target="_blank" rel="noopener">https://blog.csdn.net/cherdw/article/details/54971771</a></p><pre class=" language-lang-python"><code class="language-lang-python">print("Training the random forest...")from sklearn.ensemble import RandomForestClassifierforest=RandomForestClassifier(n_estimators=100)forest=forest.fit(train_data_features,train["sentiment"])</code></pre><pre><code>Training the random forest...</code></pre><h3 id="3-2-模型测试"><a href="#3-2-模型测试" class="headerlink" title="3.2 模型测试"></a>3.2 模型测试</h3><p>(1)模型测试之前，需要准备测试数据，以及提取测试数据的特征</p><pre class=" language-lang-python"><code class="language-lang-python"># 读取测试数据test=pd.read_csv("./data/testData.tsv", header=0, delimiter='\t',quoting=3)print(test.shape)# test.head()</code></pre><pre><code>(25000, 2)</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 对测试数据进行清洗num_reviews=len(test['review'])clean_test_reviews=[]print("cleaning and parsing the test set movie reviews ..\n")for i in range(0,num_reviews):    if((i+1)%1000==0):        print("Review %d of %d \n"%(i+1,num_reviews))    clean_review=review_to_words(test["review"][i])    clean_test_reviews.append(clean_review)</code></pre><pre><code>cleaning and parsing the test set movie reviews ../home/xiongzy/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I&#39;m using the best available HTML parser for this system (&quot;lxml&quot;). This usually isn&#39;t a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.The code that caused this warning is on line 193 of the file /home/xiongzy/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this: BeautifulSoup(YOUR_MARKUP})to this: BeautifulSoup(YOUR_MARKUP, &quot;lxml&quot;)  markup_type=markup_type))Review 1000 of 25000 Review 2000 of 25000 Review 3000 of 25000 Review 4000 of 25000 Review 5000 of 25000 Review 6000 of 25000 Review 7000 of 25000 Review 8000 of 25000 Review 9000 of 25000 Review 10000 of 25000 Review 11000 of 25000 Review 12000 of 25000 Review 13000 of 25000 Review 14000 of 25000 Review 15000 of 25000 Review 16000 of 25000 Review 17000 of 25000 Review 18000 of 25000 Review 19000 of 25000 Review 20000 of 25000 Review 21000 of 25000 Review 22000 of 25000 Review 23000 of 25000 Review 24000 of 25000 Review 25000 of 25000 </code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 提取测试集数据的特征test_data_features=vectorizer.transform(clean_test_reviews)test_data_features=test_data_features.toarray()</code></pre><p>(2)预测</p><pre class=" language-lang-python"><code class="language-lang-python">result=forest.predict(test_data_features)</code></pre><p>(3)整理输出数据格式</p><pre class=" language-lang-python"><code class="language-lang-python">output=pd.DataFrame(data={"id":test["id"],"sentiment":result})output.to_csv("Bag_of_Words_modle.csv",index=False,quoting=3)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种利用web服务实现模型常驻内存的简单方法</title>
      <link href="/2018/06/09/%E4%B8%80%E7%A7%8D%E5%88%A9%E7%94%A8web%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%B8%B8%E9%A9%BB%E5%86%85%E5%AD%98%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95(1)(1)/"/>
      <url>/2018/06/09/%E4%B8%80%E7%A7%8D%E5%88%A9%E7%94%A8web%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%B8%B8%E9%A9%BB%E5%86%85%E5%AD%98%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95(1)(1)/</url>
      
        <content type="html"><![CDATA[<p>利用python的web服务快速实现模型常驻内存，本方法很low，但是真的很快速，半个小时都能实现。<a id="more"></a></p><hr><h3 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h3><p>前段时间有这样一个需求，通过php接收微信服务器发送来的消息，然后把消息发送给python功能逻辑处理程序。在前期是直接使用php调用python程序，python程序进行模型加载，消息处理，然后返回给php。利用这种流程，处理过程不慢都难。但是还必须得按照这样的流程走，于是想想能不能将之前训练好的模型常驻内存。查阅网上，解决方法有很多，可以利用socket通信、做成客户/服务器模式等，这些方法都非常好，但是实现起来有一定的复杂。所以想到了这种方法。</p><h3 id="2、整体思路"><a href="#2、整体思路" class="headerlink" title="2、整体思路"></a>2、整体思路</h3><p>主要是用python以及其flask库来实现的。主要思路有以下几点：</p><blockquote><ul><li>将php调用python程序改为php通过get或post方法向python发送请求；</li><li>python程序改成web服务模式，运行的就加载模型，让它一直运行；</li><li>python接收请求，处理，并返回结果；</li></ul></blockquote><p>Flask是一个使用 Python 编写的轻量级 Web 应用框架，使用时，只需要在python中引入即可，因为需要处理get或者post请求，同时引入request，接下来就按照流程来编写代码即可。主要步骤如下：<br>(1)flask的简单实验</p><ul><li>引入包创建一个flask应用</li></ul><pre class=" language-lang-Python"><code class="language-lang-Python">from flask import Flask,requestapp = Flask(__name__)</code></pre><ul><li>定义一个方法</li></ul><pre class=" language-lang-Python"><code class="language-lang-Python">def hello():        print("hello world")</code></pre><ul><li>运行应用</li></ul><pre class=" language-lang-Python"><code class="language-lang-Python">app.debug = Trueapp.run()</code></pre><p>完整代码：</p><pre class=" language-lang-Python"><code class="language-lang-Python">from flask import Flask,requestapp = Flask(__name__)# 加载模型# 处理请求@app.route('/hello')def hello():    return("hello world")if __name__ == '__main__':    app.debug = True    app.run()</code></pre><p>此时运行该程序，在浏览器地址栏输入：127.0.0.1:5000/hello，此时浏览器中就会显示hello world，好了，那么接下来的事情就简单了。只需要把上面加载模型和处理请求部分的代码稍作修改就可以了。</p><ul><li>加载模型</li></ul><pre class=" language-lang-Python"><code class="language-lang-Python">model= load_model_to_memory()  # 原来加载模型的那些代码</code></pre><ul><li>接收请求并处理</li></ul><pre class=" language-lang-Python"><code class="language-lang-Python"># 处理请求@app.route('/deal', methods=['GET'])def deal():        # 获取php发来的消息        question = request.args.get('question',"default question")   # 键值 默认值        # 对消息进行解码        question=urllib.parse.unquote(question)        result = main_function(model)  # 该函数就是原来的主要功能逻辑处理函数        # 返回处理结果        return (urllib.parse.quote(result))</code></pre><p>（2）php与python之间通信<br>上面的程序一直运行着，于是可以通过该url地址向python程序发送请求。在php中构造get或者post请求的方法可以见另一篇文章<a href="http://bei.dreamcykj.com/2018/06/08/%E5%88%A9%E7%94%A8php%E7%9A%84curl%E5%AE%9E%E7%8E%B0post%E5%92%8Cget%E8%AF%B7%E6%B1%82%281%29/%22%E5%88%A9%E7%94%A8php%E7%9A%84curl%E5%AE%9E%E7%8E%B0post%E5%92%8Cget%E8%AF%B7%E6%B1%82%22">利用php的curl实现post和get请求</a> ，这样就实现了php与python之间的通信，如果是在linux中，可以创建一个tmux会话窗口来运行上面的那个python程序，一直运行下去。</p><h3 id="3、完整代码"><a href="#3、完整代码" class="headerlink" title="3、完整代码"></a>3、完整代码</h3><ul><li>PHP</li></ul><pre class=" language-lang-PHP"><code class="language-lang-PHP">// 通过get方式将问题传给python的web服务    public static function send_question_to_python_get($request)    {        //本地接收问题的python服务的url        $base_url="http://127.0.0.1:5000/login?question=";        // 获取问题，并将其编码        $question=$request['content'];        $en_question=urlencode($question);        $url=$base_url.$en_question;        $ch = curl_init();        //设置选项，包括URL        curl_setopt($ch, CURLOPT_URL, $url);        curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);        curl_setopt($ch, CURLOPT_HEADER, 0);        curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);//绕过ssl验证        curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, false);        //执行并获取HTML文档内容        $output = curl_exec($ch);        $output=urldecode($output);        //释放curl句柄        curl_close($ch);        return $output;    }</code></pre><ul><li>Python</li></ul><pre class=" language-lang-Python"><code class="language-lang-Python">from flask import Flask,url_for,requestimport urllib.parseapp = Flask(__name__)# 将模型加载到内存model = load_model_to_memory()# 处理请求@app.route('/deal', methods=['GET'])def deal():        # 获取php发来的消息        question = request.args.get('question',"default question")   # 键值 默认值        # 对消息进行解码        question=urllib.parse.unquote(question)        result = main_function(model)  # 该函数就是原来的主要功能逻辑处理函数        # 返回处理结果        return (urllib.parse.quote(result))if __name__ == '__main__':    app.debug = True    app.run()</code></pre><h3 id="4、相关链接"><a href="#4、相关链接" class="headerlink" title="4、相关链接"></a>4、相关链接</h3><p>【1】<a href="https://blog.csdn.net/u011054333/article/details/70151857" target="_blank" rel="noopener">Flask快速入门</a><br>【2】<a href="http://bei.dreamcykj.com/2018/06/08/%E5%88%A9%E7%94%A8php%E7%9A%84curl%E5%AE%9E%E7%8E%B0post%E5%92%8Cget%E8%AF%B7%E6%B1%82%281%29/">利用php的curl实现post和get请求</a></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
            <tag> web服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用php的curl实现post和get请求</title>
      <link href="/2018/06/08/%E5%88%A9%E7%94%A8php%E7%9A%84curl%E5%AE%9E%E7%8E%B0post%E5%92%8Cget%E8%AF%B7%E6%B1%82(1)/"/>
      <url>/2018/06/08/%E5%88%A9%E7%94%A8php%E7%9A%84curl%E5%AE%9E%E7%8E%B0post%E5%92%8Cget%E8%AF%B7%E6%B1%82(1)/</url>
      
        <content type="html"><![CDATA[<p>由于项目需求，进行了一段时间的微信开发。在微信开发的过程中，经常调用微信接口，通常是向微信服务器发送get或者post请求获取接口。下面给出两个具体的实现过程。<br><a id="more"></a></p><h4 id="1、发送post请求"><a href="#1、发送post请求" class="headerlink" title="1、发送post请求"></a>1、发送post请求</h4><pre><code>function curl_post($url,$data){     $curl = curl_init();     curl_setopt($curl, CURLOPT_URL, $url);     curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, 0);     curl_setopt($curl, CURLOPT_SSL_VERIFYHOST, 2);     curl_setopt($curl, CURLOPT_USERAGENT, $_SERVER[&#39;HTTP_USER_AGENT&#39;]);     curl_setopt($curl, CURLOPT_FOLLOWLOCATION, 1);     curl_setopt($curl, CURLOPT_AUTOREFERER, 1);     curl_setopt($curl, CURLOPT_POST, 1);     curl_setopt($curl, CURLOPT_POSTFIELDS, $data);     curl_setopt($curl, CURLOPT_TIMEOUT, 30);     curl_setopt($curl, CURLOPT_HEADER, 0);     curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);     $tmpInfo = curl_exec($curl);     if (curl_errno($curl)) {       echo &#39;Errno&#39;.curl_error($curl);    }    curl_close($curl);     return $tmpInfo; }</code></pre><p>其中$url是调用微信接口的url，$data是向微信服务器发送的数据，下同。</p><h4 id="2、发送get请求"><a href="#2、发送get请求" class="headerlink" title="2、发送get请求"></a>2、发送get请求</h4><pre><code>function http_post($url,$data)        {//            $data_string=json_encode(array(&#39;name&#39;=&gt;$name,&#39;data&#39;=&gt;$data));//            $url = &quot;http://192.168.1.40:8080/wechatdemo/&quot;;            $ch = curl_init($url);            curl_setopt($ch, CURLOPT_CUSTOMREQUEST, &quot;POST&quot;);            curl_setopt($ch, CURLOPT_POSTFIELDS, $data);//$data JSON类型字符串            curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);            curl_setopt($ch, CURLOPT_HTTPHEADER, array(&#39;Content-Type: application/json&#39;, &#39;Content-Length: &#39; . strlen($data)));            $result = curl_exec($ch);            var_dump($result);        }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
            <tag> PHP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【文本相似度】利用余弦相似性计算句子的相似度</title>
      <link href="/2018/03/23/%E3%80%90%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E3%80%91%E5%88%A9%E7%94%A8%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
      <url>/2018/03/23/%E3%80%90%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E3%80%91%E5%88%A9%E7%94%A8%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>好吧，在处理文本的过程中有陷入了一个胡同，不知道接下来该做啥了。于是又在网上找相关的资料，无意间发现了两篇写得超级超级好的博客，是讲文本相似度，之前也学过，但是这两篇博文写得非常棒，深入浅出。然后也按照博文进行了代码实现，在此记录下，也作为文本相似度相关文章的第一篇。<br><a id="more"></a></p><h3 id="1-功能分析"><a href="#1-功能分析" class="headerlink" title="1 功能分析"></a>1 功能分析</h3><p>通过词频和余弦相似性定理计算句子间的相似度，具体理论讲解见上面提到的两篇博文。<br>1、<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a><br>2、<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（二）：找出相似文章</a></p><h3 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>根据上面博文的讲解，写了一个简单的程序，代码如下，涉及到的余弦定理的计算和角度计算见参考资料。</p><pre><code>#-*- coding: UTF-8 -*-# @Time    : 2018/3/23 19:39# @Author  : beibei# @Site    : # @File    : 利用余弦相似性计算句子的相似度.py# @Software: PyCharmimport sysimport jiebaimport numpy as npdef compete_similar(sen1,sen2):    # 1分词    sege1=jieba.cut(sen1)    wordlist_1=[word for word in sege1]    sege2=jieba.cut(sen2)    wordlist_2=[word for word in sege2]    # 2列出所有的词、    word_dict=[]    for word in wordlist_1:        # 如果当前词没有加入词汇表，则将该词加入词汇表        if word not in word_dict:            word_dict.append(word)        else:            continue    for word in wordlist_2:        if word not in word_dict:            word_dict.append(word)        else:            continue    # 3计算词频、4写出词频向量    word_count_1={}    word_count_2={}    word_count_vec_1=[]    word_count_vec_2=[]    # 对于词汇表中的每一个词，统计他在每句话中出现的次数    for word in word_dict:        num1=sen1.count(word)        num2=sen2.count(word)        word_count_1[word]=num1        word_count_2[word]=num2        word_count_vec_1.append(num1)        word_count_vec_2.append(num2)    # 计算相似度    vec_1=np.array(word_count_vec_1)    vec_2=np.array(word_count_vec_2)    one_dot_two=np.dot(vec_1,vec_2)    L1=np.sqrt(np.dot(vec_1,vec_1))    L2=np.sqrt(np.dot(vec_2,vec_2))    # 余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫&quot;余弦相似性&quot;。    cos_angle=one_dot_two/(L1*L2)    angle_pi=np.arccos(cos_angle)    angle=angle_pi*360/2/np.pi    return angle# 提示参数问题def usage():    print (sys.argv[0], &quot; first sentence&quot;,&quot; second sentence&quot;)#程序的入口函数if __name__ == &#39;__main__&#39;:    arg_len = len(sys.argv)    # 判断参数是否正确！    if arg_len!=3:        usage()        sys.exit(-1)    # 获取两句话    sentence1=sys.argv[1]    sentence2=sys.argv[2]    # 计算相似性    result=compete_similar(sentence1,sentence2)    # 输出结果    print(result)</code></pre><p>运行结果为：</p><pre><code>Prefix dict has been built succesfully.19.1066053509</code></pre><h3 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3 参考资料"></a>3 参考资料</h3><p>1、<a href="https://jingyan.baidu.com/article/f54ae2fc2704a71e92b849e4.html" target="_blank" rel="noopener">python 线性代数：[14]计算向量夹角</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 文本相似度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>d2 机器学习和深度学习资料</title>
      <link href="/2018/01/24/dl2%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"/>
      <url>/2018/01/24/dl2%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<p>为了保存资料，于是上传至博客。<br><a id="more"></a></p><h2 id="机器学习-Machine-Learning-amp-深度学习-Deep-Learning-资料-Chapter-2"><a href="#机器学习-Machine-Learning-amp-深度学习-Deep-Learning-资料-Chapter-2" class="headerlink" title="机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Chapter 2)"></a>机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Chapter 2)</h2><h4 id="注-机器学习资料篇目一共500条-篇目二开始更新"><a href="#注-机器学习资料篇目一共500条-篇目二开始更新" class="headerlink" title="注:机器学习资料篇目一共500条,篇目二开始更新"></a>注:机器学习资料<a href="https://github.com/ty4z2008/Qix/blob/master/dl.md" target="_blank" rel="noopener">篇目一</a>共500条,<a href="https://github.com/ty4z2008/Qix/blob/master/dl2.md" target="_blank" rel="noopener">篇目二</a>开始更新</h4><h5 id="希望转载的朋友，你可以不用联系我．但是一定要保留原文链接，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外-某些资料在中国访问需要梯子"><a href="#希望转载的朋友，你可以不用联系我．但是一定要保留原文链接，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外-某些资料在中国访问需要梯子" class="headerlink" title="希望转载的朋友，你可以不用联系我．但是一定要保留原文链接，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外:某些资料在中国访问需要梯子."></a>希望转载的朋友，你可以不用联系我．但是<strong>一定要保留原文链接</strong>，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外:某些资料在中国访问需要梯子.</h5><ul><li><a href="http://engineering.flipboard.com/2015/05/scaling-convnets/" target="_blank" rel="noopener">《Image Scaling using Deep Convolutional Neural Networks》</a></li></ul><p>介绍:使用卷积神经网络的图像缩放.</p><ul><li><a href="http://jmlr.org/proceedings/papers/v37/" target="_blank" rel="noopener">《Proceedings of The 32nd International Conference on Machine Learning》</a></li></ul><p>介绍:ICML2015 论文集,优化4个+稀疏优化1个；强化学习4个，深度学习3个+深度学习计算1个；贝叶斯非参、高斯过程和学习理论3个；还有计算广告和社会选择.<a href="http://icml.cc/2015/?page_id=825" target="_blank" rel="noopener">ICML2015 Sessions</a>.</p><ul><li><a href="http://engineering.flipboard.com/2015/05/scaling-convnets/" target="_blank" rel="noopener">《Image Scaling using Deep Convolutional Neural Networks》</a></li></ul><p>介绍:使用卷积神经网络的图像缩放.</p><ul><li><a href="http://blogs.technet.com/b/inside_microsoft_research/archive/2015/06/08/microsoft-researchers-accelerate-computer-vision-accuracy-and-improve-3d-scanning-models.aspx" target="_blank" rel="noopener">《Microsoft researchers accelerate computer vision accuracy and improve 3D scanning models》</a></li></ul><p>介绍:，第28届IEEE计算机视觉与模式识别(CVPR)大会在美国波士顿举行。微软研究员们在大会上展示了比以往更快更准的计算机视觉图像分类新模型，并介绍了如何使用Kinect等传感器实现在动态或低光环境的快速大规模3D扫描技术.</p><ul><li><a href="https://github.com/marcotcr/mlforhumans" target="_blank" rel="noopener">《Machine Learning for Humans》</a></li></ul><p>介绍:(文本)机器学习可视化分析工具.</p><ul><li><a href="http://knowm.org/machine-learning-tools-an-overview/" target="_blank" rel="noopener">《A Plethora of Tools for Machine Learning》</a></li></ul><p>介绍:机器学习工具包/库的综述/比较.</p><ul><li><a href="http://sapblog.be/en/the-art-of-visualizing-visualizations-a-best-practice-guide/" target="_blank" rel="noopener">《The art of visualizing visualizations: a best practice guide》</a></li></ul><p>介绍:数据可视化最佳实践指南.</p><ul><li><a href="http://blog.adnanmasood.com/2015/06/08/mit-machine-learning-for-big-data-and-text-processing-class-notes-day-1/" target="_blank" rel="noopener">《MIT Machine Learning for Big Data and Text Processing Class Notes - Day 1》</a></li></ul><p>介绍:<a href="http://blog.adnanmasood.com/2015/06/08/mit-machine-learning-for-big-data-and-text-processing-class-notes-day-1/" target="_blank" rel="noopener">Day 1</a>、<a href="http://blog.adnanmasood.com/2015/06/09/mit-machine-learning-for-big-data-and-text-processing-class-notes-day-2/" target="_blank" rel="noopener">Day 2</a>、<a href="http://blog.adnanmasood.com/2015/06/11/mit-machine-learning-for-big-data-and-text-processing-class-notes-day-3/" target="_blank" rel="noopener">Day 3</a>、<a href="http://blog.adnanmasood.com/2015/06/12/mit-machine-learning-for-big-data-and-text-processing-class-notes-day-4/" target="_blank" rel="noopener">Day 4</a>、<a href="http://blog.adnanmasood.com/2015/06/12/mit-machine-learning-for-big-data-and-text-processing-class-notes-day-5/" target="_blank" rel="noopener">Day 5</a>.</p><ul><li><a href="http://whatsnext.nuance.com/in-the-labs/what-is-deep-machine-learning/" target="_blank" rel="noopener">《Getting “deep” about “deep learning”》</a></li></ul><p>介绍:深度学习之“深”——DNN的隐喻分析.</p><ul><li><a href="http://blog.otoro.net/2015/06/14/mixture-density-networks/" target="_blank" rel="noopener">《Mixture Density Networks》</a></li></ul><p>介绍:混合密度网络.</p><ul><li><a href="https://medium.com/@D33B/interview-questions-for-data-scientist-positions-5ad3c5d5b8bd" target="_blank" rel="noopener">《Interview Questions for Data Scientist Positions》</a></li></ul><p>介绍:数据科学家职位面试题.</p><ul><li><a href="http://scott.fortmann-roe.com/docs/MeasuringError.html" target="_blank" rel="noopener">《Accurately Measuring Model Prediction Error》</a></li></ul><p>介绍:准确评估模型预测误差.</p><ul><li><a href="https://github.com/donnemartin/data-science-ipython-notebooks" target="_blank" rel="noopener">《Continually updated Data Science Python Notebooks》</a></li></ul><p>介绍:Continually updated Data Science Python Notebooks.</p><ul><li><a href="https://github.com/jtleek/datasharing" target="_blank" rel="noopener">《How to share data with a statistician》</a></li></ul><p>介绍:How to share data with a statistician.</p><ul><li><a href="http://soumith.ch/eyescream/" target="_blank" rel="noopener">《The Eyescream Project NeuralNets dreaming natural images》</a></li></ul><p>介绍:来自Facebook的图像自动生成.</p><ul><li><a href="https://github.com/jtleek/datasharing" target="_blank" rel="noopener">《How to share data with a statistician》</a></li></ul><p>介绍:How to share data with a statistician.</p><ul><li><a href="http://arxiv.org/abs/1506.05869" target="_blank" rel="noopener">《A Neural Conversational Model》</a></li></ul><p>介绍:(Google)神经(感知)会话模型.</p><ul><li><a href="http://www.datasciencecentral.com/profiles/blogs/the-50-best-masters-in-data-science" target="_blank" rel="noopener">《The 50 Best Masters in Data Science》</a></li></ul><p>介绍:The 50 Best Masters in Data Science.</p><ul><li><a href="http://forum.memect.com/thread/nlp%E5%B8%B8%E7%94%A8%E4%BF%A1%E6%81%AF%E8%B5%84%E6%BA%90/" target="_blank" rel="noopener">《NLP常用信息资源》</a></li></ul><p>介绍:NLP常用信息资源.</p><ul><li><a href="http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf" target="_blank" rel="noopener">《Conditional Random Fields as Recurrent Neural Networks》</a></li></ul><p>介绍:语义图像分割的实况<a href="http://www.robots.ox.ac.uk/~szheng/crfasrnndemo" target="_blank" rel="noopener">演示</a>,通过深度学习技术和概率图模型的语义图像分割.</p><ul><li><a href="http://www.cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">《Fully Convolutional Networks for Semantic Segmentation》</a></li></ul><p>介绍:Caffe模型/代码:面向图像语义分割的全卷积网络,<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn" target="_blank" rel="noopener">模型代码</a>.</p><ul><li><a href="http://cacm.acm.org/news/188737-growing-pains-for-deep-learning/fulltext" target="_blank" rel="noopener">《Growing Pains for Deep Learning》</a></li></ul><p>介绍:深度学习——成长的烦恼.</p><ul><li><a href="http://www.sciencedirect.com/science/article/pii/S1877050914005274" target="_blank" rel="noopener">《Clustering Text Data Streams – A Tree based Approach with Ternary Function and Ternary Feature Vector 》</a></li></ul><p>介绍:基于三元树方法的文本流聚类.</p><ul><li><a href="http://cs.ucla.edu/~wwc/course/cs245a/mining%20book.pdf" target="_blank" rel="noopener">《Foundations and Advances in Data Mining》</a></li></ul><p>介绍:Free Ebook:数据挖掘基础及最新进展.</p><ul><li><a href="http://www.infoq.com/presentations/deep-learning" target="_blank" rel="noopener">《The Deep Learning Revolution: Rethinking Machine Learning Pipelines》</a></li></ul><p>介绍:深度学习革命.</p><ul><li><a href="http://blog.datalook.io/definitive-guide-data-science-good/" target="_blank" rel="noopener">《The Definitive Guide to Do Data Science for Good》</a></li></ul><p>介绍:数据科学(实践)权威指南.</p><ul><li><a href="http://research.microsoft.com/en-us/projects/mag/" target="_blank" rel="noopener">《Microsoft Academic Graph》</a></li></ul><p>介绍:37G的微软学术图谱数据集.</p><ul><li><a href="https://www.youtube.com/watch?v=UEwDwTkWwdc&amp;hd=1" target="_blank" rel="noopener">《Challenges and Opportunities Of Machine Learning In Production》</a></li></ul><p>介绍:生产环境(产品级)机器学习的机遇与挑战.</p><ul><li><a href="https://www.youtube.com/watch?v=Cu6A96TUy_o" target="_blank" rel="noopener">《Neural Nets for Newbies》</a></li></ul><p>介绍:神经网络入门.</p><ul><li><a href="http://jmlr.org/proceedings/papers/v37/hegde15.pdf" target="_blank" rel="noopener">《A Nearly-Linear Time Framework for Graph-Structured Sparsity》</a></li></ul><p>介绍:来自麻省理工的结构化稀疏论文.</p><ul><li><a href="http://jmlr.org/proceedings/papers/v37/beygelzimer15.pdf" target="_blank" rel="noopener">《Optimal and Adaptive Algorithms for Online Boosting》</a></li></ul><p>介绍:来自雅虎的机器学习小组关于在线Boosting的论文 .</p><ul><li><a href="http://www.kdnuggets.com/2015/06/top-20-python-machine-learning-open-source-projects.html" target="_blank" rel="noopener">《Top 20 Python Machine Learning Open Source Projects》</a></li></ul><p>介绍:20个最热门的开源(Python)机器学习项目.</p><ul><li><a href="http://arxiv.org/abs/1507.00398" target="_blank" rel="noopener">《The Parallel C++ Statistical Library for Bayesian Inference: QUESO》</a></li></ul><p>介绍:C++并行贝叶斯推理统计库QUESO,<a href="http://libqueso.com/" target="_blank" rel="noopener">github code</a>.</p><ul><li><a href="http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html" target="_blank" rel="noopener">《《Deep learning》Yann LeCun, Yoshua Bengio, Geoffrey Hinton (2015) 》</a></li></ul><p>介绍:Nature:LeCun/Bengio/Hinton的最新文章《深度学习》,Jürgen Schmidhuber的最新评论文章<a href="http://people.idsia.ch/~juergen/deep-learning-conspiracy.html" target="_blank" rel="noopener">《Critique of Paper by “Deep Learning Conspiracy” (Nature 521 p 436)》</a>.</p><ul><li><a href="https://github.com/ottogroup/palladium" target="_blank" rel="noopener">《Palladium》</a></li></ul><p>介绍:基于Scikit-Learn的预测分析服务框架Palladium.</p><ul><li><a href="http://hunch.net/~l2s/merged.pdf" target="_blank" rel="noopener">《Advances in Structured Prediction》</a></li></ul><p>介绍:John Langford和Hal Daume III在ICML2015上关于Learning to Search的教学讲座幻灯片.</p><ul><li><a href="https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan" target="_blank" rel="noopener">《100 open source Big Data architecture papers for data professionals》</a></li></ul><p>介绍:读完这100篇论文 就能成大数据高手,<a href="http://www.csdn.net/article/2015-07-07/2825148/1" target="_blank" rel="noopener">国内翻译</a>.</p><ul><li><a href="http://socialmedia-class.org/syllabus.html" target="_blank" rel="noopener">《Social Media &amp; Text Analytics》</a></li></ul><p>介绍:NLP课程《社交媒体与文本分析》精选阅读列表.</p><ul><li><a href="http://xyclade.github.io/MachineLearning/" target="_blank" rel="noopener">《Machine Learning for Developers》</a></li></ul><p>介绍:写给开发者的机器学习指南.</p><ul><li><a href="http://hameddaily.blogspot.com/2015/06/hot-news-detection-using-wikipedia_29.html" target="_blank" rel="noopener">《Hot news detection using Wikipedia》</a></li></ul><p>介绍:基于维基百科的热点新闻发现.</p><ul><li><a href="https://github.com/HIPS" target="_blank" rel="noopener">《Harvard Intelligent Probabilistic Systems Group》</a></li></ul><p>介绍:(Harvard)HIPS将发布可扩展/自动调参贝叶斯推理神经网络.</p><ul><li><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.html" target="_blank" rel="noopener">《An Empirical Exploration of Recurrent Network Architectures》</a></li></ul><p>介绍:面向上下文感知查询建议的层次递归编解码器.</p><ul><li><a href="http://jmlr.org/proceedings/papers/v37/tristan15.html" target="_blank" rel="noopener">《Efficient Training of LDA on a GPU by Mean-for-Mode Estimation》</a></li></ul><p>介绍:GPU上基于Mean-for-Mode估计的高效LDA训练.</p><ul><li><a href="https://www.youtube.com/watch?v=v-91JycaKjc&amp;hd=1" target="_blank" rel="noopener">《From the Lab to the Factory: Building a Production Machine Learning Infrastructure》</a></li></ul><p>介绍:从实验室到工厂——构建机器学习生产架构.</p><ul><li><a href="http://piktochart.com/6-useful-databases-to-dig-for-data/" target="_blank" rel="noopener">《6 Useful Databases to Dig for Data (and 100 more)》</a></li></ul><p>介绍:适合做数据挖掘的6个经典数据集(及另外100个列表).</p><ul><li><a href="http://www.computervisiontalks.com/deep-networks-for-computer-vision-at-google/" target="_blank" rel="noopener">《Deep Networks for Computer Vision at Google – ILSVRC2014》</a></li></ul><p>介绍:Google面向机器视觉的深度学习.</p><ul><li><a href="https://medium.com/@louisdorard/developer-considerations-for-choosing-a-machine-learning-api-20e2de15eb3a" target="_blank" rel="noopener">《How to choose a machine learning API to build predictive apps》</a></li></ul><p>介绍:构建预测类应用时如何选择机器学习API.</p><ul><li><a href="https://indico.io/blog/plotlines/" target="_blank" rel="noopener">《Exploring the shapes of stories using Python and sentiment APIs》</a></li></ul><p>介绍:Python+情感分析API实现故事情节(曲线)分析.</p><ul><li><a href="http://melodywolk.com/2015/07/21/movie-selection-using-r/" target="_blank" rel="noopener">《Movie selection using R》</a></li></ul><p>介绍:(R)基于Twitter/情感分析的口碑电影推荐,此外推荐<a href="http://freakonometrics.hypotheses.org/20002" target="_blank" rel="noopener">分类算法的实证比较分析</a>.</p><ul><li><a href="http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf" target="_blank" rel="noopener">《A Tutorial on Graph-based Semi-Supervised Learning Algorithms for NLP》</a></li></ul><p>介绍:CMU(ACL 2012)(500+页)面向NLP基于图的半监督学习算法.</p><ul><li><a href="http://arxiv.org/abs/1507.06411" target="_blank" rel="noopener">《Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment》</a></li></ul><p>介绍:从贝叶斯分析NIPS，看同行评审的意义.</p><ul><li><a href="http://videolectures.net/rldm2015_littman_computational_reinforcement/" target="_blank" rel="noopener">《Basics of Computational Reinforcement Learning》</a></li></ul><p>介绍:(RLDM 2015)计算强化学习入门.</p><ul><li><a href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" target="_blank" rel="noopener">《Deep Reinforcement Learning》</a></li></ul><p>介绍:David Silver的深度强化学习教程.</p><ul><li><a href="http://blog.adnanmasood.com/2015/07/31/on-explainability-of-deep-neural-networks/" target="_blank" rel="noopener">《On Explainability of Deep Neural Networks》</a></li></ul><p>介绍:深度神经网络的可解释性.</p><ul><li><a href="http://info.mapr.com/rs/mapr/images/rd204-010d-spark_0.pdf" target="_blank" rel="noopener">《The Essential Spark Cheat Sheet》</a></li></ul><p>介绍:Spark快速入门.</p><ul><li><a href="http://www.thetalkingmachines.com/blog/2015/7/30/machine-learning-for-sports-and-real-time-predictions" target="_blank" rel="noopener">《Machine Learning for Sports and Real Time Predictions》</a></li></ul><p>介绍:TalkingMachines:面向体育/政治和实时预测的机器学习.</p><ul><li><a href="http://web.stanford.edu/class/cs224w/index.html" target="_blank" rel="noopener">《CS224W: Social and Information Network Analysis Autumn 2014》</a></li></ul><p>介绍:Stanford社交网络与信息网络分析课程<a href="http://web.stanford.edu/class/cs224w/handouts.html" target="_blank" rel="noopener">资料</a>+<a href="http://web.stanford.edu/class/cs224w/projects.html" target="_blank" rel="noopener">课设</a>+<a href="http://web.stanford.edu/class/cs224w/resources.html" target="_blank" rel="noopener">数据</a>.</p><ul><li><a href="https://www.youtube.com/playlist?list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa" target="_blank" rel="noopener">《RL Course by David Silver》</a></li></ul><p>介绍:David Silver(DeeMind)的强化学习课程,<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">slide</a>.</p><ul><li><a href="http://blog.dominodatalab.com/gpu-computing-and-deep-learning/" target="_blank" rel="noopener">《Faster deep learning with GPUs and Theano》</a></li></ul><p>介绍:基于Theano/GPU的高效深度学习.</p><ul><li><a href="https://www.edx.org/course/introduction-r-programming-microsoft-dat204x" target="_blank" rel="noopener">《Introduction to R Programming》</a></li></ul><p>介绍:来自微软的<r编程入门>.</r编程入门></p><ul><li><a href="https://github.com/cdipaolo/sentiment-server" target="_blank" rel="noopener">《Golang:Web Server For Performing Sentiment Analysis》</a></li></ul><p>介绍:(Go)情感分析API服务Sentiment Server.</p><ul><li><a href="http://deeplearning4j.org/restrictedboltzmannmachine.html" target="_blank" rel="noopener">《A Beginner’s Guide to Restricted Boltzmann Machines》</a></li></ul><p>介绍:受限波尔兹曼机初学者指南.</p><ul><li><a href="http://www.kdd.org/kdd2015/program.html" target="_blank" rel="noopener">《KDD2015十年最佳论文》</a></li></ul><p>介绍:<a href="http://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf" target="_blank" rel="noopener">Mining and Summarizing Customer Reviews </a>,<a href="http://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf" target="_blank" rel="noopener">Mining High-Speed Data Streams</a>,<a href="http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf" target="_blank" rel="noopener">Optimizing Search Engines using Clickthrough Data</a>.</p><ul><li><a href="http://www.hellophp.cn/archives/733" target="_blank" rel="noopener">《Nvidia Deep Learning Courses》</a></li></ul><p>介绍:Nvidia深度学习课程.</p><ul><li><a href="https://sites.google.com/site/deeplearningsummerschool/" target="_blank" rel="noopener">《Deep Learning Summer School 2015》</a></li></ul><p>介绍:2015年深度学习暑期课程，推荐<a href="http://www.iro.umontreal.ca/~memisevr" target="_blank" rel="noopener">讲师主页</a>.</p><ul><li><a href="http://www.cvrobot.net/image-recognition-progression-based-on-deep-learning-by-baidu/" target="_blank" rel="noopener">《百度深度学习的图像识别进展》</a></li></ul><p>介绍:这是一篇关于百度文章<a href="http://www.ccf.org.cn/sites/ccf/xhdtnry.jsp?contentId=2857471255804" target="_blank" rel="noopener">《基于深度学习的图像识别进展：百度的若干实践》</a>的摘要,建议两篇文章结合起来阅读.</p><ul><li><a href="http://rnd.azoft.com/machine-learning-methods-video-annotation/" target="_blank" rel="noopener">《Machine Learning Methods in Video Annotation》</a></li></ul><p>介绍:视频标注中的机器学习技术.</p><ul><li><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" target="_blank" rel="noopener">《Training Recurrent Neural Networks》</a></li></ul><p>介绍:博士论文:(Ilya Sutskever)RNN训练.</p><ul><li><a href="http://blog.adnanmasood.com/2015/07/31/on-explainability-of-deep-neural-networks/" target="_blank" rel="noopener">《On Explainability of Deep Neural Networks》</a></li></ul><p>介绍:深度神经网络的灰色区域：可解释性问题，<a href="http://www.csdn.net/article/2015-08-17/2825471" target="_blank" rel="noopener">中文版</a>.</p><ul><li><a href="http://www.fodop.com/ar-1002" target="_blank" rel="noopener">《Machine Learning Libraries in GoLang by Category》</a></li></ul><p>介绍:Golang 实现的机器学习库资源汇总.</p><ul><li><a href="http://blog.shakirm.com/wp-content/uploads/2015/07/SVDL.pdf" target="_blank" rel="noopener">《A Statistical View of Deep Learning》</a></li></ul><p>介绍:深度学习的统计分析.</p><ul><li><a href="http://www.researchgate.net/publication/279853751_DEEP_LEARNING_FOR_NLP_-_TIPS_AND_TECHNIQUES" target="_blank" rel="noopener">《Deep Learning For NLP - Tips And Techniques》</a></li></ul><p>介绍:面向NLP的深度学习技术与技巧.</p><ul><li><a href="http://blog.kaggle.com/2015/08/18/crowdflower-scripts-approaching-nlp/" target="_blank" rel="noopener">《CrowdFlower Competition Scripts: Approaching NLP》</a></li></ul><p>介绍:Kaggle’s CrowdFlower竞赛NLP代码集锦.</p><ul><li><a href="http://web.stanford.edu/class/cs224u/index.html" target="_blank" rel="noopener">《CS224U: Natural Language Understanding》</a></li></ul><p>介绍:斯坦福的自然语言理解课程.</p><ul><li><a href="http://freemind.pluskid.org/machine-learning/deep-learning-and-shallow-learning/" target="_blank" rel="noopener">《Deep Learning and Shallow Learning》</a></li></ul><p>介绍:Deep Learning与Shallow Learning 介绍</p><ul><li><a href="http://www.ics.uci.edu/~welling/teaching/ICS273Afall11/IntroMLBook.pdf" target="_blank" rel="noopener">《A First Encounter with Machine Learning》</a></li></ul><p>介绍:这是一本机器学习的电子书,作者<a href="http://www.ics.uci.edu/~welling/" target="_blank" rel="noopener">Max Welling</a>先生在机器学习教学上面有着丰富的经验,这本书小但精致.</p><ul><li><a href="http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf" target="_blank" rel="noopener">《Click Models for Web Search》</a></li></ul><p>介绍:由荷兰阿姆斯特丹大学 &amp; 谷歌瑞士著.</p><ul><li><a href="http://www.cnblogs.com/shouhuxianjian/p/4529235.html" target="_blank" rel="noopener">《Hinton CSC321课程/Deep Learning/Notes on CNN/Python/Theano/CUDA/OpenCV/…》</a></li></ul><p>介绍:介绍个乐于总结和翻译机器学习和计算机视觉类资料的博客,包含的内容：Hinton的CSC321课程的总结；Deep Learning综述；Notes on CNN的总结；python的原理总结；Theano基础知识和练习总结；CUDA原理和编程；OpenCV一些总结.</p><ul><li><a href="http://blogs.technet.com/b/machinelearning/archive/2015/09/01/which-algorithm-family-can-answer-my-question.aspx" target="_blank" rel="noopener">《Which Algorithm Family Can Answer My Question?》</a></li></ul><p>介绍:针对具体问题(应用场景)如何选择机器学习算法(系列).</p><ul><li><a href="http://www.learndatasci.stfi.re/free-books/" target="_blank" rel="noopener">《Free Data Science Books》</a></li></ul><p>介绍:数据科学免费书分类集合</p><ul><li><a href="http://www.superlectures.com/iscslp2014/tutorial-4-deep-learning-for-speech-generation-and-synthesis" target="_blank" rel="noopener">《Tutorial 4: Deep Learning for Speech Generation and Synthesis》</a></li></ul><p>介绍:深度学习在语音合成最新进展有哪些？推荐MSRA的Frank Soong老师关于语音合成的深度学习方法的录像和幻灯片与以及谷歌的LSTM-RNN合成<a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42624.pdf" target="_blank" rel="noopener">介绍</a>,<a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43893.pdf" target="_blank" rel="noopener">论文</a></p><ul><li><a href="https://leanpub.com/artofdatascience" target="_blank" rel="noopener">《The Art of Data Science》</a></li></ul><p>介绍:新书(可免费下载):数据科学的艺术</p><ul><li><a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" target="_blank" rel="noopener">《Pattern Recognition and Machine Learning》</a></li></ul><p>介绍:模式识别与机器学习书籍推荐,本书是微软剑桥研究院大神Bishop所写，算是最为广为认知的机器学习教材之一，内容覆盖全面，难度中上，适合研究生<a href="https://www.dropbox.com/s/sx95jq7n7zerrjl/PRML_Translation.pdf?dl=0" target="_blank" rel="noopener">中文版</a> or <a href="http://pan.baidu.com/s/1hqheD5E" target="_blank" rel="noopener">备份</a></p><ul><li><a href="http://piksels.com/wp-content/uploads/2009/01/visualizingdata.pdf" target="_blank" rel="noopener">《an introduction to visualizing DATA》</a></li></ul><p>介绍:数据可视化介绍(23页袖珍小册子)</p><ul><li><a href="https://www.cs.cmu.edu/~yww/papers/emnlp2015petpeeves.pdf" target="_blank" rel="noopener">《That’s So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets ∗》</a></li></ul><p>介绍:这篇论文荣获EMNLP2015的最佳数据/资源奖优秀奖,<a href="https://www.cs.cmu.edu/~yww/data/petpeeves.zip" target="_blank" rel="noopener">标注的推特数据集</a></p><ul><li><a href="http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/" target="_blank" rel="noopener">《26 Things I Learned in the Deep Learning Summer School》</a></li></ul><p>介绍:作者在深度学习的思考.</p><ul><li><a href="http://keshif.me/demo/VisTools" target="_blank" rel="noopener">《Data-Visualization Tools &amp; Books》</a></li></ul><p>介绍:数据可视化常用工具软件资源汇总</p><ul><li><a href="http://www.cedar.buffalo.edu/~srihari/CSE574/" target="_blank" rel="noopener">《Machine Learning and Probabilistic Graphical Models Course》</a></li></ul><p>介绍:Buffalo大学教授Sargur Srihari的“机器学习和概率图模型”的视频课程</p><ul><li><a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html" target="_blank" rel="noopener">《Understanding Machine Learning: From Theory to Algorithms》</a></li></ul><p>介绍:耶路撒冷希伯来大学教授Shai Shalev-Shwartz和滑铁卢大学教授Shai Ben-David的新书Understanding Machine Learning: From Theory to Algorithms,此书写的比较偏理论，适合对机器学习理论有兴趣的同学选读</p><ul><li><a href="http://machinelearningmastery.com/machine-learning-checklist/" target="_blank" rel="noopener">《Machine Learning Checklist》</a></li></ul><p>介绍:机器学习学习清单</p><ul><li><a href="http://www.zhihu.com/question/32318281" target="_blank" rel="noopener">《NLP界有哪些神级人物？》</a></li></ul><p>介绍:知乎上面的一篇关于NLP界有哪些神级人物？提问。首推Michael Collins </p><ul><li><a href="http://www.csdn.net/article/2015-09-08/2825647" target="_blank" rel="noopener">《机器学习温和指南》</a></li></ul><p>介绍:机器学习与NLP专家、MonkeyLearn联合创始人&amp;CEO Raúl Garreta面向初学者大体概括使用机器学习过程中的重要概念，应用程序和挑战，旨在让读者能够继续探寻机器学习知识。</p><ul><li><a href="http://nbviewer.ipython.org/github/pprett/pydata-gbrt-tutorial/blob/master/gbrt-tutorial.ipynb" target="_blank" rel="noopener">《Gradient Boosted Regression Trees》</a></li></ul><p>介绍:(IPN)基于Scikit-Learn的GBRT(Gradient Boost Regression Tree)教程，<a href="http://orbi.ulg.ac.be/bitstream/2268/163521/1/slides.pdf" target="_blank" rel="noopener">slide</a></p><ul><li><a href="http://www.comp.nus.edu.sg/~dbsystem/singa/" target="_blank" rel="noopener">《Apache SINGA : Distributed Deep Learning System》</a></li></ul><p>介绍: 无需做深度学习就能用的分布式深度学习软件.</p><ul><li><a href="http://dl.acm.org/citation.cfm?id=2800178" target="_blank" rel="noopener">《E-commerce Recommendation with Personalized Promotion》</a></li></ul><p>介绍: 在亚马逊数据和众包Mechanical Turk上，实现了来自彩票和拍卖的机制，以收集用户对产品的乐意购买价格（WTP，willingness-to-pay）训练集。 E-commerce Recommendation with Personalized Promotion [Zhao,RecSys15] 回归模型预测未知WTP，提升卖家利润和消费者满意度</p><ul><li><a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x" target="_blank" rel="noopener">《Scalable Machine Learning》</a></li></ul><p>介绍:来自伯克利分校的大规模机器学习.</p><ul><li><a href="http://www.52ml.net/star" target="_blank" rel="noopener">《机器学习资料大汇总》</a></li></ul><p>介绍:来自52ml的机器学习资料大汇总.</p><ul><li><a href="http://www.cis.upenn.edu/~nenkova/1500000015-Nenkova.pdf" target="_blank" rel="noopener">《Automatic Summarization》</a></li></ul><p>介绍:这本书的作<a href="http://www.cis.upenn.edu/~nenkova/" target="_blank" rel="noopener">者McKeown</a>是2013年世界首个数据科学院（位于哥伦比亚大学）主任，她亦是ACL、AAAI和ACM Fellow .</p><ul><li><a href="http://www.emnlp2015.org/proceedings/EMNLP/index.html" target="_blank" rel="noopener">《Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing》</a></li></ul><p>介绍:EMNLP-15文本摘要若干.</p><ul><li><a href="http://www.slideshare.net/xamat/recommender-systems-machine-learning-summer-school-2014-cmu" target="_blank" rel="noopener">《Recommender Systems (Machine Learning Summer School 2014 @ CMU)》</a></li></ul><p>介绍:来自Netflix的Xavier Amatriain在Summer School 2014 @ CMU上长达4小时的报告，共248页，是对推荐系统发展的一次全面综述，其中还包括Netflix在个性化推荐方面的一些经验介绍.</p><ul><li><a href="http://www.ecmlpkdd2015.org/sites/default/files/ECMLPKDD2015Slides.pdf" target="_blank" rel="noopener">《BigData Stream Mining》</a></li></ul><p>介绍:(ECML PKDD 2015)大数据流挖掘教程,此外推荐<a href="http://www.ecmlpkdd2015.org/program/tutorial-list" target="_blank" rel="noopener">ECML PKDD 2015 Tutorial列表</a>.</p><ul><li><a href="https://github.com/maxpumperla/elephas" target="_blank" rel="noopener">《Deep learning on Spark with Keras》</a></li></ul><p>介绍:Spark上的<a href="https://github.com/fchollet/keras" target="_blank" rel="noopener">Keras</a>深度学习框架Elephas.</p><ul><li><a href="https://www.youtube.com/watch?v=7KCWcx-YIRI&amp;hd=1" target="_blank" rel="noopener">《Prof. Surya Ganguli - The statistical physics of deep learning》</a></li></ul><p>介绍:Surya Ganguli深度学习统计物理学.</p><ul><li><a href="http://cmlakhan.github.io/courses/videos.html" target="_blank" rel="noopener">《(系统/算法/机器学习/深度学习/图模型/优化/…)在线视频课程列表》</a></li></ul><p>介绍:(系统/算法/机器学习/深度学习/图模型/优化/…)在线视频课程列表.</p><ul><li><a href="http://chdoig.github.io/pytexas2015-topic-modeling/" target="_blank" rel="noopener">《Introduction to Topic Modeling in Python》</a></li></ul><p>介绍:(PyTexas 2015)Python主题建模.</p><ul><li><a href="http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop/" target="_blank" rel="noopener">《Large Scale Distributed Deep Learning on Hadoop Clusters》</a></li></ul><p>介绍:Hadoop集群上的大规模分布式机器学习.</p><ul><li><a href="http://www.vordot.com/deep-learning-employers-w-12020/" target="_blank" rel="noopener">《Top Deep Learning Employers Based On LinkedIn Data》</a></li></ul><p>介绍:基于LinkedIn数据得出的深度学习热门”东家”排行.</p><ul><li><a href="https://vimeo.com/19569529" target="_blank" rel="noopener">《Neural Net in C++ Tutorial》</a></li></ul><p>介绍:(c++)神经网络手把手实现教程.</p><ul><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">《Large-scale CelebFaces Attributes (CelebA) Dataset》</a></li></ul><p>介绍:香港中文大学汤晓鸥教授实验室公布的大型人脸识别数据集： Large-scale CelebFaces Attributes (CelebA) Dataset 10K 名人，202K 脸部图像，每个图像40余标注属性.</p><ul><li><a href="https://www.cs.nyu.edu/web/Research/Theses/goroshin_ross.pdf" target="_blank" rel="noopener">《Unsupervised Feature Learning in Computer Vision》</a></li></ul><p>介绍:面向机器视觉的无监督特征学习,<a href="https://cs.nyu.edu/~goroshin/" target="_blank" rel="noopener">Ross Goroshin’s webpage</a>.</p><ul><li><a href="http://arxiv.org/pdf/1506.03099v3.pdf" target="_blank" rel="noopener">《Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks》</a></li></ul><p>介绍:谷歌研究院Samy Bengio等人最近写的RNN的Scheduled Sampling训练方法论文.</p><ul><li><a href="https://manish.wordpress.com/2015/10/02/essential-machine-learning-algorithms-in-a-nutshell/" target="_blank" rel="noopener">《Essential Machine Learning Algorithms in a nutshell》</a></li></ul><p>介绍:机器学习基本算法简要入门.</p><ul><li><a href="http://blog.josephmisiti.com/a-huge-list-of-machine-learning-repositories/" target="_blank" rel="noopener">《A Huge List of Machine Learning And Statistics Repositories》</a></li></ul><p>介绍:Github机器学习/数学/统计/可视化/深度学习相关项目大列表.</p><ul><li><a href="http://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs.html" target="_blank" rel="noopener">《Information Processing and Learning》</a></li></ul><p>介绍:CMU的信息论课程.</p><ul><li><a href="http://arxiv.org/pdf/1506.03099v3.pdf" target="_blank" rel="noopener">《Scheduled sampling for sequence prediction with recurrent neural networks》</a></li></ul><p>介绍:谷歌研究院<a href="http://bengio.abracadoudou.com/" target="_blank" rel="noopener">Samy Bengio</a>等人最近写的RNN的Scheduled Sampling训练方法论文.</p><ul><li><a href="http://www.csdn.net/article/2015-10-01/2825840" target="_blank" rel="noopener">《基于Hadoop集群的大规模分布式深度学习》</a></li></ul><p>介绍:基于Hadoop集群的大规模分布式深度学习.</p><ul><li><a href="http://arxiv.org/abs/1506.02626" target="_blank" rel="noopener">《Learning both Weights and Connections for Efficient Neural Networks习》</a></li></ul><p>介绍:来自斯坦福大学及NVIDIA的工作，很实在很实用。采用裁剪网络连接及重训练方法，可大幅度减少CNN模型参数。针对AlexNet、VGG等模型及ImageNet数据，不损失识别精度情况下，模型参数可大幅度减少9-13倍.</p><ul><li><a href="http://www.comp.nus.edu.sg/~dbsystem/singa/" target="_blank" rel="noopener">《Apache Singa —A General Distributed Deep Learning Platform》</a></li></ul><p>介绍:无需做深度学习就能用的分布式深度学习软件,<a href="https://github.com/apache/incubator-singa" target="_blank" rel="noopener">github</a>.</p><ul><li><a href="http://www.analyticsvidhya.com/blog/2015/09/ultimate-data-scientists-world-today/" target="_blank" rel="noopener">《24 Ultimate Data Scientists To Follow in the World Today》</a></li></ul><p>介绍:当今世界最NB的25位大数据科学家,通过他们的名字然后放在google中搜索肯定能找到很多很棒的资源<a href="http://blog.csdn.net/heyongluoyao8/article/details/48598169" target="_blank" rel="noopener">译文</a>.</p><ul><li><a href="https://github.com/nreimers/deeplearning4nlp-tutorial/tree/master/2015-10_Lecture/" target="_blank" rel="noopener">《Deep Learning for NLP - Lecture October 2015》</a></li></ul><p>介绍:Nils Reimers面向NLP的深度学习(Theano/Lasagne)系列教程.</p><ul><li><a href="https://ccle.ucla.edu/mod/page/view.php?id=834267" target="_blank" rel="noopener">《Connection between probability theory and real analysis》</a></li></ul><p>介绍:主讲人是<a href="https://ccle.ucla.edu/mod/page/view.php?id=834267" target="_blank" rel="noopener">陶哲轩</a>,资料<a href="http://www.math.duke.edu/~rtd/PTE/PTE4_1.pdf" target="_blank" rel="noopener">Probability: Theory and Examples</a>,<a href="https://terrytao.wordpress.com/category/275a-probability-theory/" target="_blank" rel="noopener">笔记</a>.</p><ul><li><a href="http://www.districtdatalabs.com/#!resources/c21hq" target="_blank" rel="noopener">《Data Science Learning Resources》</a></li></ul><p>介绍:数据科学(学习)资源列表.</p><ul><li><a href="http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="noopener">《8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset》</a></li></ul><p>介绍:应对非均衡数据集分类问题的八大策略.</p><ul><li><a href="https://datarithms.wordpress.com/2015/08/16/top-20-data-science-moocs/" target="_blank" rel="noopener">《Top 20 Data Science MOOCs》</a></li></ul><p>介绍:重点推荐的20个数据科学相关课程.</p><ul><li><a href="https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks/" target="_blank" rel="noopener">《Recurrent Neural Networks》</a></li></ul><p>介绍:递归神经网络.</p><ul><li><a href="http://www.cs.duke.edu/courses/fall15/compsci527/notes/hog.pdf" target="_blank" rel="noopener">《Histograms of Oriented Gradients》</a></li></ul><p>介绍:(HOG)学习笔记.</p><ul><li><a href="http://aidanhorner.blogspot.co.uk/2015/10/computational-modelling-courses.html" target="_blank" rel="noopener">《Computational modelling courses》</a></li></ul><p>介绍:计算建模/计算神经学课程汇总.</p><ul><li><a href="http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html" target="_blank" rel="noopener">《How We Use Deep Learning to Classify Business Photos at Yelp》</a></li></ul><p>介绍:(Yelp)基于深度学习的商业图片分类.</p><ul><li><a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">《Neural Networks and Deep Learning》</a></li></ul><p>介绍:免费在线书《Neural Networks and Deep Learning》神经网络与深度学习。目前提供了前四章的草稿，<a href="http://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&amp;mid=400067748&amp;idx=1&amp;sn=9c88eadfba5462281cd496e85ba3329c" target="_blank" rel="noopener">第一章</a>通过手写数字识别的例子介绍NN，第二章讲反向传播算法，第三章讲反向传播算法的优化，第四章讲NN为什么能拟合任意函数。大量python代码例子和交互动画，生动有趣.<a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/" target="_blank" rel="noopener">中文版</a></p><ul><li><a href="http://www.datasciguide.com/books-to-read-if-you-might-be-interested-in-data-science/" target="_blank" rel="noopener">《Books to Read if You Might Be Interested in Data Science》</a></li></ul><p>介绍:数据科学大咖荐书(入门).</p><ul><li><a href="https://github.com/andrewt3000/DL4NLP" target="_blank" rel="noopener">《Deep Learning for NLP resources》</a></li></ul><p>介绍:NLP 深度学习资源列表.</p><ul><li><a href="http://gitxiv.com/" target="_blank" rel="noopener">《GitXiv》</a></li></ul><p>介绍:很多arXiv上面知名论文可以在这个网站找到github的项目链接.</p><ul><li><a href="http://arxiv.org/pdf/1510.07945v1.pdf" target="_blank" rel="noopener">《Learning Multi-Domain Convolutional Neural Networks for Visual Tracking》</a></li></ul><p>介绍:深度学习在视觉跟踪的探索.</p><ul><li><a href="http://fullstackml.com/2015/10/29/beginners-guide-apache-spark-machine-learning-scenario-with-a-large-input-dataset/" target="_blank" rel="noopener">《Beginners Guide: Apache Spark Machine Learning Scenario With A Large Input Dataset》</a></li></ul><p>介绍:Spark机器学习入门实例——大数据集(30+g)二分类.</p><ul><li><a href="https://www.semanticscholar.org/" target="_blank" rel="noopener">《Semantic Scholar》</a></li></ul><p>介绍:保罗艾伦人工智能实验室表示，Google Scholar是十年前的产物，他们现在想要做进一步的提高。于是推出了全新的，专门针对科学家设计的学术搜索引擎Semantic Scholar.</p><ul><li><a href="http://www.acad.bg/ebook/ml/MITPress-%20SemiSupervised%20Learning.pdf" target="_blank" rel="noopener">《Semi-Supervised Learning》</a></li></ul><p>介绍:半监督学习,Chapelle.篇篇都是经典,作者包括Vapnik,Bengio,Lafferty,Jordan.此外推荐<a href="http://pages.cs.wisc.edu/~jerryzhu/" target="_blank" rel="noopener">Xiaojin (Jerry) Zhu</a>编写的<a href="http://www.morganclaypool.com/doi/abs/10.2200/S00196ED1V01Y200906AIM006" target="_blank" rel="noopener">Introduction to Semi-Supervised Learning</a>.</p><p>介绍:Spark机器学习入门实例——大数据集(30+g)二分类.</p><ul><li><a href="http://www.analyticsvidhya.com/blog/2015/11/free-resources-beginners-deep-learning-neural-network/" target="_blank" rel="noopener">《Free Resources for Beginners on Deep Learning and Neural Network》</a></li></ul><p>介绍:为入门者准备的深度学习与神经网络免费资源.</p><ul><li><a href="http://tensorflow.org/" target="_blank" rel="noopener">《TensorFlow is an Open Source Software Library for Machine Intelligence》</a></li></ul><p>介绍:Google 开源最新机器学习系统 TensorFlow,此外提供TensorFlow白皮书<a href="http://pan.baidu.com/s/1jGyFPki" target="_blank" rel="noopener">white paper of tensorflow 2015</a>.<a href="https://news.ycombinator.com/item?id=10532957" target="_blank" rel="noopener">hacker news</a>,<a href="https://www.youtube.com/watch?v=90-S1M7Ny_o&amp;t=21m2s" target="_blank" rel="noopener">Google大牛解读TensorFlow</a></p><ul><li><a href="https://github.com/samsung/veles" target="_blank" rel="noopener">《Veles:Distributed machine learning platform》</a></li></ul><p>介绍:三星开源的快速深度学习应用程序开发分布式平台.</p><ul><li><a href="https://github.com/Microsoft/DMTK" target="_blank" rel="noopener">《DMTK:Microsoft Distributed Machine Learning Tookit 》</a></li></ul><p>介绍:分布式机器学习工具包.</p><ul><li><a href="http://wiki.knoesis.org/index.php/BigDataTutorial" target="_blank" rel="noopener">《Semantics Approach to Big Data and Event Processing》</a></li></ul><p>介绍:语义大数据——大数据/事件处理的语义方法.</p><ul><li><a href="http://www.zhihu.com/question/29411132" target="_blank" rel="noopener">《LSTM(Long Short Term Memory)和RNN(Recurrent)学习教程》</a></li></ul><p>介绍:LSTM(Long Short Term Memory)和RNN(Recurrent)学习教程.</p><ul><li><a href="http://marvin.is/" target="_blank" rel="noopener">《Marvin：A minimalist GPU-only N-dimensional ConvNet framework》</a></li></ul><p>介绍:Princeton Vision Group的深度学习库开源.</p><ul><li><a href="http://ufora.github.io/ufora/" target="_blank" rel="noopener">《Ufora is a compiled, automatically parallel subset of python for data science and numerical computing》</a></li></ul><p>介绍:基于AWS的自动分布式科学计算库Ufora,<a href="https://medium.com/art-marketing/why-i-open-sourced-five-years-of-work-c5b5e0e38a6d" target="_blank" rel="noopener">Why I Open Sourced Five Years of Work</a>.</p><ul><li><a href="https://www.youtube.com/watch?v=wBKfGaakFp8&amp;hd=1" target="_blank" rel="noopener">《Deep Learning and Deep Data Science - PyCon SE 2015》</a></li></ul><p>介绍:(PyCon SE 2015)深度学习与深度数据科学.</p><ul><li><a href="https://scholar.google.com/citations?user=rSVIHasAAAAJ&amp;hl=zh-CN&amp;oi=ao" target="_blank" rel="noopener">《Zhi-Hua Zhou Papers》</a></li></ul><p>介绍:推荐南京大学机器学习与数据挖掘研究所所长——周志华教授的Google学术主页.</p><ul><li><a href="https://leanpub.com/lm" target="_blank" rel="noopener">《Advanced Linear Models for Data Science》</a></li></ul><p>介绍:免费书:面向数据科学的高级线性模型.</p><ul><li><a href="http://arxiv.org/abs/1511.05641" target="_blank" rel="noopener">《Net2Net: Accelerating Learning via Knowledge Transfer》</a></li></ul><p>介绍:基于知识迁移的神经网络高效训练Net2Net.</p><ul><li><a href="https://www.youtube.com/playlist?list=PLFze15KrfxbF0n1zTNoFIaDpxnSyfgNgc" target="_blank" rel="noopener">《徐亦达机器学习课程 Variational Inference》</a></li></ul><p>介绍:徐亦达机器学习课程 Variational Inference.</p><ul><li><a href="http://arxiv.org/abs/1511.05497v1" target="_blank" rel="noopener">《Learning the Architecture of Deep Neural Networks》</a></li></ul><p>介绍:深度神经网络结构学习.</p><ul><li><a href="http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf" target="_blank" rel="noopener">《Multimodal Deep Learning》</a></li></ul><p>介绍:来自斯坦福大学的Multimodal Deep Learning papers.</p><ul><li><a href="http://chiffon.gitcafe.io/2015/11/16/long.html" target="_blank" rel="noopener">《深度学习简析,TensorFlow,Torch,Theano,Mxnet》</a></li></ul><p>介绍:深度学习简析,TensorFlow,Torch,Theano,<a href="https://github.com/dmlc/mxnet" target="_blank" rel="noopener">Mxnet</a>.</p><ul><li><a href="https://medium.com/notes-essays-cs183c-technology-enabled-blitzscalin/latest" target="_blank" rel="noopener">《”Notes Essays —CS183C: Technology-enabled Blitzscaling — Stanford University》</a></li></ul><p>介绍:这个专栏是一个stanford学生做的CS183c课程的一个note，该课程是由Reid Hoffman等互联网boss级人物开设的，每节课请一位巨头公司的相关负责人来做访谈，讲述该公司是怎么scale的。最新两期分别请到了雅虎的梅姐和airbnb创始人Brian Chesky。.</p><ul><li><a href="https://github.com/nyu-dl/NLP_DL_Lecture_Note" target="_blank" rel="noopener">《Natural Language Understanding with Distributed Representation》</a></li></ul><p>介绍:基于分布式表示的自然语言理解(100+页),<a href="http://arxiv.org/abs/1511.07916" target="_blank" rel="noopener">论文</a>.</p><ul><li><a href="http://link.springer.com/book/10.1007/978-1-4899-7637-6" target="_blank" rel="noopener">《Recommender Systems Handbook》</a></li></ul><p>介绍:推荐系统手册.</p><ul><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/index.html" target="_blank" rel="noopener">《Understanding LSTM Networks》</a></li></ul><p>介绍:理解LSTM网络<a href="http://www.csdn.net/article/2015-11-25/2826323" target="_blank" rel="noopener">翻译</a>.</p><ul><li><a href="https://www.linkedin.com/pulse/machine-learning-quora-xavier-amatriain" target="_blank" rel="noopener">《Machine Learning at Quora》</a></li></ul><p>介绍:机器学习在quora中的应用.</p><ul><li><a href="http://arxiv.org/abs/1511.09249" target="_blank" rel="noopener">《On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models》</a></li></ul><p>介绍:思维学习——RL+RNN算法信息论.</p><ul><li><a href="https://blog.rjmetrics.com/2015/12/01/the-5-ways-data-scientists-keep-learning-after-college/" target="_blank" rel="noopener">《The 5 Ways Data Scientists Keep Learning After College》</a></li></ul><p>介绍:数据科学家毕业后继续学习的5种方式.</p><ul><li><a href="http://arxiv.org/abs/1404.7828" target="_blank" rel="noopener">《Deep Learning in Neural Networks: An Overview》</a></li></ul><p>介绍:深度学习在神经网络的应用.</p><ul><li><a href="http://arxiv.org/abs/1511.06429" target="_blank" rel="noopener">《Contextual Learning》</a></li></ul><p>介绍:上下文学习,<a href="https://gitlab.tubit.tu-berlin.de/rbo-lab/concarne" target="_blank" rel="noopener">代码</a>.</p><ul><li><a href="http://pythonforengineers.com/machine-learning-for-complete-beginners/" target="_blank" rel="noopener">《Machine Learning For Complete Beginners》</a></li></ul><p>介绍:机器学习零基础入门,<a href="https://github.com/shantnu/Titanic-Machine-Learning" target="_blank" rel="noopener">代码</a>.</p><ul><li><a href="http://www.ccf.org.cn/sites/ccf/xhdtnry.jsp?contentId=2897719129810" target="_blank" rel="noopener">《2015年中国计算机学会（CCF）优秀博士学位论文》</a></li></ul><p>介绍:2015年度CCF优秀博士学位论文奖论文列表.</p><ul><li><a href="http://cs.nju.edu.cn/lwj/L2H.html" target="_blank" rel="noopener">《Learning to Hash Paper, Code and Dataset》</a></li></ul><p>介绍:Learning to Hash Paper, Code and Dataset.</p><ul><li><a href="https://www.youtube.com/watch?v=dtGhSE1PFh0" target="_blank" rel="noopener">《Neural networks with Theano and Lasagne》</a></li></ul><p>介绍:(PyData2015)基于Theano/Lasagne的CNN/RNN教程,<a href="https://github.com/ebenolson/pydata2015" target="_blank" rel="noopener">github</a>.</p><ul><li><a href="http://vdisk.weibo.com/s/ayG13we2ltDAT" target="_blank" rel="noopener">《神经网络与深度学习讲义》</a></li></ul><p>介绍:复旦大学<a href="http://weibo.com/xpqiu" target="_blank" rel="noopener">邱锡鹏</a>老师编写的神经网络与深度学习讲义,<a href="http://vdisk.weibo.com/s/ayG13we2lDzcV" target="_blank" rel="noopener">ppt</a>.</p><ul><li><a href="http://www.dmtk.io/index.html" target="_blank" rel="noopener">《Microsoft Open Sources Distributed Machine Learning Toolkit》</a></li></ul><p>介绍:微软亚洲研究院开源分布式机器学习工具包.</p><ul><li><a href="https://www.zhihu.com/question/20398418" target="_blank" rel="noopener">《语音识别的技术原理是什么？》</a></li></ul><p>介绍:语音识别的技术原理浅析</p><ul><li><a href="http://www.cs.berkeley.edu/~jordan/" target="_blank" rel="noopener">《Michael I. Jordan》</a></li></ul><p>介绍:迈克尔·I.乔丹的主页.根据主页可以找到很多资源。迈克尔·I.乔丹是知名的计算机科学和统计学学者，主要研究机器学习和人工智能。他的重要贡献包括指出了机器学习与统计学之间的联系，并推动机器学习界广泛认识到贝叶斯网络的重要性。</p><ul><li><a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="noopener">《Geoff Hinton》</a></li></ul><p>介绍:杰弗里·埃弗里斯特·辛顿 FRS是一位英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。辛顿是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者.通过他的主页可以发掘到很多Paper以及优秀学生的paper,此外推荐他的学生<a href="http://yann.lecun.com/" target="_blank" rel="noopener">Yann Lecun</a>主页</p><ul><li><a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" target="_blank" rel="noopener">《Yoshua Bengio》</a></li></ul><p>介绍:Yoshua Bengio是机器学习方向的牛人,如果你不知道可以阅读<a href="http://www.infoq.com/cn/articles/ask-yoshua-bengio" target="_blank" rel="noopener">对话机器学习大神Yoshua Bengio（上）</a>,<a href="http://www.infoq.com/cn/articles/ask-yoshua-bengio-2" target="_blank" rel="noopener">对话机器学习大神Yoshua Bengio（下）</a></p><ul><li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/CIKM-keynote-Nov2014.pdf" target="_blank" rel="noopener">《Large Scale Deep Learning within google》</a></li></ul><p>介绍:google大规模深度学习应用演进</p><ul><li><a href="http://goodfeli.github.io/dlbook/" target="_blank" rel="noopener">《Deep Learning: An MIT Press Book in Preparation》</a></li></ul><p>介绍:MIT出版的深度学习电子书,公开电子书</p><ul><li><a href="http://arxiv.org/abs/1512.06293" target="_blank" rel="noopener">《A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction》</a></li></ul><p>介绍:深度卷积神经网络(CNN)提取特征的数学理论</p><ul><li><a href="http://research.microsoft.com/en-us/um/people/kahe/" target="_blank" rel="noopener">《Microsoft Research Asia：Kaiming He》</a></li></ul><p>介绍:推荐微软亚洲研究院何恺明主页</p><ul><li><a href="http://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">《Speech and Language Processing (3rd ed. draft)》</a></li></ul><p>介绍:《语音与语言处理》第三版(草稿)</p><ul><li><a href="http://web.stanford.edu/~jurafsky/li15/" target="_blank" rel="noopener">《LSA 311: Computational Lexical Semantics - Summer 2015》</a></li></ul><p>介绍:Stanford新课”计算词汇语义学”</p><ul><li><a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=397" target="_blank" rel="noopener">《上海交大张志华老师的统计机器学习与机器学习导论视频》</a></li></ul><p>介绍:上海交大张志华老师的统计机器学习与机器学习导论视频<a href="http://pan.baidu.com/s/1mgPi7jU" target="_blank" rel="noopener">链接:</a>密码: r9ak .<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=398" target="_blank" rel="noopener">概率基础</a></p><ul><li><a href="http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239" target="_blank" rel="noopener">《Computational Linguistics and Deep Learning》</a></li></ul><p>介绍:computational linguistics and deep learning<a href="http://techtalks.tv/talks/computational-linguistics-and-deep-learning/61759/" target="_blank" rel="noopener">视频</a>,推荐<a href="https://speakerdeck.com/baojie/deep-learning-an-introduction-from-the-nlp-perspective-by-kevin-duh" target="_blank" rel="noopener">Deep Learning: An Introduction from the NLP Perspective</a></p><ul><li><a href="https://www.youtube.com/watch?v=zfVfpMcUkq8" target="_blank" rel="noopener">《Black Hat USA 2015 - Deep Learning On Disassembly》</a></li></ul><p>介绍:(BlackHat2015)深度学习应用之流量鉴别(协议鉴别/异常检测),[slide])(<a href="https://www.blackhat.com/docs/us-15/materials/us-15-Wang-The-Applications-Of-Deep-Learning-On-Traffic-Identification.pdf),[material](https://www.blackhat.com/docs/us-15/materials/us-15-Wang-The-Applications-Of-Deep-Learning-On-Traffic-Identification-wp.pdf" target="_blank" rel="noopener">https://www.blackhat.com/docs/us-15/materials/us-15-Wang-The-Applications-Of-Deep-Learning-On-Traffic-Identification.pdf),[material](https://www.blackhat.com/docs/us-15/materials/us-15-Wang-The-Applications-Of-Deep-Learning-On-Traffic-Identification-wp.pdf</a>)</p><ul><li><a href="http://www.librec.net/" target="_blank" rel="noopener">《LibRec：A Java Library for Recommender Systems》</a></li></ul><p>介绍:一个推荐系统的Java库</p><ul><li><a href="http://arxiv.org/abs/1512.07372" target="_blank" rel="noopener">《Multi-centrality Graph Spectral Decompositions and their Application to Cyber Intrusion Detection》</a></li></ul><p>介绍:多中心图的谱分解及其在网络入侵检测中的应用(MC-GPCA&amp;MC-GDL)</p><ul><li><a href="http://people.duke.edu/~ccc14/sta-663/" target="_blank" rel="noopener">《Computational Statistics in Python》</a></li></ul><p>介绍:用Python学计算统计学</p><ul><li><a href="http://blog.datumbox.com/new-open-source-machine-learning-framework-written-in-java/" target="_blank" rel="noopener">《New open-source Machine Learning Framework written in Java》</a></li></ul><p>介绍:datumbox-framework——Java的开源机器学习框架，该框架重点是提供大量的机器学习算法和统计检验，并能够处理中小规模的数据集</p><ul><li><a href="http://jiwonkim.org/awesome-rnn/" target="_blank" rel="noopener">《Awesome Recurrent Neural Networks》</a></li></ul><p>介绍:递归神经网络awesome系列,涵盖了书籍,项目,paper等</p><ul><li><a href="http://homes.cs.washington.edu/~pedrod/" target="_blank" rel="noopener">《Pedro Domingos》</a></li></ul><p>介绍:Pedro Domingos是华盛顿大学的教授,主要研究方向是机器学习与数据挖掘.在2015年的ACM webinar会议,曾发表了关于<a href="http://www.almosthuman.cn/2015/11/28/t8ysa/" target="_blank" rel="noopener">盘点机器学习领域的五大流派</a>主题演讲.他的个人主页拥有很多相关研究的paper以及他的教授课程.</p><ul><li><a href="http://dustintran.com/blog/video-resources-for-machine-learning/" target="_blank" rel="noopener">《Video resources for machine learning》</a></li></ul><p>介绍:机器学习视频集锦</p><ul><li><a href="https://medium.com/@abduljaleel/deep-machine-learning-libraries-and-frameworks-5fdf2bb6bfbe#.lwn2iyjsn" target="_blank" rel="noopener">《Deep Machine Learning libraries and frameworks》</a></li></ul><p>介绍:深度机器学习库与框架</p><ul><li><a href="https://github.com/Flowerowl/Big-Data-Resources" target="_blank" rel="noopener">《大数据/数据挖掘/推荐系统/机器学习相关资源》</a></li></ul><p>介绍:这篇文章内的推荐系统资源很丰富,作者很有心,摘录了《推荐系统实战》内引用的论文.</p><ul><li><a href="http://nbviewer.ipython.org/github/jakevdp/AAS227Workshop/blob/master/Index.ipynb" target="_blank" rel="noopener">《Bayesian Methods in Astronomy: Hands-on Statistics》</a></li></ul><p>介绍:(天文学)贝叶斯方法/MCMC教程——统计实战</p><ul><li><a href="http://web.stanford.edu/~hastie/StatLearnSparsity/index.html" target="_blank" rel="noopener">《Statistical Learning with Sparsity: The Lasso and Generalizations》</a></li></ul><p>介绍:免费书:统计稀疏学习,作者<a href="http://web.stanford.edu/~hastie/" target="_blank" rel="noopener">Trevor Hastie</a>与<a href="http://statweb.stanford.edu/~tibs/" target="_blank" rel="noopener">Rob Tibshirani</a>都是斯坦福大学的教授,Trevor Hastie更是在统计学学习上建树很多</p><ul><li><a href="http://www.mango-solutions.com/wp/2016/01/the-evolution-of-distributed-programming-in-r/" target="_blank" rel="noopener">《The Evolution of Distributed Programming in R》</a></li></ul><p>介绍:R分布式计算的进化,此外推荐<a href="https://aschinchon.wordpress.com/2016/01/07/climatic-change-at-a-glance/" target="_blank" rel="noopener">(R)气候变化可视化</a>,<a href="http://blog.revolutionanalytics.com/2016/01/getting-started-with-markov-chains.html" target="_blank" rel="noopener">(R)马尔可夫链入门</a></p><ul><li><a href="http://www.nervanasys.com/neon-workshop-at-startup-ml-sentiment-analysis-and-deep-reinforcement-learning/" target="_blank" rel="noopener">《neon workshop at Startup.ML: Sentiment Analysis and Deep Reinforcement Learning》</a></li></ul><p>介绍:Nervana Systems在<a href="http://startup.ml/" target="_blank" rel="noopener">Startup.ML</a>的主题研讨会——情感分析与深度强化学习</p><ul><li><a href="http://timdettmers.com/2015/03/26/convolution-deep-learning/" target="_blank" rel="noopener">《Understanding Convolution in Deep Learning》</a></li></ul><p>介绍:深度学习卷积概念详解.</p><ul><li><a href="http://faroba.com/2015/12/03/a-python-libraries-for-building-recommender-systems/" target="_blank" rel="noopener">《Python libraries for building recommender systems》</a></li></ul><p>介绍:Python推荐系统开发库汇总.</p><ul><li><a href="http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html" target="_blank" rel="noopener">《Neural networks class - Université de Sherbrooke》</a></li></ul><p>介绍:超棒的神经网络课程，深入浅出介绍深度学习，由Hugo Larochelle（Yoshua Bengio的博士生，Geoffrey Hinton之前的博士后）主讲，强烈推荐.</p><ul><li><a href="http://vision.stanford.edu/teaching/cs231n/index.html" target="_blank" rel="noopener">《CS231n: Convolutional Neural Networks for Visual Recognition》</a></li></ul><p>介绍:斯坦福新课程,面向视觉识别的卷积神经网络(Fei-Fei Li &amp; Andrej Karpathy),<a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" target="_blank" rel="noopener">slides+video</a>,<a href="http://cs231n.github.io/" target="_blank" rel="noopener">homework</a>.</p><ul><li><a href="http://yanran.li/peppypapers/2015/12/11/nips-2015-deep-learning-symposium-part-i.html" target="_blank" rel="noopener">《NIPS 2015 Deep Learning Symposium Part I》</a></li></ul><p>介绍:NIPS 2015会议总结第一部分,<a href="http://yanran.li/peppypapers/2016/01/09/nips-2015-deep-learning-symposium-part-ii.html" target="_blank" rel="noopener">第二部分</a>.</p><ul><li><a href="http://michaelxiang.me/2015/12/16/python-machine-learning-list/" target="_blank" rel="noopener">《python机器学习入门资料梳理》</a></li></ul><p>介绍:python机器学习入门资料梳理.</p><ul><li><a href="http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/" target="_blank" rel="noopener">《Reading Text in the Wild with Convolutional Neural Networks》</a></li></ul><p>介绍:牛津大学著名视觉几何组VGG在IJCV16年首卷首期: Reading Text in the Wild with Convolutional Neural Networks,Jaderberg。这篇期刊文章融合了之前两篇会议(ECCV14,NIPS14ws)，定位和识别图片中的文本(叫text spotting)。 端到端系统: 检测Region + 识别CNN。论文、数据和代码.</p><ul><li><a href="http://riemenschneider.hayko.at/vision/dataset/" target="_blank" rel="noopener">《Yet Another Computer Vision Index To Datasets (YACVID)》</a></li></ul><p>介绍:计算机视觉的一个较大的数据集索引, 包含387个标签，共收录了314个数据集合，点击标签云就可以找到自己需要的库了.</p><ul><li><a href="http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html" target="_blank" rel="noopener">《Why SLAM Matters, The Future of Real-Time SLAM, and Deep Learning vs SLAM》</a></li></ul><p>介绍:Tombone 对 ICCV SLAM workshop 的总结： the future of SLAM, SLAM vs deep learning  重点介绍了 monoSLAM 和 LSD-SLAM，而且讨论了 feature-based 和 feature-free method 的长短。在全民deep learning做visual perception的时候，再来读读CV中的 geometry.</p><ul><li><a href="https://github.com/NervanaSystems/neon" target="_blank" rel="noopener">《Python based Deep Learning Framework by Nervana™》</a></li></ul><p>介绍:Nervana Systems的开源深度学习框架neon发布.</p><ul><li><a href="http://image-net.org/challenges/ilsvrc+mscoco2015" target="_blank" rel="noopener">《mageNet and MS COCO Visual Recognition Challenges video and slider》</a></li></ul><p>介绍:ICCV 2015的ImageNet比赛以及MS COCO竞赛联合研讨会的幻灯片和视频.</p><ul><li><a href="http://blog.districtdatalabs.com/an-introduction-to-machine-learning-with-python" target="_blank" rel="noopener">《An Introduction to Machine Learning with Python》</a></li></ul><p>介绍:Python机器学习入门.</p><ul><li><a href="http://arxiv.org/abs/1512.00965" target="_blank" rel="noopener">《Neural Enquirer: Learning to Query Tables with Natural Language》</a></li></ul><p>介绍:Neural Enquirer 第二版.</p><ul><li><a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank" rel="noopener">《Deep Learning - Taking machine learning to the next level》</a></li></ul><p>介绍:[Google]基于TensorFlow的深度学习/机器学习课程.</p><ul><li><a href="http://www.r-bloggers.com/100-must-read-r-bloggers-posts-for-2015/" target="_blank" rel="noopener">《100 “must read” R-bloggers’ posts for 2015》</a></li></ul><p>介绍:R-bloggers网站2015”必读”的100篇文章,R语言学习的福音.</p><ul><li><a href="http://www.cs.ubc.ca/~murphyk/MLbook/index.html" target="_blank" rel="noopener">《Machine Learning: a Probabilistic Perspective》</a></li></ul><p>介绍:推荐书籍:&lt;机器学习：概率视角&gt;,样章<a href="http://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf" target="_blank" rel="noopener">Undirected graphical models Markov random fields</a>.</p><ul><li><a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">《Deep learning Book》</a></li></ul><p>介绍:这是一本在线的深度学习书籍,合著者有Ian Goodfellow, Yoshua Bengio 和 Aaron Courville.如果你是一位新入门的学员可以先看这本书籍<a href="https://www.quora.com/How-can-one-get-started-with-machine-learning-1" target="_blank" rel="noopener">Yoshua Bengio: How can one get started with machine learning?</a>.<a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">中文译本</a></p><ul><li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings" target="_blank" rel="noopener">《UFLDL Recommended Readings》</a></li></ul><p>介绍:UFLDL推荐的深度学习阅读列表.</p><ul><li><a href="http://www.cse.buffalo.edu/~hungngo/classes/2015/705/" target="_blank" rel="noopener">《CSE 705: Deep Learning (Spring 2015)》</a></li></ul><p>介绍:纽约州立大学布法罗分校2015年春季机器学习课程主页.</p><ul><li><a href="https://github.com/Theano/Theano" target="_blank" rel="noopener">《Theano is a Deep learning Python library 》</a></li></ul><p>介绍: Theano是主流的深度学习Python库之一，亦支持GPU,入门比较难.推荐<a href="https://github.com/marekrei/theano-tutorial" target="_blank" rel="noopener">Theano tutorial</a>,<a href="http://deeplearning.net/software/theano/tutorial/" target="_blank" rel="noopener">Document</a></p><ul><li><a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf" target="_blank" rel="noopener">《Statistical Language Models Based On Neural Networks》</a></li></ul><p>介绍:博士论文:神经网络统计语言模型.</p><ul><li><a href="http://www.infoq.com/cn/articles/machine-learning-automatic-classification-of-text-data" target="_blank" rel="noopener">《文本数据的机器学习自动分类方法(上)》</a></li></ul><p>介绍:<a href="http://www.infoq.com/cn/articles/machine-learning-automatic-classification-of-text-data-part2" target="_blank" rel="noopener">文本数据的机器学习自动分类方法(下)</a>.</p><ul><li><a href="http://arxiv.org/abs/1601.06759" target="_blank" rel="noopener">《Pixel Recurrent Neural Networks》</a></li></ul><p>介绍:用RNN预测像素，可以把被遮挡的图片补充完整.</p><ul><li><a href="https://github.com/Microsoft/CNTK" target="_blank" rel="noopener">《Computational Network Toolkit (CNTK)》</a></li></ul><p>介绍:微软研究院把其深度学习工具包CNTK,想进一步了解和学习CNTK的同学可以看前几天公布的《CNTK白皮书》<a href="http://research.microsoft.com/pubs/226641/CNTKBook-20160121.pdf" target="_blank" rel="noopener">An Introduction to Computational Networks and the Computational Network Toolkit</a>.</p><ul><li><a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" target="_blank" rel="noopener">《Kalman and Bayesian Filters in Python》</a></li></ul><p>介绍: 卡尔曼滤波器教材，用尽量少的数学和推导，传授直觉和经验，全部Python示例，内容覆盖卡尔曼滤波器、扩展卡尔曼滤波，无迹卡尔曼滤波等，包括练习和参考答案</p><ul><li><a href="https://leanpub.com/LittleInferenceBook" target="_blank" rel="noopener">《Statistical inference for data science》</a></li></ul><p>介绍:在线免费书:面向数据科学的统计推断，R示例代码，很不错<a href="https://github.com/bcaffo/LittleInferenceBook" target="_blank" rel="noopener">GitHub</a>.</p><ul><li><a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf" target="_blank" rel="noopener">《Learning Deep Architectures for AI》</a></li></ul><p>介绍:这本书是由Yoshua Bengio撰写的教程,其内容包含了学习人工智能所使用的深度学习架构的学习资源,书中的项目<code>已停止更新</code><a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" target="_blank" rel="noopener">DeepLearnToolbox</a>.</p><ul><li><a href="https://github.com/ujjwalkarn/Machine-Learning-Tutorials" target="_blank" rel="noopener">《Machine Learning Tutorials》</a></li></ul><p>介绍:这是一份机器学习和深度学习教程，文章和资源的清单。这张清单根据各个主题进行撰写，包括了许多与深度学习有关的类别、计算机视觉、加强学习以及各种架构.</p><ul><li><a href="https://github.com/donnemartin/data-science-ipython-notebooks" target="_blank" rel="noopener">《Data science ipython notebooks》</a></li></ul><p>介绍:这是由Donne Martin策划收集的IPython笔记本。话题涵盖大数据、Hadoop、scikit-learn和科学Python堆栈以及很多其他方面的内容。至于深度学习，像是TensorFlow、Theano和Caffe之类的框架也均被涵盖其中，当然还有相关的特定构架和概念等.</p><ul><li><a href="http://www.deepdetect.com/" target="_blank" rel="noopener">《Open Source Deep Learning Server》</a></li></ul><p>介绍:开源的深度学习服务,DeepDetect是C++实现的基于外部机器学习/深度学习库（目前是Caffe）的API。给出了图片训练（ILSVRC）和文本训练（基于字的情感分析，NIPS15）的样例，以及根据图片标签索引到ElasticSearch中<a href="https://github.com/beniz/deepdetect" target="_blank" rel="noopener">github</a>.</p><ul><li><a href="http://www.kdnuggets.com/" target="_blank" rel="noopener">《Data Mining, Analytics, Big Data, and Data Science》</a></li></ul><p>介绍:这是国外的一个科技频道,涵盖了数据挖掘,分析以及数据科学类的文章.偶尔还有机器学习精选.</p><ul><li><a href="http://docs.salford-systems.com/dm-stat.pdf" target="_blank" rel="noopener">《Data Mining And Statistics: What’s The Connection?》</a></li></ul><p>介绍:经典论文:数据挖掘与统计学.</p><ul><li><a href="https://drive.google.com/file/d/0BxKBnD5y2M8NVnBpbWVwYUpQTjg/view" target="_blank" rel="noopener">《(NIPS’2015 Tutorial)Yoshua Bengio深度学习》</a></li></ul><p>介绍:NIPS’2015 Tutorial by Yoshua Bengio.</p><ul><li><a href="https://github.com/NervanaSystems/neon" target="_blank" rel="noopener">《(NENO:Python based Deep Learning Framework》</a></li></ul><p>介绍:Nervana Systems的开源深度学习框架neon发布.</p><ul><li><a href="http://matt.might.net/articles/books-papers-materials-for-graduate-students/" target="_blank" rel="noopener">《(Matt Might:Reading for graduate students》</a></li></ul><p>介绍:犹他州大学Matt Might教授推荐的研究生阅读清单.</p><ul><li><a href="https://github.com/caesar0301/awesome-public-datasets" target="_blank" rel="noopener">《Awesome Public Datasets》</a></li></ul><p>介绍:开放数据集.</p><ul><li><a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-1" target="_blank" rel="noopener">《Introduction to Probability - The Science of Uncertainty》</a></li></ul><p>介绍:(edX)不确定性的科学——概率论导论(MITx).</p><ul><li><a href="http://xrds.acm.org/blog/2016/02/r-software-and-tools-for-everyday-use/" target="_blank" rel="noopener">《R software and tools for everyday use》</a></li></ul><p>介绍:R语言开发常用软件/工具推荐.</p><ul><li><a href="http://yerevann.github.io//2016/02/05/implementing-dynamic-memory-networks/" target="_blank" rel="noopener">《Implementing Dynamic memory networks》</a></li></ul><p>介绍:动态记忆网络实现.</p><ul><li><a href="http://deeplearning4j.org/zh-index.html/" target="_blank" rel="noopener">《Deeplearning4j 中文主页》</a></li></ul><p>介绍:英文<a href="http://deeplearning4j.org" target="_blank" rel="noopener">主页</a></p><ul><li><a href="http://www.ngdata.com/big-data-analysis-resources/" target="_blank" rel="noopener">《Big Data Analysis Learning Resources: 50 Courses, Blogs, Tutorials, And More For Mastering Big Data Analytics》</a></li></ul><p>介绍:50个大数据分析最佳学习资源(课程、博客、教程等)</p><ul><li><a href="http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/" target="_blank" rel="noopener">《A Full Hardware Guide to Deep Learning》</a></li></ul><p>介绍:深度学习的全面硬件指南，从GPU到RAM、CPU、SSD、PCIe,<a href="http://www.almosthuman.cn/2016/02/04/bqrzz/" target="_blank" rel="noopener">译文</a></p><ul><li><a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="noopener">《Deep Residual Networks》</a></li></ul><p>介绍:kaiming开源作品</p><ul><li><a href="https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/" target="_blank" rel="noopener">《The Definitive Guide to Natural Language Processing》</a></li></ul><p>介绍:自然语言处理(NLP)权威指南</p><ul><li><a href="https://blog.twitter.com/2015/evaluating-language-identification-performance" target="_blank" rel="noopener">《Evaluating language identification performance》</a></li></ul><p>介绍:如何在社会媒体上做语言检测？没有数据怎么办？推特官方公布了一个十分难得的数据集：12万标注过的Tweets，有70种语言</p><ul><li><a href="http://www.iclr.cc/doku.php?id=iclr2016:main&amp;#accepted_papers_conference_track" target="_blank" rel="noopener">《ICLR 2016 Accepted Papers》</a></li></ul><p>介绍:深度学习和机器学习重要会议ICLR 2016录取文章</p><ul><li><a href="http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide/" target="_blank" rel="noopener">《Machine Learning: An In-Depth, Non-Technical Guide - Part 1》</a></li></ul><p>介绍:机器学习——深度非技术指南</p><ul><li><a href="http://blog.hubspot.com/marketing/toolbox-for-data-storytelling" target="_blank" rel="noopener">《Data Storytelling 101: Helpful Tools for Gathering Ideas, Designing Content &amp; More》</a></li></ul><p>介绍:数据叙事入门指南——创意生成/数据采集/内容设计相关资源推荐</p><ul><li><a href="http://nlp.stanford.edu/blog/wikitablequestions-a-complex-real-world-question-understanding-dataset/" target="_blank" rel="noopener">《WikiTableQuestions: a Complex Real-World Question Understanding Dataset》</a></li></ul><p>介绍:WikiTableQuestions——复杂真实问答数据集</p><ul><li><a href="http://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#64ede4f16796" target="_blank" rel="noopener">《Big Data: 35 Brilliant And Free Data Sources For 2016》</a></li></ul><p>介绍:(2016版)35个超棒的免费大数据源</p><ul><li><a href="http://arxiv.org/pdf/1511.06051v3.pdf" target="_blank" rel="noopener">《SPARKNET: training deep networks in spark》</a></li></ul><p>介绍:Ion Stoica和 Michael I. Jordan两位大家首次联手发文，CAFFE和SPARK完美结合，分布式深度学习混搭模式！<a href="https://github.com/amplab/SparkNet" target="_blank" rel="noopener">github</a></p><ul><li><a href="http://memkite.com/deep-learning-bibliography/" target="_blank" rel="noopener">《DeepLearning.University – An Annotated Deep Learning Bibliography | Memkite》</a></li></ul><p>介绍:深度学习(分类)文献集</p><ul><li><a href="http://rt.dgyblog.com/ref/ref-learning-deep-learning.html" target="_blank" rel="noopener">《Learning Deep Learning》</a></li></ul><p>介绍:深度学习阅读列表</p><ul><li><a href="http://awesome42.com/" target="_blank" rel="noopener">《Awesome42 The easiest way to find R packages》</a></li></ul><p>介绍:探索R包的好网站Awesome 42</p><ul><li><a href="http://mlbase.org/" target="_blank" rel="noopener">《MLbase:Distributed Machine Learning Made Easy》</a></li></ul><p>介绍:MLbase是<a href="http://cs.brown.edu/~kraskat/" target="_blank" rel="noopener">Prof. Dr. Tim Kraska</a>的一个研究项目，MLbase是一个分布式机器学习管理系统</p><ul><li><a href="http://www.comp.nus.edu.sg/~ooibc/singa-tomm.pdf" target="_blank" rel="noopener">《Deep Learning At Scale and At Ease》</a></li></ul><p>介绍:分布式深度学习平台<a href="http://singa.incubator.apache.org/index.html" target="_blank" rel="noopener">SINGA</a>介绍</p><ul><li><a href="http://datasciencereport.com/2016/02/19/apache-spark/" target="_blank" rel="noopener">《Learn All About Apache Spark (100x Faster than Hadoop MapReduce)》</a></li></ul><p>介绍:Spark视频集锦</p><ul><li><a href="http://www.parallelr.com/r-deep-neural-network-from-scratch/" target="_blank" rel="noopener">《R For Deep Learning (I): Build Fully Connected Neural Network From Scratch》</a></li></ul><p>介绍:R语言深度学习第一节:从零开始</p><ul><li><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" target="_blank" rel="noopener">《A Visual Introduction to Machine Learning》</a></li></ul><p>介绍:图解机器学习</p><ul><li><a href="http://aminer.org/citation" target="_blank" rel="noopener">《Citation Network Dataset》</a></li></ul><p>介绍:AMiner论文引用数据集(v7:2,244,021 papers and 4,354,534 citation relationships)</p><ul><li><a href="https://www.reddit.com/r/MachineLearning/comments/47ast8/best_free_machine_learning_ebooks/" target="_blank" rel="noopener">《Best Free Machine Learning Ebooks》</a></li></ul><p>介绍:10本最佳机器学习免费书</p><ul><li><a href="http://videolectures.net/iccv2015_santiago/" target="_blank" rel="noopener">《International Conference on Computer Vision (ICCV) 2015, Santiago》</a></li></ul><p>介绍:ICCV15视频集</p><ul><li><a href="https://github.com/yahoo/CaffeOnSpark" target="_blank" rel="noopener">《CaffeOnSpark Open Sourced for Distributed Deep Learning on Big Data Clusters》</a></li></ul><p>介绍::(Yahoo)基于Hadoop/Spark的分布式Caffe实现CaffeOnSpark</p><ul><li><a href="http://research.microsoft.com/en-us/people/hangli/l2r.pdf" target="_blank" rel="noopener">《A Short Introduction to Learning to Rank》</a></li></ul><p>介绍:Learning to Rank简介</p><ul><li><a href="https://aminer.org/search/t=b&amp;q=Deep%20Learning" target="_blank" rel="noopener">《Global Deep learning researcher》</a></li></ul><p>介绍:全球深度学习专家列表,涵盖研究者主页</p><ul><li><a href="http://www.kdnuggets.com/2016/03/top-spark-ecosystem-projects.html" target="_blank" rel="noopener">《Top Spark Ecosystem Projects》</a></li></ul><p>介绍:<a href="http://www.infoq.com/cn/news/2016/03/spark-eco-project" target="_blank" rel="noopener">Spark生态顶级项目汇总</a></p><ul><li><a href="http://dl.acm.org/citation.cfm?id=2856767&amp;preflayout=flat" target="_blank" rel="noopener">《Proceedings of the 21st International Conference on Intelligent User Interfaces》</a></li></ul><p>介绍:<a href="http://iui.acm.org/2016/" target="_blank" rel="noopener">ACM IUI’16</a>论文集<a href="http://halley.exp.sis.pitt.edu/cn3/proceedingswithauthors.php?conferenceID=139" target="_blank" rel="noopener">Conference Navigator - Proceedings</a></p><ul><li><a href="http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide/" target="_blank" rel="noopener">《Machine Learning: An In-Depth, Non-Technical Guide - Part 1》</a></li></ul><p>介绍:深入机器学习,<a href="http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-2/" target="_blank" rel="noopener">2</a>,<a href="http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-3/" target="_blank" rel="noopener">3</a>,<a href="http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-4/" target="_blank" rel="noopener">4</a></p><ul><li><a href="http://www.computervisiontalks.com/tag/deep-learning-course/" target="_blank" rel="noopener">《Oxford Deep Learning》</a></li></ul><p>介绍:<a href="https://www.cs.ox.ac.uk/people/nando.defreitas/" target="_blank" rel="noopener">Nando de Freitas</a>在 Oxford 开设的深度学习课程,<a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu" target="_blank" rel="noopener">课程youtube地址</a>,Google DeepMind的研究科学家,此外<a href="http://www.computervisiontalks.com/" target="_blank" rel="noopener">首页:computervisiontalks</a>的内容也很丰富,如果你是做机器视觉方面的研究,推荐也看看其他内容.肯定收获也不小.还有,这位<a href="https://www.youtube.com/channel/UC0z_jCi0XWqI8awUuQRFnyw" target="_blank" rel="noopener">youtube主页</a>顶过的视频也很有份量</p><ul><li><a href="https://www.coursera.org/course/neuralnets" target="_blank" rel="noopener">《Neural Networks for Machine Learning》</a></li></ul><p>介绍:Geoffrey Hinton在Coursera开设的MOOC</p><ul><li><a href="http://news.startup.ml/" target="_blank" rel="noopener">《Deep Learning News》</a></li></ul><p>介绍:深度学习领域的Hacker news.紧跟深度学习的新闻、研究进展和相关的创业项目。从事机器学习,深度学习领域的朋友建议每天看一看</p><ul><li><a href="http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf" target="_blank" rel="noopener">《Maxout Networks》</a></li></ul><p>介绍:Maxout网络剖析</p><ul><li><a href="http://papers.nips.cc/" target="_blank" rel="noopener">《Advances in Neural Information Processing Systems》</a></li></ul><p>介绍:NIPS领域的会议paper集锦</p><ul><li><a href="http://www.nature.com/nrg/journal/v16/n6/abs/nrg3920.html" target="_blank" rel="noopener">《Machine learning applications in genetics and genomics》</a></li></ul><p>介绍:机器学习在生物工程领域的应用,如果你从事生物工程领域,可以先阅读一篇文章<a href="https://www.zhihu.com/question/41428117/answer/91045285" target="_blank" rel="noopener">详细介绍</a></p><ul><li><a href="http://arxiv.org/abs/1603.06430" target="_blank" rel="noopener">《Deep Learning in Bioinformatics》</a></li></ul><p>介绍:深度学习在生物信息学领域的应用</p><ul><li><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank" rel="noopener">《A Few Useful Things to Know about Machine Learning》</a></li></ul><p>介绍:一些关于机器学习需要知道知识,对于刚刚入门机器学习的同学应该读一读</p><ul><li><a href="http://mlg.eng.cam.ac.uk/" target="_blank" rel="noopener">《Cambridge Machine Learning Group》</a></li></ul><p>介绍:剑桥大学机器学习用户组主页,网罗了剑桥大学一些机器学习领域专家与新闻</p><ul><li><a href="https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects" target="_blank" rel="noopener">《Randy Olson’s data analysis and machine learning projects》</a></li></ul><p>介绍:<a href="http://www.randalolson.com/blog/" target="_blank" rel="noopener">Randy Olson’s</a>的一些数据分析与机器学习项目库,是学习实践的好材料</p><ul><li><a href="https://github.com/sjwhitworth/golearn" target="_blank" rel="noopener">《GoLearn:Golang machine learning library》</a></li></ul><p>介绍:Golang机器学习库,简单,易扩展</p><ul><li><a href="https://github.com/collinhundley/Swift-AI" target="_blank" rel="noopener">《Swift Ai》</a></li></ul><p>介绍:用Swift开发苹果应用的倒是很多，而用来做机器学习的就比较少了.Swift Ai在这方面做了很多聚集.可以看看</p><ul><li><a href="https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/" target="_blank" rel="noopener">《Please explain Support Vector Machines (SVM) like I am a 5 year old》</a></li></ul><p>介绍:如何向一位5岁的小朋友解释支持向量机(SVM)</p><ul><li><a href="https://www.reddit.com/r/MachineLearning/" target="_blank" rel="noopener">《reddit Machine learning》</a></li></ul><p>介绍: reddit的机器学习栏目</p><ul><li><a href="http://blog.csdn.net/carson2005/article/details/6601109" target="_blank" rel="noopener">《ComputerVision resource》</a></li></ul><p>介绍: 计算机视觉领域的一些牛人博客，超有实力的研究机构等的网站链接.做计算机视觉方向的朋友建议多关注里面的资源</p><ul><li><a href="http://mmlab.ie.cuhk.edu.hk/index.html" target="_blank" rel="noopener">《Multimedia Laboratory Homepage》</a></li></ul><p>介绍:香港中文大学深度学习研究主页,此外研究小组对<a href="http://mmlab.ie.cuhk.edu.hk/project_deep_learning.html" target="_blank" rel="noopener">2013年deep learning 的最新进展和相关论文</a>做了整理,其中useful links的内容很受益</p><ul><li><a href="http://www.anneschuth.nl/wp-content/uploads/thesis_anne-schuth_search-engines-that-learn-from-their-users.pdf" target="_blank" rel="noopener">《Search Engines that Learn from Their Users》</a></li></ul><p>介绍: 这是一篇关于搜索引擎的博士论文,对现在普遍使用的搜索引擎google，bing等做了分析.对于做搜索类产品的很有技术参考价值</p><ul><li><a href="http://machinelearningmastery.com/deep-learning-books/" target="_blank" rel="noopener">《Deep Learning Books》</a></li></ul><p>介绍: 深度学习书籍推荐(毕竟这类书比较少).</p><ul><li><a href="http://arxiv.org/abs/1604.01662" target="_blank" rel="noopener">《Towards Bayesian Deep Learning: A Survey》</a></li></ul><p>介绍: 贝叶斯定理在深度学习方面的研究论文.</p><ul><li><a href="http://arxiv.org/abs/1604.00981" target="_blank" rel="noopener">《Revisiting Distributed Synchronous SGD》</a></li></ul><p>介绍: 来自谷歌大脑的重温分布式梯度下降.同时推荐<a href="http://wxwidget.github.io/blog/2014/08/17/large-scale-deep-network/" target="_blank" rel="noopener">大规模分布式深度网络</a></p><ul><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.6931&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">《Research Issues in Social Computing》</a></li></ul><p>介绍: 社交计算研究相关问题综述.</p><ul><li><a href="https://www.quora.com/What-are-some-important-areas-of-research-in-social-computing-right-now" target="_blank" rel="noopener">《What are some important areas of research in social computing right now?》</a></li></ul><p>介绍: 社交计算应用领域概览,里面有些经典论文推荐</p><ul><li><a href="http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf" target="_blank" rel="noopener">《Collaborative Filtering Recommender Systems》</a></li></ul><p>介绍: 协同过滤在推荐系统应用.</p><ul><li><a href="http://www.cs.utexas.edu/~ml/papers/cbcf-aaai-02.pdf" target="_blank" rel="noopener">《Content-Boosted Collaborative Filtering for Improved Recommendations》</a></li></ul><p>介绍: 协同过滤在内容推荐的研究.</p><ul><li><a href="http://siplab.tudelft.nl/sites/default/files/sigir06_similarityfusion.pdf" target="_blank" rel="noopener">《Unifying User-based and Item-based Collaborative Filtering Approaches by Similarity Fusion》</a></li></ul><p>介绍: 协同过滤经典论文.</p><ul><li><a href="http://glaros.dtc.umn.edu/gkhome/fetch/papers/www10_sarwar.pdf" target="_blank" rel="noopener">《Item-based Collaborative Filtering Recommendation Algorithms》</a></li></ul><p>介绍: 协同过滤算法.</p><ul><li><a href="http://www.cin.ufpe.br/~idal/rs/Amazon-Recommendations.pdf" target="_blank" rel="noopener">《Amazon.com Recommendations Item-to-Item Collaborative Filtering》</a></li></ul><p>介绍: 亚马逊对于协同过滤算法应用.</p><ul><li><a href="http://yifanhu.net/PUB/cf.pdf" target="_blank" rel="noopener">《Collaborative Filtering for Implicit Feedback Datasets》</a></li></ul><p>介绍: 协同过滤的隐式反馈数据集处理.</p><ul><li><a href="http://www.iquilezles.org/www/index.htm" target="_blank" rel="noopener">《Tutorials, papers and code for computer graphics, fractals and demoscene》</a></li></ul><p>介绍: 计算机图形，几何等论文，教程，代码.做计算机图形的推荐收藏.</p><ul><li><a href="http://www.columbia.edu/~jw2966/6886_Fa2015.html" target="_blank" rel="noopener">《ELEN 6886 Sparse Representation and High-Dimensional Geometry》</a></li></ul><p>介绍: 推荐哥伦比亚大学课程，稀疏表示和高维几何.12年由Elsevier、13年至今由PAMI(仍由Elsevier赞助)设立的青年研究者奖(Young Researcher Award)授予完成博士学位后七年内取得杰出贡献的；由CV社区提名，在CVPR会议上宣布。2015年得主是哥大助理教授John Wright，09年<a href="http://www.columbia.edu/~jw2966/papers/WYGSM09-PAMI.pdf" target="_blank" rel="noopener">《健壮人脸识别的稀疏表示法》</a>引用已超5K.</p><ul><li><a href="https://www.quora.com/What-would-be-your-advice-to-a-software-engineer-who-wants-to-learn-machine-learning-3/answer/Alex-Smola-1" target="_blank" rel="noopener">《Software engineer how to learning Machine learning》</a></li></ul><p>介绍: CMU机器学习系著名教授Alex Smola在Quora对于《程序员如何学习Machine Learning》的建议：Alex推荐了不少关于线性代数、优化、系统、和统计领域的经典教材和资料.</p><ul><li><a href="http://www.opengardensblog.futuretext.com/archives/2015/08/book-review-fundamentals-of-deep-learning-designing-next-generation-artificial-intelligence-algorithms-by-nikhil-buduma.html" target="_blank" rel="noopener">《Book review: Fundamentals of Deep Learning》</a></li></ul><p>介绍: 书籍推荐,深度学习基础.<a href="https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book" target="_blank" rel="noopener">源码</a></p><ul><li><a href="http://learnbigcode.github.io/" target="_blank" rel="noopener">《Learning from Big Code》</a></li></ul><p>介绍: 软件工程领域现在也对机器学习和自然语言处理很感兴趣，有人推出了“大代码”的概念，分享了不少代码集合，并且觉得ML可以用在预测代码Bug，预测软件行为，自动写新代码等任务上。大代码数据集下载</p><ul><li><a href="http://handong1587.github.io/deep_learning/2015/10/09/object-detection.html" target="_blank" rel="noopener">《Object Detection》</a></li></ul><p>介绍: 深度学习进行目标识别的资源列表：包括RNN、MultiBox、SPP-Net、DeepID-Net、Fast R-CNN、DeepBox、MR-CNN、Faster R-CNN、YOLO、DenseBox、SSD、Inside-Outside Net、G-CNN</p><ul><li><a href="https://www.facebook.com/yann.lecun/posts/10153505343037143" target="_blank" rel="noopener">《Deep Learning: Course by Yann LeCun at Collège de France 2016(Slides in English)》</a></li></ul><p>介绍: Yann LeCun 2016深度学习课程的幻灯片（Deep Learning  Course by Yann LeCun at Collège de France 2016）<a href="http://pan.baidu.com/s/1jIIrljg" target="_blank" rel="noopener">百度云</a>密码: cwsm <a href="https://drive.google.com/folderview?id=0BxKBnD5y2M8NclFWSXNxa0JlZTg&amp;usp=sharing" target="_blank" rel="noopener">原地址</a></p><ul><li><a href="http://hci.stanford.edu/" target="_blank" rel="noopener">《Stanford HCI Group》</a></li></ul><p>介绍: 斯坦福人机交互组五篇CHI16文章。1.众包激励机制的行为经济学研究：批量结算比单任务的完成率高。2.在众包专家和新手间建立联系：微实习。3.词嵌入结合众包验证的词汇主题分类（如猫、狗属于宠物）。4.词嵌入结合目标识别的活动预测。5.鼓励出错以加快众包速度。</p><ul><li><a href="https://github.com/nborwankar/LearnDataScience" target="_blank" rel="noopener">《Learn Data Science》</a></li></ul><p>介绍: 自学数据科学</p><ul><li><a href="https://www.youtube.com/watch?v=L8Y2_Cq2X5s" target="_blank" rel="noopener">《CS224D Lecture 7 - Introduction to TensorFlow》</a></li></ul><p>介绍: 本课是<a href="http://cs224d.stanford.edu/" target="_blank" rel="noopener">CS224D</a>一节介绍TensorFlow课程，<a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf" target="_blank" rel="noopener">ppt</a>,<a href="http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb" target="_blank" rel="noopener">DeepDreaming with TensorFlow</a></p><ul><li><a href="http://autumnai.com/leaf/book/leaf.html" target="_blank" rel="noopener">《Leaf - Machine Learning for Hackers》</a></li></ul><p>介绍: Leaf是一款机器学习的开源框架，专为黑客打造，而非为科学家而作。它用Rust开发，传统的机器学习，现今的深度学习通吃。<a href="https://github.com/autumnai/leaf" target="_blank" rel="noopener">Leaf</a></p><ul><li><a href="http://on-demand.gputechconf.com/gtc/2016/video/S6853.html" target="_blank" rel="noopener">《MXnet:Flexible and Efficient library for deep learning》</a></li></ul><p>介绍: <a href="http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php" target="_blank" rel="noopener">GTC 2016</a>视频，MXnet的手把手深度学习tutorial,相关参考资料<a href="https://github.com/dmlc/mxnet-gtc-tutorial" target="_blank" rel="noopener">MXNet Tutorial for NVidia GTC 2016.</a></p><ul><li><a href="https://gym.openai.com/" target="_blank" rel="noopener">《OpenAI Gym: Toolkit for developing, comparing reinforcement learning algorithms》</a></li></ul><p>介绍: OpenAI Gym：开发、比较强化学习算法工具箱</p><ul><li><a href="https://tensortalk.com/?cat=conference-iclr-2016" target="_blank" rel="noopener">《conference-iclr-2016 Papers and Code》</a></li></ul><p>介绍: 机器学习会议ICLR 2016 论文的代码集合</p><ul><li><a href="https://github.com/JimmyLin192/GraphicalModel/blob/master/Probabilistic%20Graphical%20Models%20Principles%20and%20Techniques.pdf" target="_blank" rel="noopener">《probabilistic graphical models principles and techniques》</a></li></ul><p>介绍: 此书是斯坦福大学概率图模型大牛Daphne Koller所写，主要涉及的是贝叶斯网络和马尔科夫逻辑网络的learning和inference问题，同时又对PGM有深刻的理论解释，是学习概率图模型必看的书籍。难度中上，适合有一些ML基础的研究生.[备份地址](<a href="https://vk.com/doc168073_304660839?hash=39a33dd8aa6b141d8a&amp;dl=b6674" target="_blank" rel="noopener">https://vk.com/doc168073_304660839?hash=39a33dd8aa6b141d8a&amp;dl=b6674</a></p><ul><li><a href="https://github.com/intel-analytics/BigDL" target="_blank" rel="noopener">《BigDL: Distributed Deep learning on Apache Spark》</a></li></ul><p>介绍: Spark分布式深度学习库BigDL</p><ul><li><a href="http://www.kdnuggets.com/2017/01/machine-learning-cyber-security.html" target="_blank" rel="noopener">《Machine Learning and Cyber Security Resources》</a></li></ul><p>介绍: 这是一份关于机器学习和数据挖掘在网络安全方面应用的资源帖，包含了一些重要的站点，论文，书籍,斯坦福课程以及一些有用的教程.</p><ul><li><a href="http://selfdrivingcars.mit.edu/" target="_blank" rel="noopener">《6.S094: Deep Learning for Self-Driving Cars》</a></li></ul><p>介绍: 麻省理工学院（MIT）开设课程.S094：自主驾驶汽车的深度学习</p><ul><li><a href="http://techtalks.tv/icml/2016/" target="_blank" rel="noopener">《ICML 2016 Conference and Workshops Video》</a></li></ul><p>介绍: ICML 2016视频集锦</p><ul><li><a href="https://github.com/JustFollowUs/Machine-Learning" target="_blank" rel="noopener">《机器学习Machine-Learning》</a></li></ul><p>介绍: 机器学习推荐学习路线及参考资料</p><ul><li><a href="https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist" target="_blank" rel="noopener">《TensorFlow and deep learning, without a PhD》</a></li></ul><p>介绍:新手入门，通过TensorFlow入门深度学习</p><ul><li><a href="https://blog.ycombinator.com/how-to-get-into-natural-language-processing/" target="_blank" rel="noopener">《How To Get Into Natural Language Processing》</a></li></ul><p>介绍: 自然语言处理(NLP)入门指南</p><ul><li><a href="https://arxiv.org/abs/1702.01361" target="_blank" rel="noopener">《Deep learning and the Schrödinger equation》</a></li></ul><p>介绍:通过神经网络跳过数值方法求解薛定谔方程。</p><ul><li><a href="http://www.dmtk.io/slides/distributedML-aaai2017.pdf" target="_blank" rel="noopener">《Recent Advances in Distributed Machine Learning》</a></li></ul><p>介绍:微软亚洲研究院的刘铁岩等人近日在AAAI 2017上做的有关优化以及大规模机器学习的Tutorial。很值得一看。里面对传统的优化算法，特别是一些理论特性以及分布式算法的相应理论特性都有一个比较详尽的总结。非常适合想快速了解这些领域的学者和工程师。另外，这个Tutorial还介绍了DMTK的一些情况，作为一个分布式计算平台的优缺点，还顺带比较了Spark和TensorFlow等流行框架。</p><ul><li><a href="https://sites.google.com/site/dliftutorial/" target="_blank" rel="noopener">《Deep Learning Implementations and Frameworks (DLIF)》</a></li></ul><p>介绍:AAAI 2017的Tutorial，专门讲述了深度学习框架的设计思想和实现，比较若干种流行框架（Caffe、MXNet、TensorFlow、Chainer等）的性能和异同。 </p><ul><li><a href="https://github.com/yahoo/TensorFlowOnSpark" target="_blank" rel="noopener">《Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters》</a></li></ul><p>介绍:雅虎开源基于spark与TensorFlow的分布式数据深度学习框架,博文<a href="https://yahooeng.tumblr.com/post/157196488076/open-sourcing-tensorflowonspark-distributed-deep" target="_blank" rel="noopener">介绍</a></p><ul><li><a href="http://r2rt.com/deconstruction-with-discrete-embeddings.html" target="_blank" rel="noopener">《Deconstruction with Discrete Embeddings》</a></li></ul><p>介绍:用离散嵌入解构模糊数据</p><ul><li><a href="https://sites.google.com/site/wildml2016nips/schedule" target="_blank" rel="noopener">《Reliable Machine Learning in the Wild - NIPS 2016 Workshop》</a></li></ul><p>介绍:视频发布：自然场景可靠机器学习(NIPS 2016 Workshop)</p><ul><li><a href="https://research.google.com/audioset/" target="_blank" rel="noopener">《A large-scale dataset of manually annotated audio events》</a></li></ul><p>介绍:Google发布大规模音频数据集</p><ul><li><a href="https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network" target="_blank" rel="noopener">《5 algorithms to train a neural network》</a></li></ul><p>介绍:训练神经网络的5种算法</p><ul><li><a href="https://github.com/stanfordnlp/cs224n-winter17-notes" target="_blank" rel="noopener">《Course notes for CS224N Winter17》</a></li></ul><p>介绍:笔记：斯坦福CS224n深度学习NLP课程(2017)</p><ul><li><a href="https://github.com/telecombcn-dl/2017-persontyle" target="_blank" rel="noopener">《Persontyle Workshop for Applied Deep Learning》</a></li></ul><p>介绍:伦敦深度学习研讨会资料</p><ul><li><a href="https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/" target="_blank" rel="noopener">《Understanding, generalisation, and transfer learning in deep neural networks》</a></li></ul><p>介绍:论文导读：深度神经网络理解、泛化与迁移学习,<a href="https://blog.acolyer.org/" target="_blank" rel="noopener">acolyer blog</a>上还有很多经典推荐可以阅读</p><ul><li><a href="http://www.cs.princeton.edu/courses/archive/spr06/cos598C/papers/AndrieuFreitasDoucetJordan2003.pdf" target="_blank" rel="noopener">《An Introduction to MCMC for Machine Learning》</a></li></ul><p>介绍:面向机器学习的马尔科夫链蒙特卡洛(MCMC)</p><ul><li><a href="https://github.com/endymecy/awesome-deeplearning-resources" target="_blank" rel="noopener">《Awesome Deep learning papers and other resources》</a></li></ul><p>介绍:深度学习论文与资源大列表(论文、预训练模型、课程、图书、软件、应用、相关列表等)</p><ul><li><a href="https://github.com/karthikncode/nlp-datasets" target="_blank" rel="noopener">《Datasets for Natural Language Processing》</a></li></ul><p>介绍:自然语言处理NLP数据集列表</p><ul><li><a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers" target="_blank" rel="noopener">《Machine Learning for Software Engineers》</a></li></ul><p>介绍:软件工程师的机器学习</p><ul><li><a href="https://github.com/wilsonfreitas/awesome-quant" target="_blank" rel="noopener">《Quantitative Finance resources》</a></li></ul><p>介绍:量化金融(Quants)资源列表</p><ul><li><a href="https://books.google.com.hk/books?id=7vS2y-mQmpAC" target="_blank" rel="noopener">《What Computers Still Can’t Do.》</a></li></ul><p>介绍:《计算机仍然不能做什么——人工理性批判》<a href="http://shc2000.sjtu.edu.cn/20120630/MIT.htm" target="_blank" rel="noopener">MIT版导言</a></p><ul><li><a href="https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view" target="_blank" rel="noopener">《In-Datacenter Performance Analysis of a Tensor Processing Unit》</a></li></ul><p>介绍:谷歌发论文详解TPU</p><ul><li><a href="http://www.aaai.org/Library/ICWSM/icwsm17contents.php" target="_blank" rel="noopener">《Proceedings of the Eleventh International Conference on Web and Social Medias》</a></li></ul><p>介绍:2017年ICWSM会议论文合集，业内对它的评价是:”算是最顶级也是最早的有关社会计算的会议”。里面的论文大部分是研究社交网络的，例如twitter，emoji，游戏。对于社交媒体来说内容还是挺前沿的。如果你是做社会计算的还是可以看看。毕竟是行业内数一数二的会议。对了，只要是你知道名字的有名社交媒体都有投稿.[陌陌不算]</p><ul><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html" target="_blank" rel="noopener">《NTUEE ML 2017》</a></li></ul><p>介绍:台大李宏毅中文机器学习课程(2017)</p><ul><li><a href="https://www.youtube.com/playlist?list=PLwv-rHS37fS9sj62f4HAbqSrC1EiPsNZx" target="_blank" rel="noopener">《TensorFlow Dev Summit 2017》</a></li></ul><p>介绍:2017 TensorFlow 开发者峰会(中文字幕) </p><ul><li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="noopener">《Convolutional Neural Networks for Visual Recognition (CS231n Spring 2017)》</a></li></ul><p>介绍:斯坦福2017季CS231n深度视觉识别课程视频</p>]]></content>
      
      
      <categories>
          
          <category> 资料整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dl-机器学习和深度学习资料</title>
      <link href="/2018/01/24/dl-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"/>
      <url>/2018/01/24/dl-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<p>为了保存资料，于是上传至博客。<br><a id="more"></a></p><h2 id="机器学习-Machine-Learning-amp-深度学习-Deep-Learning-资料-Chapter-1"><a href="#机器学习-Machine-Learning-amp-深度学习-Deep-Learning-资料-Chapter-1" class="headerlink" title="机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Chapter 1)"></a>机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Chapter 1)</h2><h5 id="注-机器学习资料篇目一共500条-篇目二开始更新"><a href="#注-机器学习资料篇目一共500条-篇目二开始更新" class="headerlink" title="注:机器学习资料篇目一共500条,篇目二开始更新"></a>注:机器学习资料<a href="https://github.com/ty4z2008/Qix/blob/master/dl.md" target="_blank" rel="noopener">篇目一</a>共500条,<a href="https://github.com/ty4z2008/Qix/blob/master/dl2.md" target="_blank" rel="noopener">篇目二</a>开始更新</h5><h5 id="希望转载的朋友，你可以不用联系我．但是一定要保留原文链接，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外-某些资料在中国访问需要梯子"><a href="#希望转载的朋友，你可以不用联系我．但是一定要保留原文链接，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外-某些资料在中国访问需要梯子" class="headerlink" title="希望转载的朋友，你可以不用联系我．但是一定要保留原文链接，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外:某些资料在中国访问需要梯子."></a>希望转载的朋友，你可以不用联系我．但是<strong>一定要保留原文链接</strong>，因为这个项目还在继续也在不定期更新．希望看到文章的朋友能够学到更多．此外:某些资料在中国访问需要梯子.</h5><ul><li><a href="http://www.erogol.com/brief-history-machine-learning/" target="_blank" rel="noopener">《Brief History of Machine Learning》</a></li></ul><p>介绍:这是一篇介绍机器学习历史的文章，介绍很全面，从感知机、神经网络、决策树、SVM、Adaboost到随机森林、Deep Learning.</p><ul><li><a href="http://www.idsia.ch/~juergen/DeepLearning15May2014.pdf" target="_blank" rel="noopener">《Deep Learning in Neural Networks: An Overview》</a></li></ul><p>介绍:这是瑞士人工智能实验室Jurgen Schmidhuber写的最新版本《神经网络与深度学习综述》本综述的特点是以时间排序，从1940年开始讲起，到60-80年代，80-90年代，一直讲到2000年后及最近几年的进展。涵盖了deep learning里各种tricks，引用非常全面.</p><ul><li><a href="http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/" target="_blank" rel="noopener">《A Gentle Introduction to Scikit-Learn: A Python Machine Learning Library》</a></li></ul><p>介绍:这是一份python机器学习库,如果您是一位python工程师而且想深入的学习机器学习.那么这篇文章或许能够帮助到你.</p><ul><li><a href="http://machinelearningmastery.com/how-to-layout-and-manage-your-machine-learning-project/" target="_blank" rel="noopener">《How to Layout and Manage Your Machine Learning Project》</a></li></ul><p>介绍:这一篇介绍如果设计和管理属于你自己的机器学习项目的文章，里面提供了管理模版、数据管理与实践方法.</p><ul><li><a href="https://medium.com/code-poet/80ea3ec3c471" target="_blank" rel="noopener">《Machine Learning is Fun!》</a></li></ul><p>介绍:如果你还不知道什么是机器学习，或则是刚刚学习感觉到很枯燥乏味。那么推荐一读。这篇文章已经被翻译成中文,如果有兴趣可以移步<a href="http://blog.jobbole.com/67616/" target="_blank" rel="noopener">http://blog.jobbole.com/67616/</a></p><ul><li><a href="http://cran.r-project.org/doc/contrib/Liu-R-refcard.pdf" target="_blank" rel="noopener">《R语言参考卡片》</a></li></ul><p>介绍:R语言是机器学习的主要语言,有很多的朋友想学习R语言，但是总是忘记一些函数与关键字的含义。那么这篇文章或许能够帮助到你</p><ul><li><a href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/" target="_blank" rel="noopener">《Choosing a Machine Learning Classifier》</a></li></ul><p>介绍:我该如何选择机器学习算法，这篇文章比较直观的比较了Naive Bayes，Logistic Regression，SVM，决策树等方法的优劣，另外讨论了样本大小、Feature与Model权衡等问题。此外还有已经翻译了的版本:<a href="http://www.52ml.net/15063.html" target="_blank" rel="noopener">http://www.52ml.net/15063.html</a></p><ul><li><a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks" target="_blank" rel="noopener">《An Introduction to Deep Learning: From Perceptrons to Deep Networks》</a></li></ul><p>介绍：深度学习概述：从感知机到深度网络，作者对于例子的选择、理论的介绍都很到位，由浅入深。翻译版本：<a href="http://www.cnblogs.com/xiaowanyer/p/3701944.html" target="_blank" rel="noopener">http://www.cnblogs.com/xiaowanyer/p/3701944.html</a></p><ul><li><p><a href="http://vdisk.weibo.com/s/ayG13we2vxyKl" target="_blank" rel="noopener">《The LION Way: Machine Learning plus Intelligent Optimization》</a></p><p>介绍:&lt;机器学习与优化&gt;这是一本机器学习的小册子, 短短300多页道尽机器学习的方方面面. 图文并茂, 生动易懂, 没有一坨坨公式的烦恼. 适合新手入门打基础, 也适合老手温故而知新. 比起MLAPP/PRML等大部头, 也许这本你更需要!具体内容推荐阅读:<a href="http://intelligent-optimization.org/LIONbook/" target="_blank" rel="noopener">http://intelligent-optimization.org/LIONbook/</a> </p></li><li><p><a href="http://php-52cs.rhcloud.com/?cat=7" target="_blank" rel="noopener">《深度学习与统计学习理论》</a></p></li></ul><p>介绍:作者是来自百度，不过他本人已经在2014年4月份申请离职了。但是这篇文章很不错如果你不知道深度学习与支持向量机/统计学习理论有什么联系？那么应该立即看看这篇文章.</p><ul><li><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_notes.pdf" target="_blank" rel="noopener">《计算机科学中的数学》</a></li></ul><p>介绍:这本书是由谷歌公司和MIT共同出品的计算机科学中的数学：<a href="Mathematics for Computer Science">Mathematics for Computer Science</a>，Eric Lehman et al 2013 。分为5大部分：1）证明，归纳。2）结构，数论，图。3）计数，求和，生成函数。4）概率，随机行走。5）递归。等等</p><ul><li><a href="http://research.microsoft.com/en-US/people/kannan/book-no-solutions-aug-21-2014.pdf" target="_blank" rel="noopener">《信息时代的计算机科学理论(Foundations of Data Science)》</a></li></ul><p>介绍：信息时代的计算机科学理论,目前国内有纸质书购买，<a href="https://itunes.apple.com/us/book/introduction-to-data-science/id529088127" target="_blank" rel="noopener">iTunes购买</a></p><ul><li><a href="http://vdisk.weibo.com/s/ayG13we2vx5qg" target="_blank" rel="noopener">《Data Science with R》</a></li></ul><p>介绍:这是一本由雪城大学新编的第二版《数据科学入门》教材：偏实用型，浅显易懂，适合想学习R语言的同学选读。</p><ul><li><a href="http://www.informit.com/articles/article.aspx?p=2213858" target="_blank" rel="noopener">《Twenty Questions for Donald Knuth》</a></li></ul><p>介绍:这并不是一篇文档或书籍。这是篇向图灵奖得主Donald Knuth提问记录稿： 近日， Charles Leiserson, Al Aho, Jon Bentley等大神向Knuth提出了20个问题，内容包括TAOCP，P/NP问题，图灵机，逻辑，以及为什么大神不用电邮等等。</p><ul><li><a href="http://arxiv.org/pdf/1402.4304v2.pdf" target="_blank" rel="noopener">《Automatic Construction and Natural-Language Description of Nonparametric Regression Models》</a></li></ul><p>介绍：不会统计怎么办？不知道如何选择合适的统计模型怎么办？那这篇文章你的好好读一读了麻省理工Joshua B. Tenenbaum和剑桥Zoubin Ghahramani合作，写了一篇关于automatic statistician的文章。可以自动选择回归模型类别，还能自动写报告…</p><ul><li><a href="http://openreview.net/venue/iclr2014" target="_blank" rel="noopener">《ICLR 2014论文集》</a></li></ul><p>介绍:对深度学习和representation learning最新进展有兴趣的同学可以了解一下</p><ul><li><a href="http://www-nlp.stanford.edu/IR-book/" target="_blank" rel="noopener">《Introduction to Information Retrieval》</a></li></ul><p>介绍：这是一本信息检索相关的书籍，是由斯坦福Manning与谷歌副总裁Raghavan等合著的Introduction to Information Retrieval一直是北美最受欢迎的信息检索教材之一。最近作者增加了该课程的幻灯片和作业。IR相关资源：<a href="http://www-nlp.stanford.edu/IR-book/information-retrieval.html" target="_blank" rel="noopener">http://www-nlp.stanford.edu/IR-book/information-retrieval.html</a></p><ul><li><a href="http://www.denizyuret.com/2014/02/machine-learning-in-5-pictures.html" target="_blank" rel="noopener">《Machine learning in 10 pictures》</a></li></ul><p>介绍:Deniz Yuret用10张漂亮的图来解释机器学习重要概念：1. Bias/Variance Tradeoff 2. Overfitting 3. Bayesian / Occam’s razor 4. Feature combination 5. Irrelevant feature 6. Basis function 7. Discriminative / Generative 8. Loss function 9. Least squares 10. Sparsity.很清晰</p><ul><li><a href="http://webscope.sandbox.yahoo.com/catalog.php?datatype=l" target="_blank" rel="noopener">《雅虎研究院的数据集汇总》</a></li></ul><p>介绍：雅虎研究院的数据集汇总： 包括语言类数据，图与社交类数据，评分与分类数据，计算广告学数据，图像数据，竞赛数据，以及系统类的数据。</p><ul><li><a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank" rel="noopener">《An Introduction to Statistical Learning with Applications in R》</a></li></ul><p>介绍：这是一本斯坦福统计学著名教授Trevor Hastie和Robert Tibshirani的新书，并且在2014年一月已经开课：<a href="https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about" target="_blank" rel="noopener">https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about</a></p><ul><li><a href="http://machinelearningmastery.com/best-machine-learning-resources-for-getting-started/" target="_blank" rel="noopener">Best Machine Learning Resources for Getting Started</a></li></ul><p>介绍：机器学习最佳入门学习资料汇总是专为机器学习初学者推荐的优质学习资源，帮助初学者快速入门。而且这篇文章的介绍已经被翻译成<a href="http://article.yeeyan.org/view/22139/410514" target="_blank" rel="noopener">中文版</a>。如果你不怎么熟悉，那么我建议你先看一看中文的介绍。</p><ul><li><a href="http://blog.sina.com.cn/s/blog_bda0d2f10101fpp4.html" target="_blank" rel="noopener">My deep learning reading list</a></li></ul><p>介绍:主要是顺着Bengio的PAMI review的文章找出来的。包括几本综述文章，将近100篇论文，各位山头们的Presentation。全部都可以在google上找到。</p><ul><li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00266ED1V01Y201005HLT008?journalCode=hlt" target="_blank" rel="noopener">Cross-Language Information Retrieval</a></li></ul><p>介绍：这是一本书籍，主要介绍的是跨语言信息检索方面的知识。理论很多</p><ul><li><a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1/index.html?ca=drs-" target="_blank" rel="noopener">探索推荐引擎内部的秘密，第 1 部分: 推荐引擎初探</a></li></ul><p>介绍:本文共有三个系列，作者是来自IBM的工程师。它主要介绍了推荐引擎相关算法，并帮助读者高效的实现这些算法。 <a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html?ca=drs-" target="_blank" rel="noopener">探索推荐引擎内部的秘密，第 2 部分: 深度推荐引擎相关算法 - 协同过滤</a>,<a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy3/index.html?ca=drs-" target="_blank" rel="noopener">探索推荐引擎内部的秘密，第 3 部分: 深度推荐引擎相关算法 - 聚类</a></p><ul><li><a href="http://mimno.infosci.cornell.edu/b/articles/ml-learn/" target="_blank" rel="noopener">《Advice for students of machine learning》</a></li></ul><p>介绍：康奈尔大学信息科学系助理教授David Mimno写的《对机器学习初学者的一点建议》， 写的挺实际，强调实践与理论结合，最后还引用了冯 • 诺依曼的名言: “Young man, in mathematics you don’t understand things. You just get used to them.”</p><ul><li><a href="http://web.stanford.edu/group/pdplab/pdphandbook/" target="_blank" rel="noopener">分布式并行处理的数据</a></li></ul><p>介绍：这是一本关于分布式并行处理的数据《Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises》,作者是斯坦福的James L. McClelland。着重介绍了各种神级网络算法的分布式实现,做Distributed Deep Learning 的童鞋可以参考下</p><ul><li><a href="http://blogs.technet.com/b/machinelearning/archive/2014/07/01/what-is-machine-learning.aspx" target="_blank" rel="noopener">《“机器学习”是什么？》</a></li></ul><p>介绍:【“机器学习”是什么？】John Platt是微软研究院杰出科学家，17年来他一直在机器学习领域耕耘。近年来机器学习变得炙手可热，Platt和同事们遂决定开设<a href="http://blogs.technet.com/b/machinelearning/" target="_blank" rel="noopener">博客</a>，向公众介绍机器学习的研究进展。机器学习是什么，被应用在哪里？来看Platt的这篇<a href="http://blogs.technet.com/b/machinelearning/archive/2014/07/01/what-is-machine-learning.aspx" target="_blank" rel="noopener">博文</a></p><ul><li><a href="http://icml.cc/2014/index/article/15.htm" target="_blank" rel="noopener">《2014年国际机器学习大会ICML 2014 论文》</a></li></ul><p>介绍：2014年国际机器学习大会（ICML）已经于6月21-26日在国家会议中心隆重举办。本次大会由微软亚洲研究院和清华大学联手主办，是这个有着30多年历史并享誉世界的机器学习领域的盛会首次来到中国，已成功吸引海内外1200多位学者的报名参与。干货很多，值得深入学习下</p><ul><li><a href="http://blogs.technet.com/b/machinelearning/archive/2014/07/11/machine-learning-for-industry-a-case-study.aspx" target="_blank" rel="noopener">《Machine Learning for Industry: A Case Study》</a></li></ul><p>介绍：这篇文章主要是以Learning to Rank为例说明企业界机器学习的具体应用，RankNet对NDCG之类不敏感，加入NDCG因素后变成了LambdaRank，同样的思想从神经网络改为应用到Boosted Tree模型就成就了LambdaMART。<a href="http://research.microsoft.com/en-us/people/cburges/?WT.mc_id=Blog_MachLearn_General_DI" target="_blank" rel="noopener">Chirs Burges</a>，微软的机器学习大神，Yahoo 2010 Learning to Rank Challenge第一名得主，排序模型方面有RankNet，LambdaRank，LambdaMART，尤其以LambdaMART最为突出，代表论文为：<br><a href="http://research.microsoft.com/en-us/um/people/cburges/tech_reports/msr-tr-2010-82.pdf" target="_blank" rel="noopener">From RankNet to LambdaRank to LambdaMART: An Overview</a><br>此外，Burges还有很多有名的代表作，比如：<a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf" target="_blank" rel="noopener">A Tutorial on Support Vector Machines for Pattern Recognition</a><br><a href="http://research.microsoft.com/en-us/um/people/cburges/tech_reports/tr-2004-56.pdf" target="_blank" rel="noopener">Some Notes on Applied Mathematics for Machine Learning</a></p><ul><li><a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/" target="_blank" rel="noopener">100 Best GitHub: Deep Learning</a></li></ul><p>介绍:100 Best GitHub: Deep Learning</p><ul><li><a href="http://www.52ml.net/12019.html" target="_blank" rel="noopener">《UFLDL-斯坦福大学Andrew Ng教授“Deep Learning”教程》</a></li></ul><p>介绍:本教程将阐述无监督特征学习和深度学习的主要观点。通过学习，你也将实现多个功能学习/深度学习算法，能看到它们为你工作，并学习如何应用/适应这些想法到新问题上。本教程假定机器学习的基本知识（特别是熟悉的监督学习，逻辑回归，梯度下降的想法），如果你不熟悉这些想法，我们建议你去这里<a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning" target="_blank" rel="noopener">机器学习课程</a>，并先完成第II，III，IV章（到逻辑回归）。此外这关于这套教程的源代码在github上面已经有python版本了<a href="https://github.com/jatinshah/ufldl_tutorial" target="_blank" rel="noopener"> UFLDL Tutorial Code</a></p><p>*<a href="http://research.microsoft.com/pubs/217165/ICASSP_DeepTextLearning_v07.pdf" target="_blank" rel="noopener">《Deep Learning for Natural Language Processing and Related Applications》</a></p><p>介绍:这份文档来自微软研究院,精髓很多。如果需要完全理解，需要一定的机器学习基础。不过有些地方会让人眼前一亮,茅塞顿开。</p><ul><li><a href="https://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="noopener">Understanding Convolutions</a></li></ul><p>介绍:这是一篇介绍图像卷积运算的文章，讲的已经算比较详细的了</p><ul><li><a href="http://mlss2014.com/" target="_blank" rel="noopener">《Machine Learning Summer School》</a></li></ul><p>介绍：每天请一个大牛来讲座，主要涉及机器学习，大数据分析，并行计算以及人脑研究。<a href="https://www.youtube.com/user/smolix" target="_blank" rel="noopener">https://www.youtube.com/user/smolix</a>    （需翻墙）</p><ul><li><a href="https://github.com/josephmisiti/awesome-machine-learning" target="_blank" rel="noopener">《Awesome Machine Learning》</a></li></ul><p>介绍：一个超级完整的机器学习开源库总结，如果你认为这个碉堡了，那后面这个列表会更让你惊讶：【Awesome Awesomeness】,国内已经有热心的朋友进行了翻译<a href="http://blog.jobbole.com/73806/" target="_blank" rel="noopener">中文介绍</a>，<a href="https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md" target="_blank" rel="noopener">机器学习数据挖掘免费电子书</a></p><ul><li><a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="noopener">斯坦福《自然语言处理》课程视频</a></li></ul><p>介绍:ACL候任主席、斯坦福大学计算机系Chris Manning教授的《自然语言处理》课程所有视频已经可以在斯坦福公开课网站上观看了（如Chrome不行，可用IE观看） 作业与测验也可以下载。</p><ul><li><a href="http://freemind.pluskid.org/machine-learning/deep-learning-and-shallow-learning/" target="_blank" rel="noopener">《Deep Learning and Shallow Learning》</a></li></ul><p>介绍:对比 Deep Learning 和 Shallow Learning 的好文，来着浙大毕业、MIT 读博的 Chiyuan Zhang 的博客。</p><ul><li><a href="http://benanne.github.io/2014/08/05/spotify-cnns.html" target="_blank" rel="noopener">《Recommending music on Spotify with deep learning》</a></li></ul><p>介绍:利用卷积神经网络做音乐推荐。</p><ul><li><a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">《Neural Networks and Deep Learning》</a></li></ul><p>介绍：神经网络的免费在线书，已经写了三章了，还有对应的开源代码：<a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener">https://github.com/mnielsen/neural-networks-and-deep-learning</a> 爱好者的福音。</p><ul><li><a href="http://machinelearningmastery.com/java-machine-learning/" target="_blank" rel="noopener">《Java Machine Learning》</a></li></ul><p>介绍：Java机器学习相关平台和开源的机器学习库，按照大数据、NLP、计算机视觉和Deep Learning分类进行了整理。看起来挺全的，Java爱好者值得收藏。</p><ul><li><a href="http://www.oschina.net/translate/6-tips-for-writing-better-code" target="_blank" rel="noopener">《Machine Learning Theory: An Introductory Primer》</a></li></ul><p>介绍：机器学习最基本的入门文章，适合零基础者</p><ul><li><a href="http://www.ctocio.com/hotnews/15919.html" target="_blank" rel="noopener">《机器学习常见算法分类汇总》</a></li></ul><p>介绍：机器学习的算法很多。很多时候困惑人们都是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，我们从两个方面来给大家介绍，第一个方面是学习的方式，第二个方面是算法的类似性。</p><ul><li><a href="http://suanfazu.com/discussion/68/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87survey%E5%90%88%E9%9B%86" target="_blank" rel="noopener">《机器学习经典论文/survey合集》</a></li></ul><p>介绍：看题目你已经知道了是什么内容,没错。里面有很多经典的机器学习论文值得仔细与反复的阅读。</p><ul><li><a href="http://work.caltech.edu/library/" target="_blank" rel="noopener">《机器学习视频库》</a></li></ul><p>介绍：视频由加州理工学院（Caltech）出品。需要英语底子。</p><ul><li><a href="http://suanfazu.com/discussion/109/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E4%B9%A6%E7%B1%8D" target="_blank" rel="noopener">《机器学习经典书籍》</a></li></ul><p>介绍：总结了机器学习的经典书籍，包括数学基础和算法理论的书籍，可做为入门参考书单。</p><ul><li><a href="http://efytimes.com/e1/fullnews.asp?edid=121516" target="_blank" rel="noopener">《16 Free eBooks On Machine Learning》</a></li></ul><p>介绍:16本机器学习的电子书，可以下载下来在pad，手机上面任意时刻去阅读。不多我建议你看完一本再下载一本。</p><ul><li><a href="http://www.erogol.com/large-set-machine-learning-resources-beginners-mavens/" target="_blank" rel="noopener">《A Large set of Machine Learning Resources for Beginners to Mavens》</a></li></ul><p>介绍:标题很大，从新手到专家。不过看完上面所有资料。肯定是专家了</p><ul><li><a href="http://article.yeeyan.org/view/22139/410514" target="_blank" rel="noopener">《机器学习最佳入门学习资料汇总》</a></li></ul><p>介绍：入门的书真的很多，而且我已经帮你找齐了。</p><ul><li><a href="http://users.soe.ucsc.edu/~niejiazhong/slides/chandra.pdf" target="_blank" rel="noopener">《Sibyl》</a></li></ul><p>介绍：Sibyl 是一个监督式机器学习系统，用来解决预测方面的问题，比如 YouTube 的视频推荐。</p><ul><li><a href="http://www.slideshare.net/ssuser9cc1bd/piji-li-dltm" target="_blank" rel="noopener">《Neural Network &amp; Text Mining》</a></li></ul><p>介绍:关于(Deep) Neural Networks在 NLP 和 Text Mining 方面一些paper的总结</p><ul><li><a href="http://www.cnblogs.com/lxy2017/p/3927226.html" target="_blank" rel="noopener">《前景目标检测1（总结）》</a></li></ul><p>介绍:计算机视觉入门之前景目标检测1（总结）</p><ul><li><a href="http://www.52ml.net/17004.html" target="_blank" rel="noopener">《行人检测》</a></li></ul><p>介绍:计算机视觉入门之行人检测</p><ul><li><a href="http://www.kdnuggets.com/2014/08/deep-learning-important-resources-learning-understanding.html" target="_blank" rel="noopener">《Deep Learning – important resources for learning and understanding》</a></li></ul><p>介绍:Important resources for learning and understanding . Is awesome</p><ul><li><a href="http://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer" target="_blank" rel="noopener">《Machine Learning Theory: An Introductory Primer》</a></li></ul><p>介绍:这又是一篇机器学习初学者的入门文章。值得一读</p><ul><li><a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">《Neural Networks and Deep Learning》</a></li></ul><p>介绍:在线Neural Networks and Deep Learning电子书</p><ul><li><a href="http://www.52nlp.cn/python-%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB-%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86-%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98" target="_blank" rel="noopener">《Python 网页爬虫 &amp; 文本处理 &amp; 科学计算 &amp; 机器学习 &amp; 数据挖掘兵器谱》</a></li></ul><p>介绍:python的17个关于机器学习的工具</p><ul><li><a href="http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E7%A5%9E%E5%A5%87%E7%9A%84%E4%BC%BD%E7%8E%9B%E5%87%BD%E6%95%B0%E4%B8%8A/" target="_blank" rel="noopener">《神奇的伽玛函数(上)》</a></li></ul><p>介绍:下集在这里<a href="http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E7%A5%9E%E5%A5%87%E7%9A%84%E4%BC%BD%E7%8E%9B%E5%87%BD%E6%95%B0%E4%B8%8A/" target="_blank" rel="noopener">神奇的伽玛函数(下)</a></p><ul><li><a href="http://cxwangyi.github.io/notes/2014-01-20-distributed-machine-learning.html" target="_blank" rel="noopener">《分布式机器学习的故事》</a></li></ul><p>介绍:作者王益目前是腾讯广告算法总监，王益博士毕业后在google任研究。这篇文章王益博士7年来从谷歌到腾讯对于分布机器学习的所见所闻。值得细读</p><ul><li><a href="http://metacademy.org/roadmaps/cjrd/level-up-your-ml" target="_blank" rel="noopener">《机器学习提升之道（Level-Up Your Machine Learning）》</a></li></ul><p>介绍:把机器学习提升的级别分为0~4级，每级需要学习的教材和掌握的知识。这样，给机器学习者提供一个上进的路线图，以免走弯路。另外，整个网站都是关于机器学习的，资源很丰富。</p><ul><li><a href="http://www.mlsurveys.com/" target="_blank" rel="noopener">《Machine Learning Surveys》</a></li></ul><p>介绍:机器学习各个方向综述的网站</p><ul><li><a href="http://deeplearning.net/reading-list/" target="_blank" rel="noopener">《Deep Learning Reading list》</a></li></ul><p>介绍:深度学习阅资源列表</p><ul><li><a href="http://research.microsoft.com/pubs/219984/DeepLearningBook_RefsByLastFirstNames.pdf" target="_blank" rel="noopener">《Deep Learning: Methods and Applications》</a></li></ul><p>介绍：这是一本来自微的研究员 li Peng和Dong Yu所著的关于深度学习的方法和应用的电子书</p><ul><li><a href="http://pan.baidu.com/s/1pJ0ok7T" target="_blank" rel="noopener">《Machine Learning Summer School 2014》</a></li></ul><p>介绍:2014年七月CMU举办的机器学习夏季课刚刚结束 有近50小时的视频、十多个PDF版幻灯片，覆盖 深度学习，贝叶斯，分布式机器学习，伸缩性 等热点话题。所有13名讲师都是牛人：包括大牛Tom Mitchell （他的［机器学习］是名校的常用教材），还有CMU李沐 .（1080P高清哟）</p><ul><li><a href="http://users.soe.ucsc.edu/~niejiazhong/slides/chandra.pdf" target="_blank" rel="noopener">《Sibyl: 来自Google的大规模机器学习系统》</a></li></ul><p>介绍:在今年的IEEE/IFIP可靠系统和网络（DSN）国际会议上，Google软件工程师Tushar Chandra做了一个关于Sibyl系统的主题演讲。 Sibyl是一个监督式机器学习系统，用来解决预测方面的问题，比如YouTube的视频推荐。详情请阅读<a href="http://www.infoq.com/cn/news/2014/07/google-sibyl" target="_blank" rel="noopener">google sibyl</a></p><ul><li><a href="http://googleresearch.blogspot.com/2014/09/building-deeper-understanding-of-images.html" target="_blank" rel="noopener">《Building a deeper understanding of images》</a></li></ul><p>介绍:谷歌研究院的Christian Szegedy在谷歌研究院的博客上简要地介绍了他们今年参加ImageNet取得好成绩的GoogLeNet系统.是关于图像处理的。</p><ul><li><a href="https://github.com/memect/hao/blob/master/awesome/bayesian-network-python.md" target="_blank" rel="noopener">《Bayesian network 与python概率编程实战入门》</a></li></ul><p>介绍:贝叶斯学习。如果不是很清可看看<a href="http://www.infoq.com/cn/news/2014/07/programming-language-bayes" target="_blank" rel="noopener">概率编程语言与贝叶斯方法实践</a></p><ul><li><a href="http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/" target="_blank" rel="noopener">《AMA: Michael I Jordan》</a></li></ul><p>介绍:网友问伯克利机器学习大牛、美国双料院士Michael I. Jordan：”如果你有10亿美金，你怎么花？Jordan: “我会用这10亿美金建造一个NASA级别的自然语言处理研究项目。” </p><ul><li><a href="http://www.cnblogs.com/tornadomeet/p/3395593.html" target="_blank" rel="noopener">《机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）》</a></li></ul><p>介绍:常见面试之机器学习算法思想简单梳理,此外作者还有一些其他的<a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">机器学习与数据挖掘文章</a>和<a href="http://www.cnblogs.com/tornadomeet/tag/Deep%E3%80%80Learning/" target="_blank" rel="noopener">深度学习文章</a>,不仅是理论还有源码。</p><ul><li><a href="http://www.kdnuggets.com/2014/09/most-viewed-web-mining-lectures-videolectures.html" target="_blank" rel="noopener">《文本与数据挖掘视频汇总》</a></li></ul><p>介绍：Videolectures上最受欢迎的25个文本与数据挖掘视频汇总</p><ul><li><a href="http://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/" target="_blank" rel="noopener">《怎么选择深度学习的GPUs》</a></li></ul><p>介绍:在Kaggle上经常取得不错成绩的Tim Dettmers介绍了他自己是怎么选择深度学习的GPUs, 以及个人如何构建深度学习的GPU集群: <a href="http://t.cn/RhpuD1G" target="_blank" rel="noopener">http://t.cn/RhpuD1G</a> </p><ul><li><a href="http://www.infoq.com/cn/news/2014/09/depth-model" target="_blank" rel="noopener">《对话机器学习大神Michael Jordan：深度模型》</a></li></ul><p>介绍:对话机器学习大神Michael Jordan</p><ul><li><a href="http://blog.sina.com.cn/s/blog_46d0a3930101fswl.html" target="_blank" rel="noopener">《Deep Learning 和 Knowledge Graph 引爆大数据革命》</a></li></ul><p>介绍:还有２，３部分。<a href="http://blog.sina.com.cn/s/blog_46d0a3930101gs5h.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_46d0a3930101gs5h.html</a></p><ul><li><a href="http://blog.sina.com.cn/s/blog_46d0a3930101h6nf.html" target="_blank" rel="noopener">《Deep Learning 教程翻译》</a></li></ul><p>介绍:是Stanford 教授 Andrew Ng 的 Deep Learning 教程，国内的机器学习爱好者很热心的把这个教程翻译成了中文。如果你英语不好，可以看看这个</p><ul><li><a href="http://markus.com/deep-learning-101/" target="_blank" rel="noopener">《Deep Learning 101》</a></li></ul><p>介绍:因为近两年来，深度学习在媒体界被炒作很厉害（就像大数据）。其实很多人都还不知道什么是深度学习。这篇文章由浅入深。告诉你深度学究竟是什么！</p><ul><li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="noopener">《UFLDL Tutorial》</a></li></ul><p>介绍:这是斯坦福大学做的一免费课程（很勉强），这个可以给你在深度学习的路上给你一个学习的思路。里面提到了一些基本的算法。而且告诉你如何去应用到实际环境中。<a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" rel="noopener">中文版</a></p><ul><li><a href="http://deeplearning.cs.toronto.edu/" target="_blank" rel="noopener">《Toronto Deep Learning Demos》</a></li></ul><p>介绍:这是多伦多大学做的一个深度学习用来识别图片标签／图转文字的demo。是一个实际应用案例。有源码</p><ul><li><a href="http://metacademy.org/roadmaps/rgrosse/deep_learning" target="_blank" rel="noopener">《Deep learning from the bottom up》</a></li></ul><p>介绍:机器学习模型，阅读这个内容需要有一定的基础。</p><ul><li><a href="http://cran.r-project.org/web/views/" target="_blank" rel="noopener">《R工具包的分类汇总》</a></li></ul><p>介绍: (CRAN Task Views, 34种常见任务,每个任务又各自分类列举若干常用相关工具包) 例如: 机器学习，自然语言处理，时间序列分析，空间信息分析，多重变量分析，计量经济学，心理统计学，社会学统计，化学计量学，环境科学，药物代谢动力学 等</p><ul><li><a href="http://www.ctocio.com/hotnews/15919.html" target="_blank" rel="noopener">《机器学习常见算法分类汇总》</a></li></ul><p>介绍: 机器学习无疑是当前数据分析领域的一个热点内容。很多人在平时的工作中都或多或少会用到机器学习的算法。本文为您总结一下常见的机器学习算法，以供您在工作和学习中参考.</p><ul><li><a href="http://blog.csdn.net/zouxy09/article/details/8775360" target="_blank" rel="noopener">《Deep Learning（深度学习）学习笔记整理系列》</a></li></ul><p>介绍: 很多干货，而且作者还总结了好几个系列。另外还作者还了一个<a href="http://blog.csdn.net/zouxy09/article/details/14222605" target="_blank" rel="noopener">文章导航</a>.非常的感谢作者总结。</p><p><a href="http://blog.csdn.net/zouxy09/article/details/8775488" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之（二）</a></p><p><a href="http://blog.csdn.net/zouxy09/article/details/8775518" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之（三）</a></p><p><a href="http://blog.csdn.net/zouxy09/article/details/8775524" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之（四）</a></p><p><a href="http://blog.csdn.net/zouxy09/article/details/8777094" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之（五）</a></p><p><a href="http://blog.csdn.net/zouxy09/article/details/8781396" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之（六）</a></p><p><a href="http://blog.csdn.net/zouxy09/article/details/8781543" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之（七）</a></p><p><a href="http://blog.csdn.net/zouxy09/article/details/8782018" target="_blank" rel="noopener">DeepLearning（深度学习）学习笔记整理系列之（八）</a></p><ul><li><a href="http://research.microsoft.com/apps/video/default.aspx?id=206976&amp;l=i" target="_blank" rel="noopener">《Tutorials Session A - Deep Learning for Computer Vision》</a></li></ul><p>介绍:传送理由：Rob Fergus的用深度学习做计算机是觉的NIPS 2013教程。有mp4, mp3, pdf各种<a href="http://msrvideo.vo.msecnd.net/rmcvideos/206976/dl/206976.pdf" target="_blank" rel="noopener">下载</a> 他是纽约大学教授，目前也在Facebook工作，他2014年的8篇<a href="http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php?n=PmWiki.Publications" target="_blank" rel="noopener">论文</a></p><ul><li><a href="https://github.com/xpqiu/fnlp/" target="_blank" rel="noopener">《FudanNLP》</a></li></ul><p>介绍:FudanNLP，这是一个复旦大学计算机学院开发的开源中文自然语言处理（NLP）工具包<br>Fudan NLP里包含中文分词、关键词抽取、命名实体识别、词性标注、时间词抽取、语法分析等功能，对搜索引擎 文本分析等极为有价值。</p><ul><li><a href="http://engineering.linkedin.com/large-scale-machine-learning/open-sourcing-ml-ease" target="_blank" rel="noopener">《Open Sourcing ml-ease》</a></li></ul><p>介绍:LinkedIn 开源的机器学习工具包,支持单机, Hadoop cluster，和 Spark cluster 重点是 logistic regression 算法</p><ul><li><a href="http://ztl2004.github.io/MachineLearningWeekly/index.html" target="_blank" rel="noopener">《机器学习周刊》</a></li></ul><p>介绍:对于英语不好，但又很想学习机器学习的朋友。是一个大的福利。机器学习周刊目前主要提供中文版，还是面向广大国内爱好者，内容涉及机器学习、数据挖掘、并行系统、图像识别、人工智能、机器人等等。谢谢作者</p><ul><li><a href="http://v.163.com/special/opencourse/daishu.html" target="_blank" rel="noopener">《线性代数》</a></li></ul><p>介绍：《线性代数》是《机器学习》的重要数学先导课程。其实《线代》这门课讲得浅显易懂特别不容易，如果一上来就讲逆序数及罗列行列式性质，很容易让学生失去学习的兴趣。我个人推荐的最佳《线性代数》课程是麻省理工Gilbert Strang教授的课程。 <a href="http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/" target="_blank" rel="noopener">课程主页</a></p><ul><li><a href="http://blog.andreamostosi.name/big-data/" target="_blank" rel="noopener">《Big-data》</a></li></ul><p>介绍:大数据数据处理资源、工具不完备列表，从框架、分布式编程、分布式文件系统、键值数据模型、图数据模型、数据可视化、列存储、机器学习等。很赞的资源汇总。</p><ul><li><a href="http://yahoolabs.tumblr.com/post/97839313996/machine-learning-for-smart-dummies" target="_blank" rel="noopener">《machine learning for smart dummies》</a></li></ul><p>介绍:雅虎邀请了一名来自本古里安大学的访问学者，制作了一套关于机器学习的系列视频课程。本课程共分为7期，详细讲解了有关SVM, boosting, nearest neighbors, decision trees 等常规机器学习算法的理论基础知识。</p><ul><li><a href="http://arxiv.org/abs/1409.7770" target="_blank" rel="noopener">《Entanglement-Based Quantum Machine Learning》</a></li></ul><p>介绍:应对大数据时代，量子机器学习的第一个实验 <a href="http://arxiv-web3.library.cornell.edu/pdf/1409.7770.pdf" target="_blank" rel="noopener">paper 下载</a></p><ul><li><a href="http://www.wired.com/2014/01/how-to-hack-okcupid/all/" target="_blank" rel="noopener">《How a Math Genius Hacked OkCupid to Find True Love》</a></li></ul><p>介绍:Wired杂志报道了UCLA数学博士Chris McKinlay （图1）通过大数据手段+机器学习方法破解婚恋网站配对算法找到真爱的故事,通过Python脚本控制着12个账号，下载了婚恋网站2万女用户的600万问题答案，对他们进行了统计抽样及聚类分析（图2，3），最后终于收获了真爱。科技改变命运！</p><ul><li><a href="https://www.edx.org/course/mitx/mitx-6-832x-underactuated-robotics-3511" target="_blank" rel="noopener">《Underactuated Robotics》</a></li></ul><p>介绍:MIT的Underactuated Robotics于 2014年10月1日开课，该课属于MIT研究生级别的课程，对机器人和非线性动力系统感兴趣的朋友不妨可以挑战一下这门课程！</p><ul><li><a href="http://www.csdn.net/article/2014-12-26/2823330" target="_blank" rel="noopener">《mllib实践经验(1)》</a></li></ul><p>介绍:mllib实践经验分享</p><ul><li><a href="http://www.seobythesea.com/2014/09/google-turns-deep-learning-classification-fight-web-spam/" target="_blank" rel="noopener">《Google Turns To Deep Learning Classification To Fight Web Spam》</a></li></ul><p>介绍:Google用Deep Learning做的antispam(反垃圾邮件)</p><ul><li><a href="https://github.com/memect/hao/blob/master/awesome/nlp.md" target="_blank" rel="noopener">《NLP常用信息资源》</a></li></ul><p>介绍:NLP常用信息资源* <a href="https://github.com/memect/hao/blob/master/awesome/nlp.md" target="_blank" rel="noopener">《NLP常用信息资源》</a></p><ul><li><a href="https://github.com/soulmachine/machine-learning-cheat-sheet" target="_blank" rel="noopener">《机器学习速查表》</a></li></ul><p>介绍:机器学习速查表</p><ul><li><a href="http://arnetminer.org/conferencebestpapers" target="_blank" rel="noopener">《Best Papers vs. Top Cited Papers in Computer Science》</a></li></ul><p>介绍：从1996年开始在计算机科学的论文中被引用次数最多的论文</p><ul><li><a href="http://mmcheng.net/zh/itam/" target="_blank" rel="noopener">《InfiniTAM: 基于深度图像的体数据集成框架》</a></li></ul><p>介绍：把今年的一个ACM Trans. on Graphics (TOG)论文中的代码整理为一个开源的算法框架，共享出来了。欢迎大家使用。可以实时的采集3D数据、重建出三维模型。Online learning，GPU Random forest，GPU CRF也会后续公开。</p><ul><li><a href="http://karpathy.github.io/neuralnets/" target="_blank" rel="noopener">《Hacker’s guide to Neural Networks》</a></li></ul><p>介绍：【神经网络黑客指南】现在，最火莫过于深度学习（Deep Learning），怎样更好学习它？可以让你在浏览器中，跑起深度学习效果的超酷开源项目<a href="https://github.com/karpathy/convnetjs" target="_blank" rel="noopener">ConvNetJS</a>作者karpathy告诉你，最佳技巧是，当你开始写代码，一切将变得清晰。他刚发布了一本图书，不断在线更新</p><ul><li><a href="http://machinelearningmastery.com/building-a-production-machine-learning-infrastructure/" target="_blank" rel="noopener">《Building a Production Machine Learning Infrastructure》</a></li></ul><p>介绍：前Google广告系统工程师Josh Wills 讲述工业界和学术界机器学习的异同,大实话</p><ul><li><a href="http://neo4j.com/blog/deep-learning-sentiment-analysis-movie-reviews-using-neo4j/" target="_blank" rel="noopener">《Deep Learning Sentiment Analysis for Movie Reviews using Neo4j》</a></li></ul><p>介绍：使用<a href="http://www.neo4j.org/" target="_blank" rel="noopener">Neo4j</a> 做电影评论的情感分析。</p><ul><li><a href="http://memkite.com/deep-learning-bibliography/" target="_blank" rel="noopener">《DeepLearning.University – An Annotated Deep Learning Bibliography》</a></li></ul><p>介绍：不仅是资料，而且还对有些资料做了注释。</p><ul><li><a href="http://www.datarobot.com/blog/a-primer-on-deep-learning/" target="_blank" rel="noopener">《A primer on deeping learning》</a></li></ul><p>介绍：深度学习入门的初级读本</p><ul><li><a href="https://news.ycombinator.com/item?id=8379571" target="_blank" rel="noopener">《Machine learning is teaching us the secret to teaching 》</a></li></ul><p>介绍：机器学习教会了我们什么？</p><ul><li><a href="http://scikit-learn.org/stable/documentation.html" target="_blank" rel="noopener">《scikit-learn：用于机器学习的Python模块》</a></li></ul><p>介绍：scikit-learn是在SciPy基础上构建的用于机器学习的Python模块。</p><ul><li><a href="http://www.infoq.com/cn/news/2014/10/interview-michael-jordan" target="_blank" rel="noopener">《对话机器学习大神Michael Jordan：解析领域中各类模型》</a></li></ul><p>介绍：乔丹教授（Michael I. Jordan）教授是机器学习领域神经网络的大牛，他对深度学习、神经网络有着很浓厚的兴趣。因此，很多提问的问题中包含了机器学习领域的各类模型，乔丹教授对此一一做了解释和展望。</p><ul><li><a href="http://www.redblobgames.com/pathfinding/a-star/introduction.html" target="_blank" rel="noopener">《A*搜索算法的可视化短教程》</a></li></ul><p>介绍：A*搜索是人工智能基本算法，用于高效地搜索图中两点的最佳路径, 核心是 g(n)+h(n): g(n)是从起点到顶点n的实际代价，h(n)是顶点n到目标顶点的估算代价。<a href="https://github.com/memect/hao/issues/256" target="_blank" rel="noopener">合集</a></p><ul><li><a href="http://code.csdn.net/news/2822123" target="_blank" rel="noopener">《基于云的自然语言处理开源项目FudanNLP》</a></li></ul><p>介绍：本项目利用了Microsoft Azure，可以在几分种内完成NLP on Azure Website的部署，立即开始对FNLP各种特性的试用，或者以REST API的形式调用FNLP的语言分析功能</p><ul><li><a href="http://www.youku.com/playlist_show/id_22935176.html" target="_blank" rel="noopener">《吴立德《概率主题模型&amp;数据科学基础》</a></li></ul><p>介绍：现任复旦大学首席教授、计算机软件博士生导师。计算机科学研究所副所长.内部课程</p><ul><li><a href="http://ml.memect.com/article/machine-learning-guide.html" target="_blank" rel="noopener">《机器学习入门资源不完全汇总》</a></li></ul><p>介绍：好东西的干货真的很多</p><ul><li><a href="http://memkite.com/deep-learning-bibliography/" target="_blank" rel="noopener">《收集从2014年开始深度学习文献》</a></li></ul><p>介绍：从硬件、图像到健康、生物、大数据、生物信息再到量子计算等，Amund Tveit等维护了一个DeepLearning.University小项目：收集从2014年开始深度学习文献，相信可以作为深度学习的起点,<a href="https://github.com/memkite/DeepLearningBibliography" target="_blank" rel="noopener">github</a></p><ul><li><a href="http://emnlp2014.org/papers/pdf/EMNLP2014148.pdf" target="_blank" rel="noopener">《EMNLP上两篇关于股票趋势的应用论文 》</a></li></ul><p>介绍：EMNLP上两篇关于<a href="http://emnlp2014.org/papers/pdf/EMNLP2014148.pdf" target="_blank" rel="noopener">stock trend</a> 用到了deep model组织特征；<a href="http://emnlp2014.org/papers/pdf/EMNLP2014120.pdf" target="_blank" rel="noopener"> Exploiting Social Relations and Sentiment for Stock Prediction</a>用到了stock network。</p><ul><li><a href="http://deeplearning.net/tutorial/deeplearning.pdf" target="_blank" rel="noopener">《Bengio组（蒙特利尔大学LISA组）深度学习教程 》</a></li></ul><p>介绍：作者是深度学习一线大牛Bengio组写的教程，算法深入显出，还有实现代码，一步步展开。</p><ul><li><a href="http://arxiv.org/pdf/1410.5401v1.pdf" target="_blank" rel="noopener">《学习算法的Neural Turing Machine 》</a></li></ul><p>介绍：许多传统的机器学习任务都是在学习function，不过谷歌目前有开始学习算法的趋势。谷歌另外的这篇学习Python程序的<a href="http://arxiv.org/pdf/1410.4615v1.pdf" target="_blank" rel="noopener">Learning to Execute</a>也有相似之处</p><ul><li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00607ED2V01Y201410HLT026" target="_blank" rel="noopener">《Learning to Rank for Information Retrieval and Natural Language Processing》</a></li></ul><p>介绍：作者是华为技术有限公司，诺亚方舟实验室，首席科学家的李航博士写的关于信息检索与自然语言处理的文章</p><ul><li><a href="http://www.aclweb.org/anthology/D11-1147" target="_blank" rel="noopener">《Rumor has it: Identifying Misinformation in Microblogs》</a></li></ul><p>介绍：利用机用器学习在谣言的判别上的应用,此外还有两个。一个是识别垃圾与虚假信息的<a href="http://digital.cs.usu.edu/~kyumin/tutorial/www-tutorial.pdf" target="_blank" rel="noopener">paper</a>.还有一个是<a href="http://www.datatang.com/news/details_1319.htm" target="_blank" rel="noopener">网络舆情及其分析技术</a></p><ul><li><a href="http://study.163.com/course/introduction/854064.htm" target="_blank" rel="noopener">《R机器学习实践》</a></li></ul><p>介绍：该课程是网易公开课的收费课程，不贵，超级便宜。主要适合于对利用R语言进行机器学习，数据挖掘感兴趣的人。</p><ul><li><a href="http://ifeve.com/bigdataanalyticsbeyondhadoop_evolutionofmlrealizaton/" target="_blank" rel="noopener">《大数据分析：机器学习算法实现的演化》</a></li></ul><p>介绍：本章中作者总结了三代机器学习算法实现的演化：第一代非分布式的， 第二代工具如Mahout和Rapidminer实现基于Hadoop的扩展，第三代如Spark和Storm实现了实时和迭代数据处理。<a href="http://ifeve.com/wp-content/uploads/2014/05/big-data-analytics-beyond-hadoop.pdf" target="_blank" rel="noopener">BIG DATA ANALYTICS BEYOND HADOOP</a></p><ul><li><a href="http://book.douban.com/subject/5921462/" target="_blank" rel="noopener">《图像处理，分析与机器视觉》</a></li></ul><p>介绍：讲计算机视觉的四部奇书（应该叫经典吧）之一，另外三本是Hartley的《多图几何》、Gonzalez的《数字图像处理》、Rafael C.Gonzalez / Richard E.Woods 的<a href="http://book.douban.com/subject/1106342/" target="_blank" rel="noopener">《数字图像处理》</a></p><ul><li><a href="http://pan.baidu.com/s/1sjFeLTN" target="_blank" rel="noopener">《LinkedIn最新的推荐系统文章Browsemaps》</a></li></ul><p>介绍：里面基本没涉及到具体算法，但作者介绍了CF在LinkedIn的很多应用，以及他们在做推荐过程中获得的一些经验。最后一条经验是应该监控log数据的质量，因为推荐的质量很依赖数据的质量！</p><ul><li><a href="http://blog.sina.com.cn/s/blog_574a437f01019poo.html" target="_blank" rel="noopener">《初学者如何查阅自然语言处理（NLP）领域学术资料》</a></li></ul><p>介绍：初学者如何查阅自然语言处理（NLP）领域学术资料</p><ul><li><a href="http://www.open-electronics.org/raspberry-pi-and-the-camera-pi-module-face-recognition-tutorial/" target="_blank" rel="noopener">《树莓派的人脸识别教程》</a></li></ul><p>介绍：用树莓派和相机模块进行人脸识别</p><ul><li><a href="http://www.hangli-hl.com/uploads/3/1/6/8/3168008/short_text_conversation_mla.pdf" target="_blank" rel="noopener">《利用深度学习与大数据构建对话系统  》</a></li></ul><p>介绍：如何利用深度学习与大数据构建对话系统 </p><ul><li><a href="http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf" target="_blank" rel="noopener">《经典论文Leo Breiman：Statistical Modeling: The Two Cultures  》</a></li></ul><p>介绍：Francis Bach合作的有关稀疏建模的新综述(书)：Sparse Modeling for Image and Vision Processing，内容涉及Sparsity, Dictionary Learning, PCA, Matrix Factorization等理论，以及在图像和视觉上的应用，而且第一部分关于Why does the l1-norm induce sparsity的解释也很不错。</p><ul><li><a href="http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf" target="_blank" rel="noopener">《Reproducing Kernel Hilbert Space》</a></li></ul><p>介绍：RKHS是机器学习中重要的概念，其在large margin分类器上的应用也是广为熟知的。如果没有较好的数学基础，直接理解RKHS可能会不易。本文从基本运算空间讲到Banach和Hilbert空间，深入浅出，一共才12页。</p><ul><li><a href="http://karpathy.github.io/neuralnets/" target="_blank" rel="noopener">《Hacker’s guide to Neural Networks》</a></li></ul><p>介绍：许多同学对于机器学习及深度学习的困惑在于，数学方面已经大致理解了，但是动起手来却不知道如何下手写代码。斯坦福深度学习博士Andrej Karpathy写了一篇实战版本的深度学习及机器学习教程，手把手教你用Javascript写神经网络和SVM.</p><ul><li><a href="http://blog.csdn.net/pandalibaba/article/details/17409395" target="_blank" rel="noopener">《【语料库】语料库资源汇总》</a></li></ul><p>介绍：【语料库】语料库资源汇总</p><ul><li><a href="http://blog.jobbole.com/60809/" target="_blank" rel="noopener">《机器学习算法之旅》</a></li></ul><p>介绍：本文会过一遍最流行的机器学习算法，大致了解哪些方法可用，很有帮助。</p><ul><li><a href="http://www.csee.wvu.edu/~xinl/source.html" target="_blank" rel="noopener">《Reproducible Research in Computational Science》</a></li></ul><p>介绍：这个里面有很多关于机器学习、信号处理、计算机视觉、深入学习、神经网络等领域的大量源代码（或可执行代码）及相关论文。科研写论文的好资源</p><ul><li><a href="http://cilvr.nyu.edu/doku.php?id=deeplearning:slides:start" target="_blank" rel="noopener">《NYU 2014年的深度学习课程资料》</a></li></ul><p>介绍：NYU 2014年的深度学习课程资料，有视频</p><ul><li><a href="https://github.com/memect/hao/blob/master/awesome/computer-vision-dataset.md" target="_blank" rel="noopener">《计算机视觉数据集不完全汇总》</a></li></ul><p>介绍：计算机视觉数据集不完全汇总</p><ul><li><a href="http://mloss.org/software/" target="_blank" rel="noopener">《Machine Learning Open Source Software》</a></li></ul><p>介绍：机器学习开源软件</p><ul><li><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" rel="noopener">《LIBSVM》</a></li></ul><p>介绍：A Library for Support Vector Machines</p><ul><li><a href="http://www.support-vector-machines.org/index.html" target="_blank" rel="noopener">《Support Vector Machines》</a></li></ul><p>介绍：<a href="files.cnblogs.com/tekson/数据挖掘之经典算法.doc">数据挖掘十大经典算法</a>之一</p><ul><li><a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/" target="_blank" rel="noopener">《100 Best GitHub: Deep Learning》</a></li></ul><p>介绍：github上面100个非常棒的项目</p><ul><li><a href="http://archive.ics.uci.edu/ml" target="_blank" rel="noopener">《加州大学欧文分校(UCI)机器学习数据集仓库》</a></li></ul><p>介绍：当前加州大学欧文分校为机器学习社区维护着306个数据集。<a href="http://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">查询数据集</a></p><ul><li><a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener">《Andrej Karpathy个人主页》</a></li></ul><p>介绍：Andrej Karpathy 是斯坦福大学Li Fei-Fei的博士生，使用机器学习在图像、视频语义分析领域取得了科研和工程上的突破，发的文章不多，但每个都很扎实，在每一个问题上都做到了state-of-art.</p><ul><li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html" target="_blank" rel="noopener">《Andrej Karpathy的深度强化学习演示》</a></li></ul><p>介绍：Andrej Karpathy的深度强化学习演示，<a href="http://arxiv.org/pdf/1312.5602v1.pdf" target="_blank" rel="noopener">论文在这里</a></p><ul><li><a href="http://www.52nlp.cn/cikm-competition-topdata" target="_blank" rel="noopener">《CIKM数据挖掘竞赛夺冠算法-陈运文》</a></li></ul><p>介绍：CIKM Cup(或者称为CIKM Competition)是ACM CIKM举办的国际数据挖掘竞赛的名称。</p><ul><li><a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="noopener">《Geoffrey E. Hinton》</a></li></ul><p>介绍：杰弗里·埃弗里斯特·辛顿 FRS是一位英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。辛顿是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者.</p><ul><li><a href="http://cikm2014.fudan.edu.cn/cikm2014/Tpl/Public/slides/CIKM14_tutorial_slides_6.pdf" target="_blank" rel="noopener">《自然语言处理的深度学习理论与实际》</a></li></ul><p>介绍：微软研究院深度学习技术中心在CIKM2014 上关于《自然语言处理的深度学习理论与实际》教学讲座的幻灯片</p><ul><li><a href="http://eugenezhulenev.com/blog/2014/11/14/stock-price-prediction-with-big-data-and-machine-learning/" target="_blank" rel="noopener">《用大数据和机器学习做股票价格预测》</a></li></ul><p>介绍： 本文基于&lt;支持向量机的高频限价订单的动态建模&gt;采用了 Apache Spark和Spark MLLib从纽约股票交易所的订单日志数据构建价格运动预测模型。(股票有风险，投资谨慎)GitHub源代码托管<a href="https://github.com/ezhulenev/orderbook-dynamics" target="_blank" rel="noopener">地址</a>.</p><ul><li><a href="http://dataunion.org/?p=2011" target="_blank" rel="noopener">《关于机器学习的若干理论问题》</a></li></ul><p>介绍：徐宗本 院士将于热爱机器学习的小伙伴一起探讨有关于机器学习的几个理论性问题，并给出一些有意义的结论。最后通过一些实例来说明这些理论问题的物理意义和实际应用价值。</p><ul><li><a href="http://vdisk.weibo.com/s/D2szyg_bBVM0" target="_blank" rel="noopener">《深度学习在自然语言处理的应用》</a></li></ul><p>介绍：作者还著有《这就是搜索引擎：核心技术详解》一书，主要是介绍应用层的东西</p><ul><li><a href="http://www.cs.ubc.ca/~nando/340-2012/index.php" target="_blank" rel="noopener">《Undergraduate machine learning at UBC》</a></li></ul><p>介绍：机器学习课程</p><ul><li><a href="http://blog.sina.com.cn/s/blog_6ae183910101h4jr.html" target="_blank" rel="noopener">《人脸识别必读的N篇文章》</a></li></ul><p>介绍：人脸识别必读文章推荐</p><ul><li><a href="http://semocean.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%96%87%E7%8C%AE%E5%8F%8A%E8%B5%84%E6%96%99/" target="_blank" rel="noopener">《推荐系统经典论文文献及业界应用》</a></li></ul><p>介绍：推荐系统经典论文文献</p><ul><li><a href="http://blog.sina.com.cn/s/blog_6ae183910101h4jr.html" target="_blank" rel="noopener">《人脸识别必读的N篇文章》</a></li></ul><p>介绍：人脸识别必读文章推荐</p><ul><li><a href="http://see.xidian.edu.cn/vipsl/MLA2014/program.htm" target="_blank" rel="noopener">《第十二届中国”机器学习及其应用”研讨会PPT》</a></li></ul><p>介绍：第十二届中国”机器学习及其应用”研讨会PPT</p><ul><li><a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=398" target="_blank" rel="noopener">《统计机器学习》</a></li></ul><p>介绍：统计学习是关于计算机基于数据构建的概率统计模型并运用模型对数据进行预测和分析的一门科学，统计学习也成为统计机器学习。课程来自上海交通大学</p><ul><li><a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=397" target="_blank" rel="noopener">《机器学习导论》</a></li></ul><p>介绍：机器学习的目标是对计算机编程，以便使用样本数据或以往的经验来解决给定的问题.</p><ul><li><a href="http://cikm2014.fudan.edu.cn/" target="_blank" rel="noopener">《CIKM 2014主题报告的幻灯片》</a></li></ul><p>介绍：CIKM 2014 Jeff Dean、Qi Lu、Gerhard Weikum的主题报告的幻灯片， Alex Smola、Limsoon Wong、Tong Zhang、Chih-Jen Lin的Industry Track报告的幻灯片</p><ul><li><a href="http://deeplearning.net/software_links/" target="_blank" rel="noopener">《人工智能和机器学习领域有趣的开源项目》</a></li></ul><p>介绍：部分中文<a href="http://code.csdn.net/news/2822818" target="_blank" rel="noopener">列表</a></p><ul><li><a href="http://blog.csdn.net/suipingsp/article/details/41645779" target="_blank" rel="noopener">《机器学习经典算法详解及Python实现—基于SMO的SVM分类器》</a></li></ul><p>介绍:此外作者还有一篇<a href="http://blog.csdn.net/suipingsp/article/details/41722435" target="_blank" rel="noopener">元算法、AdaBoost　python实现文章</a></p><ul><li><a href="http://aria42.com/blog/2014/12/understanding-lbfgs/" target="_blank" rel="noopener">《Numerical Optimization: Understanding L-BFGS》</a></li></ul><p>介绍:加州伯克利大学博士Aria Haghighi写了一篇超赞的数值优化博文，从牛顿法讲到拟牛顿法，再讲到BFGS以及L-BFGS, 图文并茂，还有伪代码。强烈推荐。</p><ul><li><a href="http://www.goldencui.org/2014/12/02/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89/" target="_blank" rel="noopener">《简明深度学习方法概述（一）》</a></li></ul><p>介绍:还有续集<a href="http://www.goldencui.org/2014/12/06/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%BA%8C%EF%BC%89/" target="_blank" rel="noopener">简明深度学习方法概述（二）</a></p><ul><li><a href="http://www.johndcook.com/blog/r_language_for_programmers/" target="_blank" rel="noopener">《R language for programmers》</a></li></ul><p>介绍:Ｒ语言程序员私人定制版</p><ul><li><a href="http://www.cheyun.com/content/news/4051" target="_blank" rel="noopener">《谷歌地图解密：大数据与机器学习的结合》</a></li></ul><p>介绍:谷歌地图解密</p><ul><li><a href="http://blog.csdn.net/u012690204/article/details/41853731" target="_blank" rel="noopener">《空间数据挖掘常用方法》</a></li></ul><p>介绍:空间数据挖掘常用方法</p><ul><li><a href="https://www.kaggle.com/c/word2vec-nlp-tutorial" target="_blank" rel="noopener">《Use Google’s Word2Vec for movie reviews》</a></li></ul><p>介绍:Kaggle新比赛 ”When bag of words meets bags of popcorn“ aka ”边学边用word2vec和deep learning做NLP“ 里面全套教程教一步一步用python和gensim包的word2vec模型，并在实际比赛里面比调参数和清数据。 如果已装过gensim不要忘升级</p><ul><li><a href="http://pynlpir.readthedocs.org/en/latest/" target="_blank" rel="noopener">《PyNLPIR》</a></li></ul><p>介绍:PyNLPIR提供了NLPIR/ICTCLAS汉语分词的Python接口,此外<a href="http://zhon.readthedocs.org/en/latest/" target="_blank" rel="noopener">Zhon</a>提供了常用汉字常量，如CJK字符和偏旁，中文标点，拼音，和汉字正则表达式（如找到文本中的繁体字）</p><ul><li><a href="http://www.technologyreview.com/view/533496/why-neural-networks-look-set-to-thrash-the-best-human-go-players-for-the-first-time/" target="_blank" rel="noopener">《深度卷积神经网络下围棋》</a></li></ul><p>介绍:这文章说把最近模型识别上的突破应用到围棋软件上，打16万张职业棋谱训练模型识别功能。想法不错。训练后目前能做到不用计算，只看棋盘就给出下一步，大约10级棋力。但这篇文章太过乐观，说什么人类的最后一块堡垒马上就要跨掉了。话说得太早。不过，如果与别的软件结合应该还有潜力可挖。@万精油墨绿</p><ul><li><a href="http://mrtz.org/blog/the-nips-experiment/" target="_blank" rel="noopener">《NIPS审稿实验》</a></li></ul><p>介绍:UT Austin教授Eric Price关于今年NIPS审稿实验的详细分析,他表示，根据这次实验的结果，如果今年NIPS重新审稿的话，会有一半的论文被拒。</p><ul><li><a href="http://www.kdnuggets.com/2014/12/top-kdnuggets-2014-analytics-big-data-science-stories.html" target="_blank" rel="noopener">《2014年最佳的大数据，数据科学文章》</a></li></ul><p>介绍:KDNuggets分别总结了2014年14个阅读最多以及分享最多的文章。我们从中可以看到多个主题——深度学习，数据科学家职业，教育和薪酬，学习数据科学的工具比如R和Python以及大众投票的最受欢迎的数据科学和数据挖掘语言</p><ul><li><a href="http://blog.csdn.net/suipingsp/article/details/42101139" target="_blank" rel="noopener">《机器学习经典算法详解及Python实现—线性回归（Linear Regression）算法》</a></li></ul><p>介绍:Python实现线性回归,作者还有其他很棒的文章推荐可以看看</p><ul><li><a href="http://download.csdn.net/album/detail/1367/1/1" target="_blank" rel="noopener">《2014中国大数据技术大会33位核心专家演讲PDF》</a></li></ul><p>介绍：2014中国大数据技术大会33位核心专家演讲PDF下载</p><ul><li><a href="http://arxiv.org/abs/1412.5335" target="_blank" rel="noopener">《使用RNN和Paragraph Vector做情感分析》</a></li></ul><p>介绍：这是T. Mikolov &amp; Y. Bengio最新论文Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews ，使用RNN和PV在情感分析效果不错，［项目代码］(<a href="https://github.com/mesnilgr/iclr15)公布在github(目前是空的)。这意味着Paragraph" target="_blank" rel="noopener">https://github.com/mesnilgr/iclr15)公布在github(目前是空的)。这意味着Paragraph</a> Vector终于揭开面纱了嘛。</p><ul><li><a href="http://pan.baidu.com/s/1o6I9S18" target="_blank" rel="noopener">《NLPIR/ICTCLAS2015分词系统大会上的技术演讲 》</a></li></ul><p>介绍:NLPIR/ICTCLAS2015分词系统发布与用户交流大会上的演讲，请更多朋友检阅新版分词吧。  我们实验室同学的演讲包括：<a href="http://pan.baidu.com/s/1hqotVVm" target="_blank" rel="noopener">孙梦姝-基于评论观点挖掘的商品搜索技术研究</a> <a href="http://pan.baidu.com/s/1pJ9KuZh" target="_blank" rel="noopener">李然-主题模型</a></p><ul><li><a href="https://medium.com/code-poet/80ea3ec3c471" target="_blank" rel="noopener">《Machine Learning is Fun!》</a></li></ul><p>介绍:Convex Neural Networks 解决维数灾难 </p><ul><li><a href="http://dataunion.org/?p=5395" target="_blank" rel="noopener">《CNN的反向求导及练习》</a></li></ul><p>介绍:介绍CNN参数在使用bp算法时该怎么训练，毕竟CNN中有卷积层和下采样层，虽然和MLP的bp算法本质上相同，但形式上还是有些区别的，很显然在完成CNN反向传播前了解bp算法是必须的。此外作者也做了一个<a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html" target="_blank" rel="noopener">资源集:机器学习，深度学习，视觉，数学等</a></p><ul><li><a href="https://github.com/cloudflare/ahocorasick" target="_blank" rel="noopener">《正则表达式优化成Trie树 》</a></li></ul><p>介绍:如果要在一篇文章中匹配十万个关键词怎么办？<a href="https://github.com/cloudflare/ahocorasick" target="_blank" rel="noopener">Aho-Corasick</a> 算法利用添加了返回边的Trie树，能够在线性时间内完成匹配。 但如果匹配十万个正则表达式呢 ？ 这时候可以用到把多个正则优化成Trie树的方法，如日本人写的 <a href="http://search.cpan.org/~dankogai/Regexp-Trie-0.02/" target="_blank" rel="noopener">Regexp::Trie</a></p><ul><li><a href="http://jmozah.github.io/links/" target="_blank" rel="noopener">《Deep learning Reading List》</a></li></ul><p>介绍:深度学习阅读清单</p><ul><li><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">《Caffe》</a></li></ul><p>介绍:Caffe是一个开源的深度学习框架，作者目前在google工作，作者主页<a href="http://daggerfs.com/index.html" target="_blank" rel="noopener">Yangqing Jia (贾扬清)</a></p><ul><li><a href="https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/readme.md" target="_blank" rel="noopener">《GoogLeNet深度学习模型的Caffe复现 》</a></li></ul><p>介绍:2014 ImageNet冠军GoogLeNet深度学习模型的Caffe复现模型,<a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">GoogleNet论文</a>.</p><ul><li><a href="https://github.com/jbarrow/LambdaNet" target="_blank" rel="noopener">《LambdaNet，Haskell实现的开源人工神经网络库 》</a></li></ul><p>介绍:LambdaNetLambdaNet是由Haskell实现的一个开源的人工神经网络库，它抽象了网络创建、训练并使用了高阶函数。该库还提供了一组预定义函数，用户可以采取多种方式组合这些函数来操作现实世界数据。</p><ul><li><a href="http://wenku.baidu.com/course/view/49e8b8f67c1cfad6195fa705" target="_blank" rel="noopener">《百度余凯&amp;张潼机器学习视频》</a></li></ul><p>介绍:如果你从事互联网搜索，在线广告，用户行为分析，图像识别，自然语言理解，或者生物信息学，智能机器人，金融预测，那么这门核心课程你必须深入了解。</p><ul><li><a href="http://v.youku.com/v_show/id_XODQzNDM4MDg0.html" target="_blank" rel="noopener">《杨强在TEDxNanjing谈智能的起源》</a></li></ul><p>介绍:”人工智能研究分许多流派。其中之一以IBM为代表，认为只要有高性能计算就可得到智能，他们的‘深蓝’击败了世界象棋冠军；另一流派认为智能来自动物本能；还有个很强的流派认为只要找来专家，把他们的思维用逻辑一条条写下，放到计算机里就行……” 杨强在TEDxNanjing谈智能的起源</p><ul><li><a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf" target="_blank" rel="noopener">《深度RNN/LSTM用于结构化学习 0)序列标注Connectionist Temporal ClassificationICML06》</a></li></ul><p>介绍:1)机器翻译<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sequence to Sequence NIPS14</a> 2)成分句法<a href="http://arxiv.org/pdf/1412.7449v1.pdf" target="_blank" rel="noopener">GRAMMAR AS FOREIGN LANGUAGE</a></p><ul><li><a href="http://techblog.youdao.com/?p=915" target="_blank" rel="noopener">《Deep Learning实战之word2vec》</a></li></ul><p>介绍:网易有道的三位工程师写的word2vec的解析文档，从基本的词向量/统计语言模型-&gt;NNLM-&gt;Log-Linear/Log-Bilinear-&gt;层次化Log-Bilinear，到CBOW和Skip-gram模型，再到word2vec的各种tricks，公式推导与代码，基本上是网上关于word2vec资料的大合集，对word2vec感兴趣的朋友可以看看</p><ul><li><a href="http://mloss.org/software/" target="_blank" rel="noopener">《Machine learning open source software》</a></li></ul><p>介绍:机器学习开源软件,收录了各种机器学习的各种编程语言学术与商业的开源软件．与此类似的还有很多例如:<a href="http://www.dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/" target="_blank" rel="noopener">DMOZ - Computers: Artificial Intelligence: Machine Learning: Software</a>,　<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" rel="noopener">LIBSVM — A Library for Support Vector Machines</a>,　<a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank" rel="noopener">Weka 3: Data Mining Software in Java</a>,　<a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit-learn:Machine Learning in Python</a>,　<a href="www.nltk.org">Natural Language Toolkit:NLTK</a>,　<a href="http://mallet.cs.umass.edu/" target="_blank" rel="noopener">MAchine Learning for LanguagE Toolkit</a>,　<a href="http://orange.biolab.si/" target="_blank" rel="noopener">Data Mining - Fruitful and Fun</a>,　<a href="http://opencv.willowgarage.com/wiki/" target="_blank" rel="noopener">Open Source Computer Vision Library</a></p><ul><li><a href="http://www.guokr.com/post/512037/" target="_blank" rel="noopener">《机器学习入门者学习指南》</a></li></ul><p>介绍:作者是计算机研二(写文章的时候，现在是2015年了应该快要毕业了)，专业方向自然语言处理．这是一点他的经验之谈．对于入门的朋友或许会有帮助</p><ul><li><a href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">《A Tour of Machine Learning Algorithms》</a></li></ul><p>介绍:这是一篇关于机器学习算法分类的文章，非常好</p><ul><li><a href="http://ml.memect.com/download/2014.zip" target="_blank" rel="noopener">《2014年的《机器学习日报》大合集》</a></li></ul><p>介绍:机器学习日报里面推荐很多内容，在这里有一部分的优秀内容就是来自机器学习日报．</p><ul><li><a href="http://blog.csdn.net/abcjennifer/article/details/42493493" target="_blank" rel="noopener">《 Image classification with deep learning常用模型》</a></li></ul><p>介绍:这是一篇关于图像分类在深度学习中的文章</p><ul><li><a href="http://research.microsoft.com/en-us/people/deng/" target="_blank" rel="noopener">《自动语音识别：深度学习方法》</a></li></ul><p>介绍:作者与Bengio的兄弟Samy 09年合编《自动语音识别：核方法》 3）李开复1989年《自动语音识别》专著，其博导、94年图灵奖得主Raj Reddy作序</p><ul><li><a href="http://blog.csdn.net/heiyeshuwu/article/details/42554903" target="_blank" rel="noopener">《NLP中的中文分词技术》</a></li></ul><p>介绍: 作者是360电商技术组成员,这是一篇NLP在中文分词中的应用</p><ul><li><a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" target="_blank" rel="noopener">《Using convolutional neural nets to detect facial keypoints tutorial》</a></li></ul><p>介绍: 使用deep learning的人脸关键点检测，此外还有一篇<a href="https://www.kaggle.com/c/facial-keypoints-detection/details/deep-learning-tutorial" target="_blank" rel="noopener">AWS部署教程</a></p><ul><li><a href="http://www.amazon.cn/Advanced-Structured-Prediction-Nowozin-Sebastian/dp/0262028379" target="_blank" rel="noopener">《书籍推荐:Advanced Structured Prediction》</a></li></ul><p>介绍: 由Sebastian Nowozin等人编纂MIT出版的新书《Advanced Structured Prediction》<a href="http://t.cn/RZxipKG" target="_blank" rel="noopener">http://t.cn/RZxipKG</a> ，汇集了结构化预测领域诸多牛文，涉及CV、NLP等领域，值得一读。网上公开的几章草稿:<a href="http://www2.informatik.hu-berlin.de/~kloftmar/publications/strucBook.pdf" target="_blank" rel="noopener">一</a>,<a href="http://mlg.eng.cam.ac.uk/yutian/Publications/ChenGelfandWelling14-HerdingBookChapter.pdf" target="_blank" rel="noopener">二</a>,<a href="http://web.engr.oregonstate.edu/~sinisa/research/publications/StructPredictionChapter14.pdf" target="_blank" rel="noopener">三</a>,<a href="http://ttic.uchicago.edu/~meshi/papers/smoothCD_chapter.pdf" target="_blank" rel="noopener">四</a>,<a href="http://www.cs.ox.ac.uk/Stanislav.Zivny/homepage/publications/zwp14mit-draft.pdf" target="_blank" rel="noopener">五</a></p><ul><li><a href="http://arxiv.org/pdf/1501.01571v1.pdf" target="_blank" rel="noopener">《An Introduction to Matrix Concentration Inequalities》</a></li></ul><p>介绍: Tropp把数学家用高深装逼的数学语言写的矩阵概率不等式用初等的方法写出来，是非常好的手册，领域内的paper各种证明都在用里面的结果。虽说是初等的，但还是非常的难</p><ul><li><a href="https://agenda.weforum.org/2014/12/the-free-big-data-sources-you-should-know/" target="_blank" rel="noopener">《The free big data sources you should know》</a></li></ul><p>介绍: 不容错过的免费大数据集，有些已经是耳熟能详，有些可能还是第一次听说，内容跨越文本、数据、多媒体等，让他们伴你开始数据科学之旅吧，具体包括：Data.gov、US Census Bureau、European Union Open Data Portal、Data.gov.uk等</p><ul><li><a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html" target="_blank" rel="noopener">《A Brief Overview of Deep Learning》</a></li></ul><p>介绍: 谷歌科学家、Hinton亲传弟子Ilya Sutskever的深度学习综述及实际建议</p><ul><li><a href="http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/" target="_blank" rel="noopener">《A Deep Dive into Recurrent Neural Nets》</a></li></ul><p>介绍: 非常好的讨论递归神经网络的文章，覆盖了RNN的概念、原理、训练及优化等各个方面内容，强烈推荐！本文作者Nikhil Buduma还有一篇<a href="http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/" target="_blank" rel="noopener">Deep Learning in a Nutshell</a>值得推荐</p><ul><li><a href="http://qianjiye.de/2014/11/machine-learning-resources/" target="_blank" rel="noopener">《机器学习：学习资源》</a></li></ul><p>介绍:里面融合了很多的资源，例如竞赛，在线课程，demo，数据整合等。有分类</p><ul><li><a href="https://www.otexts.org/book/sfml" target="_blank" rel="noopener">《Statistical foundations of machine learning》</a></li></ul><p>介绍:《机器学习的统计基础》在线版，该手册希望在理论与实践之间找到平衡点，各主要内容都伴有实际例子及数据，书中的例子程序都是用R语言编写的。</p><ul><li><a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks" target="_blank" rel="noopener">《A Deep Learning Tutorial: From Perceptrons to Deep Networks》</a></li></ul><p>介绍:IVAN VASILEV写的深度学习导引：从浅层感知机到深度网络。高可读</p><ul><li><a href="http://futureoflife.org/static/data/documents/research_priorities.pdf" target="_blank" rel="noopener">《Research priorities for robust and beneficial artificial intelligence》</a></li></ul><p>介绍:鲁棒及有益的人工智能优先研究计划：一封公开信,目前已经有Stuart Russell, Tom Dietterich, Eric Horvitz, Yann LeCun, Peter Norvig, Tom Mitchell, Geoffrey Hinton, Elon Musk等人签署<a href="http://futureoflife.org/misc/open_letter" target="_blank" rel="noopener">The Future of Life Institute (FLI)</a>.这封信的背景是最近霍金和Elon Musk提醒人们注意AI的潜在威胁。公开信的内容是AI科学家们站在造福社会的角度，展望人工智能的未来发展方向，提出开发AI系统的Verification，Validity, Security, Control四点要求，以及需要注意的社会问题。毕竟当前AI在经济领域，法律，以及道德领域相关研究较少。其实还有一部美剧<a href="http://tv.sohu.com/20120925/n353925789.shtml" target="_blank" rel="noopener">《疑犯追踪》</a>,介绍了AI的演进从一开始的自我学习，过滤，图像识别，语音识别等判断危险，到第四季的时候出现了机器通过学习成长之后想控制世界的状态。说到这里推荐收看。</p><ul><li><a href="http://metacademy.org/" target="_blank" rel="noopener">《metacademy》</a></li></ul><p>介绍:里面根据词条提供了许多资源，还有相关知识结构，路线图，用时长短等。号称是”机器学习“搜索引擎</p><ul><li><a href="https://research.facebook.com/blog/879898285375829/fair-open-sources-deep-learning-modules-for-torch/" target="_blank" rel="noopener">《FAIR open sources deep-learning modules for Torch》</a></li></ul><p>介绍:Facebook人工智能研究院（FAIR）开源了一系列软件库，以帮助开发者建立更大、更快的深度学习模型。开放的软件库在 Facebook 被称作模块。用它们替代机器学习领域常用的开发环境 Torch 中的默认模块，可以在更短的时间内训练更大规模的神经网络模型。</p><ul><li><a href="http://www.cnblogs.com/ello/archive/2012/04/28/2475419.html" target="_blank" rel="noopener">《浅析人脸检测之Haar分类器方法》</a></li></ul><p>介绍:本文虽然是写于2012年，但是这篇文章完全是作者的经验之作。</p><ul><li><a href="http://www.ituring.com.cn/article/55994" target="_blank" rel="noopener">《如何成为一位数据科学家》</a></li></ul><p>介绍:本文是对《机器学习实战》作者Peter Harrington做的一个访谈。包含了书中部分的疑问解答和一点个人学习建议</p><ul><li><a href="http://www.metacademy.org/roadmaps/rgrosse/deep_learning" target="_blank" rel="noopener">《Deep learning from the bottom up》</a></li></ul><p>介绍:非常好的深度学习概述，对几种流行的深度学习模型都进行了介绍和讨论</p><ul><li><a href="http://onepager.togaware.com/TextMiningO.pdf" target="_blank" rel="noopener">《Hands-On Data Science with R Text Mining》</a></li></ul><p>介绍:主要是讲述了利用R语言进行数据挖掘</p><ul><li><a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="noopener">《Understanding Convolutions》</a></li></ul><p>介绍:帮你理解卷积神经网络，讲解很清晰，此外还有两篇<a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="noopener">Conv Nets: A Modular Perspective</a>，<a href="http://colah.github.io/posts/2014-12-Groups-Convolution/" target="_blank" rel="noopener">Groups &amp; Group Convolutions</a>. 作者的其他的关于神经网络文章也很棒</p><ul><li><a href="http://www.iro.umontreal.ca/~pift6266/H10/notes/deepintro.html#introduction-to-deep-learning-algorithms" target="_blank" rel="noopener">《Introduction to Deep Learning Algorithms》</a></li></ul><p>介绍:Deep Learning算法介绍，里面介绍了06年3篇让deep learning崛起的论文</p><ul><li><a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf" target="_blank" rel="noopener">《Learning Deep Architectures for AI》</a></li></ul><p>介绍:一本学习人工智能的书籍，作者是Yoshua Bengio，相关<a href="http://www.infoq.com/cn/articles/ask-yoshua-bengio" target="_blank" rel="noopener">国内报道</a></p><ul><li><a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="noopener">《Geoffrey E. Hinton个人主页》</a></li></ul><p>介绍:Geoffrey Hinton是Deep Learning的大牛，他的主页放了一些介绍性文章和课件值得学习</p><ul><li><a href="http://omega.albany.edu:8008/JaynesBook.html" target="_blank" rel="noopener">《PROBABILITY THEORY: THE LOGIC OF SCIENCE》</a></li></ul><p>介绍:概率论：数理逻辑书籍</p><ul><li><a href="https://github.com/h2oai/h2o" target="_blank" rel="noopener">《H2O》</a></li></ul><p>介绍:一个用来快速的统计，机器学习并且对于数据量大的数学库</p><ul><li><a href="http://www.iclr.cc/doku.php?id=iclr2015:main" target="_blank" rel="noopener">《ICLR 2015会议的arXiv稿件合集》</a></li></ul><p>介绍:在这里你可以看到最近深度学习有什么新动向。</p><ul><li><a href="http://www-nlp.stanford.edu/IR-book/" target="_blank" rel="noopener">《Introduction to Information Retrieval》</a></li></ul><p>介绍:此书在信息检索领域家喻户晓， 除提供该书的免费电子版外，还提供一个<a href="http://www-nlp.stanford.edu/IR-book/information-retrieval.html" target="_blank" rel="noopener">IR资源列表</a> ，收录了信息检索、网络信息检索、搜索引擎实现等方面相关的图书、研究中心、相关课程、子领域、会议、期刊等等，堪称全集，值得收藏</p><ul><li><a href="http://yosinski.com/mlss12/MLSS-2012-Amari-Information-Geometry/" target="_blank" rel="noopener">《Information Geometry and its Applications to Machine Learning》</a></li></ul><p>介绍:信息几何学及其在机器学习中的应用</p><ul><li><a href="http://computationallegalstudies.com/2015/01/legal-analytics-introduction-course-professors-daniel-martin-katz-michael-j-bommarito/" target="_blank" rel="noopener">《Legal Analytics – Introduction to the Course》</a></li></ul><p>介绍:课程《法律分析》介绍幻灯片。用机器学习解决法律相关分析和预测问题，相关的法律应用包括预测编码、早期案例评估、案件整体情况的预测，定价和工作人员预测，司法行为预测等。法律领域大家可能都比较陌生，不妨了解下。</p><ul><li><a href="https://github.com/yanxionglu/text_pdf" target="_blank" rel="noopener">《文本上的算法》</a></li></ul><p>介绍: 文中提到了最优，模型，最大熵等等理论，此外还有应用篇。推荐系统可以说是一本不错的阅读稿，关于模型还推荐一篇<a href="http://blog.sina.com.cn/s/blog_6742eecd0100iqcv.html" target="_blank" rel="noopener">Generative Model 与 Discriminative Model</a></p><ul><li><a href="https://github.com/karpathy/neuraltalk" target="_blank" rel="noopener">《NeuralTalk》</a></li></ul><p>介绍: NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.NeuralTalk是一个Python的从图像生成自然语言描述的工具。它实现了Google (Vinyals等，卷积神经网络CNN + 长短期记忆LSTM) 和斯坦福 (Karpathy and Fei-Fei， CNN + 递归神经网络RNN)的算法。NeuralTalk自带了一个训练好的动物模型，你可以拿狮子大象的照片来试试看</p><ul><li><a href="https://www.paypal-engineering.com/2015/01/12/deep-learning-on-hadoop-2-0-2/" target="_blank" rel="noopener">《Deep Learning on Hadoop 2.0》</a></li></ul><p>介绍:本文主要介绍了在Hadoop2.0上使用深度学习,文章来自paypal</p><ul><li><a href="http://arxiv.org/abs/1206.5533" target="_blank" rel="noopener">《Practical recommendations for gradient-based training of deep architectures》</a></li></ul><p>介绍:用基于梯度下降的方法训练深度框架的实践推荐指导,作者是<a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html" target="_blank" rel="noopener">Yoshua Bengio</a> .感谢@xuewei4d 推荐</p><ul><li><a href="http://machinelearningmastery.com/machine-learning-statistical-causal-methods/" target="_blank" rel="noopener">《Machine Learning With Statistical And Causal Methods》</a></li></ul><p>介绍: 用统计和因果方法做机器学习（视频报告）</p><ul><li><a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA" target="_blank" rel="noopener">《Machine Learning Course 180’》</a></li></ul><p>介绍: 一个讲机器学习的Youtube视频教程。160集。系统程度跟书可比拟。</p><ul><li><a href="http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html" target="_blank" rel="noopener">《回归(regression)、梯度下降(gradient descent)》</a></li></ul><p>介绍: 机器学习中的数学，作者的研究方向是机器学习，并行计算如果你还想了解一点其他的可以看看他<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/recommended-blogspots.html" target="_blank" rel="noopener">博客</a>的其他文章</p><ul><li><a href="http://tech.meituan.com/mt-recommend-practice.html" target="_blank" rel="noopener">《美团推荐算法实践》</a></li></ul><p>介绍: 美团推荐算法实践，从框架，应用，策略，查询等分析</p><ul><li><a href="http://arxiv.org/abs/1412.1632" target="_blank" rel="noopener">《Deep Learning for Answer Sentence Selection》</a></li></ul><p>介绍: 深度学习用于问答系统答案句的选取 </p><ul><li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/WWW2014.pdf" target="_blank" rel="noopener">《Learning Semantic Representations Using Convolutional Neural Networks for Web Search 》</a></li></ul><p>介绍: CNN用于WEB搜索，深度学习在文本计算中的应用</p><ul><li><a href="https://github.com/caesar0301/awesome-public-datasets" target="_blank" rel="noopener">《Awesome Public Datasets》</a></li></ul><p>介绍: Awesome系列中的公开数据集</p><ul><li><a href="http://www.academics.io/" target="_blank" rel="noopener">《Search Engine &amp; Community》</a></li></ul><p>介绍: 一个学术搜索引擎</p><ul><li><a href="http://honnibal.github.io/spaCy/" target="_blank" rel="noopener">《spaCy》</a></li></ul><p>介绍: 用Python和Cython写的工业级自然语言处理库，号称是速度最快的NLP库，快的原因一是用Cython写的，二是用了个很巧妙的hash技术，加速系统的瓶颈，NLP中稀松特征的存取</p><ul><li><a href="http://fr.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark" target="_blank" rel="noopener">《Collaborative Filtering with Spark》</a></li></ul><p>介绍: <a href="http://www.fields.utoronto.ca/video-archive/event/323/2014" target="_blank" rel="noopener">Fields</a>是个数学研究中心,上面的这份ppt是来自Fields举办的活动中Russ Salakhutdinov带来的《大规模机器学习》分享</p><ul><li><a href="http://www.7300days.com/index.php/stds/topic/list/id/27/name/Topic%20modeling" target="_blank" rel="noopener">《Topic modeling 的经典论文》</a></li></ul><p>介绍: Topic modeling 的经典论文,标注了关键点</p><ul><li><a href="http://arxiv.org/abs/1412.6564" target="_blank" rel="noopener">《Move Evaluation in Go Using Deep Convolutional Neural Networks》</a></li></ul><p>介绍: 多伦多大学与Google合作的新论文，深度学习也可以用来下围棋，据说能达到六段水平</p><ul><li><a href="http://ztl2004.github.io/MachineLearningWeekly/issue2.html" target="_blank" rel="noopener">《机器学习周刊第二期》</a></li></ul><p>介绍: 新闻，paper,课程，book，system,CES,Roboot，此外还推荐一个<a href="http://blog.newitfarmer.com/ai/deep-learning/15302/repost-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%8E%E7%BB%BC%E8%BF%B0%E8%B5%84%E6%96%99" target="_blank" rel="noopener">深度学习入门与综述资料</a></p><ul><li><a href="http://www.bigdata-madesimple.com/learning-more-like-a-human-18-free-ebooks-on-machine-learning/" target="_blank" rel="noopener">《Learning more like a human: 18 free eBooks on Machine Learning》</a></li></ul><p>介绍: 18 free eBooks on Machine Learning</p><ul><li><a href="http://www.hangli-hl.com/" target="_blank" rel="noopener">《Recommend :Hang Li Home》</a></li></ul><p>介绍:Chief scientist of Noah’s Ark Lab of Huawei Technologies.He worked at the Research Laboratories of NEC Corporation during 1990 and 2001 and Microsoft Research Asia during 2001 and 2012.<a href="http://www.hangli-hl.com/recent-publications.html" target="_blank" rel="noopener">Paper</a></p><ul><li><a href="http://memkite.com/deep-learning-bibliography/" target="_blank" rel="noopener">《DEEPLEARNING.UNIVERSITY – AN ANNOTATED DEEP LEARNING BIBLIOGRAPHY》</a></li></ul><p>介绍: DEEPLEARNING.UNIVERSITY的论文库已经收录了963篇经过分类的深度学习论文了，很多经典论文都已经收录</p><ul><li><a href="https://www.youtube.com/watch?v=wTp3P2UnTfQ&amp;hd=1" target="_blank" rel="noopener">《MLMU.cz - Radim Řehůřek - Word2vec &amp; friends (7.1.2015)》</a></li></ul><p>介绍: Radim Řehůřek(Gensim开发者)在一次机器学习聚会上的报告，关于word2vec及其优化、应用和扩展，很实用.<a href="http://pan.baidu.com/s/1c03wd24" target="_blank" rel="noopener">国内网盘</a></p><ul><li><a href="http://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html" target="_blank" rel="noopener">《Introducing streaming k-means in Spark 1.2》</a></li></ul><p>介绍:很多公司都用机器学习来解决问题，提高用户体验。那么怎么可以让机器学习更实时和有效呢？Spark MLlib 1.2里面的Streaming K-means，由斑马鱼脑神经研究的Jeremy Freeman脑神经科学家编写，最初是为了实时处理他们每半小时1TB的研究数据，现在发布给大家用了。</p><ul><li><a href="http://www.hankcs.com/nlp/lda-java-introduction-and-implementation.html" target="_blank" rel="noopener">《LDA入门与Java实现》</a></li></ul><p>介绍: 这是一篇面向工程师的LDA入门笔记，并且提供一份开箱即用Java实现。本文只记录基本概念与原理，并不涉及公式推导。文中的LDA实现核心部分采用了arbylon的LdaGibbsSampler并力所能及地注解了，在搜狗分类语料库上测试良好，开源在<a href="https://github.com/hankcs/LDA4j" target="_blank" rel="noopener">GitHub</a>上。</p><ul><li><a href="http://aminer.org/" target="_blank" rel="noopener">《AMiner - Open Science Platform》</a></li></ul><p>介绍: AMiner是一个学术搜索引擎，从学术网络中挖掘深度知识、面向科技大数据的挖掘。收集近4000万作者信息、8000万论文信息、1亿多引用关系、链接近8百万知识点；支持专家搜索、机构排名、科研成果评价、会议排名。</p><ul><li><a href="https://www.quora.com/What-are-some-interesting-Word2Vec-results" target="_blank" rel="noopener">《What are some interesting Word2Vec results?》</a></li></ul><p>介绍: Quora上的主题，讨论Word2Vec的有趣应用，Omer Levy提到了他在CoNLL2014最佳论文里的分析结果和新方法，Daniel Hammack给出了找特异词的小应用并提供了<a href="https://github.com/dhammack/Word2VecExample" target="_blank" rel="noopener">(Python)代码</a></p><ul><li><a href="http://blog.coursegraph.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB" target="_blank" rel="noopener">《机器学习公开课汇总》</a></li></ul><p>介绍: 机器学习公开课汇总,虽然里面的有些课程已经归档过了，但是还有个别的信息没有。感谢课程图谱的小编</p><ul><li><a href="http://linear.ups.edu/download.html" target="_blank" rel="noopener">《A First Course in Linear Algebra》</a></li></ul><p>介绍: 【A First Course in Linear Algebra】Robert Beezer 有答案 有移动版、打印版 使用GNU自由文档协议 引用了杰弗逊1813年的信</p><ul><li><a href="https://github.com/ShiqiYu/libfacedetection" target="_blank" rel="noopener">《libfacedetection》</a></li></ul><p>介绍:libfacedetection是深圳大学开源的一个人脸图像识别库。包含正面和多视角人脸检测两个算法.优点:速度快(OpenCV haar+adaboost的2-3倍), 准确度高 (FDDB非公开类评测排名第二），能估计人脸角度。</p><ul><li><a href="http://dl.acm.org/citation.cfm?doid=2684822.2685310" target="_blank" rel="noopener">《Inverting a Steady-State》</a></li></ul><p>介绍:WSDM2015最佳论文 把马尔可夫链理论用在了图分析上面，比一般的propagation model更加深刻一些。通过全局的平稳分布去求解每个节点影响系数模型。假设合理（转移受到相邻的影响系数影响）。可以用来反求每个节点的影响系数</p><ul><li><a href="http://pan.baidu.com/s/1pJogO7x" target="_blank" rel="noopener">《机器学习入门书单》</a></li></ul><p>介绍:机器学习入门书籍，<a href="http://www.hankcs.com/ml/machine-learning-entry-list.html" target="_blank" rel="noopener">具体介绍</a></p><ul><li><a href="http://v1v3kn.tumblr.com/post/47193952400/the-trouble-with-svms" target="_blank" rel="noopener">《The Trouble with SVMs》</a></li></ul><p>介绍: 非常棒的强调特征选择对分类器重要性的文章。情感分类中，根据互信息对复杂高维特征降维再使用朴素贝叶斯分类器，取得了比SVM更理想的效果，训练和分类时间也大大降低——更重要的是，不必花大量时间在学习和优化SVM上——特征也一样no free lunch</p><ul><li><a href="http://www.stat.cmu.edu/~larry/Wasserman.pdf" target="_blank" rel="noopener">《Rise of the Machines》</a></li></ul><p>介绍:CMU的统计系和计算机系知名教授Larry Wasserman 在《机器崛起》,对比了统计和机器学习的差异</p><ul><li><a href="http://tech.meituan.com/mt-mlinaction-how-to-ml.html" target="_blank" rel="noopener">《实例详解机器学习如何解决问题》</a></li></ul><p>介绍:随着大数据时代的到来，机器学习成为解决问题的一种重要且关键的工具。不管是工业界还是学术界，机器学习都是一个炙手可热的方向，但是学术界和工业界对机器学习的研究各有侧重，学术界侧重于对机器学习理论的研究，工业界侧重于如何用机器学习来解决实际问题。这篇文章是美团的实际环境中的实战篇</p><ul><li><a href="http://www.gaussianprocess.org/gpml/" target="_blank" rel="noopener">《Gaussian Processes for Machine Learning》</a></li></ul><p>介绍:面向机器学习的高斯过程，章节概要：回归、分类、协方差函数、模型选择与超参优化、高斯模型与其他模型关系、大数据集的逼近方法等,<a href="http://vdisk.weibo.com/s/ayG13we2vfWuT" target="_blank" rel="noopener">微盘下载</a></p><ul><li><a href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" target="_blank" rel="noopener">《FuzzyWuzzy: Fuzzy String Matching in Python》</a></li></ul><p>介绍:Python下的文本模糊匹配库，老库新推，可计算串间ratio(简单相似系数)、partial_ratio(局部相似系数)、token_sort_ratio(词排序相似系数)、token_set_ratio(词集合相似系数)等 <a href="https://github.com/seatgeek/fuzzywuzzy" target="_blank" rel="noopener">github</a></p><ul><li><a href="http://blocks.readthedocs.org/en/latest/" target="_blank" rel="noopener">《Blocks》</a></li></ul><p>介绍:Blocks是基于Theano的神经网络搭建框架，集成相关函数、管道和算法，帮你更快地创建和管理NN模块.</p><ul><li><a href="http://alex.smola.org/teaching/10-701-15/" target="_blank" rel="noopener">《Introduction to Machine Learning》</a></li></ul><p>介绍:机器学习大神Alex Smola在CMU新一期的机器学习入门课程”Introduction to Machine Learning“近期刚刚开课，课程4K高清视频同步到Youtube上，目前刚刚更新到 2.4 Exponential Families,课程视频<a href="https://www.youtube.com/playlist?list=PLZSO_6-bSqHTTV7w9u7grTXBHMH-mw3qn" target="_blank" rel="noopener">playlist</a>, 感兴趣的同学可以关注，非常适合入门.</p><ul><li><a href="http://arxiv.org/abs/1502.01423" target="_blank" rel="noopener">《Collaborative Feature Learning from Social Media》</a></li></ul><p>介绍:用社交用户行为学习图片的协同特征，可更好地表达图片内容相似性。由于不依赖于人工标签(标注)，可用于大规模图片处理，难在用户行为数据的获取和清洗；利用社会化特征的思路值得借鉴.</p><ul><li><a href="https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series" target="_blank" rel="noopener">《Introducing practical and robust anomaly detection in a time series》</a></li></ul><p>介绍:Twitter技术团队对前段时间开源的时间序列异常检测算法(S-H-ESD)R包的介绍，其中对异常的定义和分析很值得参考，文中也提到——异常是强针对性的，某个领域开发的异常检测在其他领域直接用可不行.</p><ul><li><a href="http://www.destinationcrm.com/Articles/Web-Exclusives/Viewpoints/Empower-Your-Team-to-Deal-with-Data-Quality-Issues-101308.aspx" target="_blank" rel="noopener">《Empower Your Team to Deal with Data-Quality Issues》</a></li></ul><p>介绍:聚焦数据质量问题的应对，数据质量对各种规模企业的性能和效率都至关重要，文中总结出(不限于)22种典型数据质量问题显现的信号，以及典型的数据质量解决方案(清洗、去重、统一、匹配、权限清理等)</p><ul><li><a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90" target="_blank" rel="noopener">《中文分词入门之资源》</a></li></ul><p>介绍:中文分词入门之资源.</p><ul><li><a href="https://www.youtube.com/playlist?list=PLnDbcXCpYZ8lCKExMs8k4PtIbani9ESX3" target="_blank" rel="noopener">《Deep Learning Summit, San Francisco, 2015》</a></li></ul><p>介绍:15年旧金山深度学习峰会视频集萃,<a href="http://pan.baidu.com/s/1ntiLMcT" target="_blank" rel="noopener">国内云盘</a></p><ul><li><a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/" target="_blank" rel="noopener">《Introduction to Conditional Random Fields》</a></li></ul><p>介绍:很好的条件随机场(CRF)介绍文章,作者的学习笔记</p><ul><li><a href="http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf" target="_blank" rel="noopener">《A Fast and Accurate Dependency Parser using Neural Networks》</a></li></ul><p>介绍: 来自Stanford，用神经网络实现快速准确的依存关系解析器</p><ul><li><a href="https://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/" target="_blank" rel="noopener">《Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning》</a></li></ul><p>介绍:做深度学习如何选择GPU的建议</p><ul><li><a href="http://new.livestream.com/accounts/10932136/events/3779068" target="_blank" rel="noopener">《Sparse Linear Models》</a></li></ul><p>介绍: Stanford的Trevor Hastie教授在H2O.ai Meet-Up上的报告，讲稀疏线性模型——面向“宽数据”(特征维数超过样本数)的线性模型,13年同<a href="http://pan.baidu.com/s/1jimPw" target="_blank" rel="noopener">主题报告</a> 、<a href="http://pan.baidu.com/s/1o6wqW6u" target="_blank" rel="noopener">讲义</a>.</p><ul><li><a href="https://github.com/jbhuang0604/awesome-computer-vision" target="_blank" rel="noopener">《Awesome Computer Vision》</a></li></ul><p>介绍: 分类整理的机器视觉相关资源列表，秉承Awesome系列风格，有质有量!作者的更新频率也很频繁</p><ul><li><a href="http://www.personal.ceu.hu/staff/Adam_Szeidl/" target="_blank" rel="noopener">《Adam Szeidl》</a></li></ul><p>介绍: social networks course</p><ul><li><a href="http://radar.oreilly.com/2015/01/building-and-deploying-large-scale-machine-learning-pipelines.html/" target="_blank" rel="noopener">《Building and deploying large-scale machine learning pipelines》</a></li></ul><p>介绍: 大规模机器学习流程的构建与部署.</p><ul><li><a href="http://download.csdn.net/detail/lswtzw/8469997" target="_blank" rel="noopener">《人脸识别开发包》</a></li></ul><p>介绍: 人脸识别二次开发包，免费，可商用，有演示、范例、说明书.</p><ul><li><a href="http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/" target="_blank" rel="noopener">《Understanding Natural Language with Deep Neural Networks Using Torch》</a></li></ul><p>介绍: 采用Torch用深度学习网络理解NLP，来自Facebook 人工智能的文章.</p><ul><li><a href="http://arxiv.org/pdf/1503.00168.pdf" target="_blank" rel="noopener">《The NLP Engine: A Universal Turing Machine for NLP》</a></li></ul><p>介绍: 来自CMU的Ed Hovy和Stanford的Jiwei Li一篇有意思的Arxiv文章,作者用Shannon Entropy来刻画NLP中各项任务的难度.</p><ul><li><a href="http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf" target="_blank" rel="noopener">《TThe Probabilistic Relevance Framework: BM25 and Beyond》</a></li></ul><p>介绍: 信息检索排序模型BM25(Besting Matching)。1）从经典概率模型演变而来 2）捕捉了向量空间模型中三个影响索引项权重的因子：IDF逆文档频率；TF索引项频率；文档长度归一化。3）并且含有集成学习的思想：组合了BM11和BM15两个模型。4）作者是BM25的提出者和Okapi实现者Robertson.</p><ul><li><a href="http://www.analyticsvidhya.com/blog/2015/03/introduction-auto-regression-moving-average-time-series/" target="_blank" rel="noopener">《Introduction to ARMA Time Series Models – simplified》</a></li></ul><p>介绍: 自回归滑动平均(ARMA)时间序列的简单介绍，ARMA是研究时间序列的重要方法，由自回归模型（AR模型）与滑动平均模型（MA模型）为基础“混合”构成.</p><ul><li><a href="http://arxiv.org/pdf/1503.01838v1.pdf" target="_blank" rel="noopener">《Encoding Source Language with Convolutional Neural Network for Machine Translation》</a></li></ul><p>介绍: 把来自target的attention signal加入source encoding CNN的输入，得到了比BBN的模型好的多neural network joint model</p><ul><li><a href="http://arxiv.org/abs/1502.03815" target="_blank" rel="noopener">《Spices form the basis of food pairing in Indian cuisine》</a></li></ul><p>介绍: 揭开印度菜的美味秘诀——通过对大量食谱原料关系的挖掘，发现印度菜美味的原因之一是其中的味道互相冲突，很有趣的文本挖掘研究</p><ul><li><a href="http://www.52nlp.cn/hmm%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E7%B4%A2%E5%BC%95" target="_blank" rel="noopener">《HMM相关文章索引》</a></li></ul><p>介绍: HMM相关文章,此外推荐<a href="http://yanyiwu.com/work/2014/04/07/hmm-segment-xiangjie.html" target="_blank" rel="noopener">中文分词之HMM模型详解</a></p><ul><li><a href="http://www.ccs.neu.edu/home/ekanou/ISU535.09X2/Handouts/Review_Material/zipfslaw.pdf" target="_blank" rel="noopener">《Zipf’s and Heap’s law》</a></li></ul><p>介绍: 1)词频与其降序排序的关系,最著名的是语言学家齐夫(Zipf,1902-1950)1949年提出的Zipf‘s law,即二者成反比关系. 曼德勃罗(Mandelbrot,1924- 2010)引入参数修正了对甚高频和甚低频词的刻画 2)Heaps’ law: 词汇表与语料规模的平方根(这是一个参数,英语0.4-0.6)成正比</p><ul><li><a href="http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/" target="_blank" rel="noopener">《I am Jürgen Schmidhuber, AMA》</a></li></ul><p>介绍: Jürgen Schmidhuber在Reddit上的AMA(Ask Me Anything)主题，有不少RNN和AI、ML的干货内容，关于开源&amp;思想&amp;方法&amp;建议……耐心阅读，相信你也会受益匪浅.</p><ul><li><a href="http://academictorrents.com/" target="_blank" rel="noopener">《学术种子网站：AcademicTorrents》</a></li></ul><p>介绍: 成G上T的学术数据，HN近期热议话题,主题涉及机器学习、NLP、SNA等。下载最简单的方法，通过BT软件，RSS订阅各集合即可</p><ul><li><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="noopener">《机器学习交互速查表》</a></li></ul><p>介绍: Scikit-Learn官网提供，在原有的Cheat Sheet基础上加上了Scikit-Learn相关文档的链接，方便浏览</p><ul><li><a href="https://timdettmers.wordpress.com/2015/03/09/deep-learning-hardware-guide/" target="_blank" rel="noopener">《A Full Hardware Guide to Deep Learning》</a></li></ul><p>介绍: 深度学习的全面硬件指南，从GPU到RAM、CPU、SSD、PCIe</p><ul><li><a href="http://hi.baidu.com/susongzhi/item/085983081b006311eafe38e7" target="_blank" rel="noopener">《行人检测(Pedestrian Detection)资源》</a></li></ul><p>介绍:Pedestrian Detection paper &amp; data</p><ul><li><a href="http://arxiv.org/abs/1502.01241" target="_blank" rel="noopener">《A specialized face-processing network consistent with the representational geometry of monkey face patches》</a></li></ul><p>介绍: 【神经科学碰撞人工智能】在脸部识别上你我都是专家，即使细微的差别也能辨认。研究已证明人类和灵长类动物在面部加工上不同于其他物种，人类使用梭状回面孔区（FFA）。Khaligh-Razavi等通过计算机模拟出人脸识别的FFA活动，堪称神经科学与人工智能的完美结合。</p><ul><li><a href="https://vimeo.com/19569529" target="_blank" rel="noopener">《Neural Net in C++ Tutorial》</a></li></ul><p>介绍: 神经网络C++教程,本文介绍了用可调节梯度下降和可调节动量法设计和编码经典BP神经网络，网络经过训练可以做出惊人和美妙的东西出来。此外作者博客的其他文章也很不错。</p><ul><li><a href="http://deeplearning4j.org/neuralnetworktable.html" target="_blank" rel="noopener">《How to Choose a Neural Network》</a></li></ul><p>介绍:deeplearning4j官网提供的实际应用场景NN选择参考表，列举了一些典型问题建议使用的神经网络</p><ul><li><a href="https://github.com/yusugomori/DeepLearning" target="_blank" rel="noopener">《Deep Learning (Python, C/C++, Java, Scala, Go)》</a></li></ul><p>介绍:一个深度学习项目,提供了Python, C/C++, Java, Scala, Go多个版本的代码</p><ul><li><a href="http://deeplearning.net/tutorial/" target="_blank" rel="noopener">《Deep Learning Tutorials》</a></li></ul><p>介绍:深度学习教程,<a href="https://github.com/lisa-lab/DeepLearningTutorials" target="_blank" rel="noopener">github</a></p><ul><li><a href="http://www.ccf.org.cn/resources/1190201776262/2015/03/12/15.pdf" target="_blank" rel="noopener">《自然语言处理的发展趋势——访卡内基梅隆大学爱德华·霍威教授》</a></li></ul><p>介绍:自然语言处理的发展趋势——访卡内基梅隆大学爱德华·霍威教授.</p><ul><li><a href="http://arxiv.org/abs/1503.03832" target="_blank" rel="noopener">《FaceNet: A Unified Embedding for Face Recognition and Clustering》</a></li></ul><p>介绍:Google对Facebook DeepFace的有力回击—— FaceNet，在LFW(Labeled Faces in the Wild)上达到99.63%准确率(新纪录)，FaceNet embeddings可用于人脸识别、鉴别和聚类.</p><ul><li><a href="http://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html" target="_blank" rel="noopener">《MLlib中的Random Forests和Boosting》</a></li></ul><p>介绍:本文来自Databricks公司网站的一篇博客文章，由Joseph Bradley和Manish Amde撰写，文章主要介绍了Random Forests和Gradient-Boosted Trees（GBTs）算法和他们在MLlib中的分布式实现，以及展示一些简单的例子并建议该从何处上手.<a href="http://www.csdn.net/article/2015-03-11/2824178" target="_blank" rel="noopener">中文版</a>.</p><ul><li><a href="http://spn.cs.washington.edu/index.shtml" target="_blank" rel="noopener">《Sum-Product Networks(SPN) 》</a></li></ul><p>介绍:华盛顿大学Pedro Domingos团队的DNN，提供论文和实现代码.</p><ul><li><a href="http://nlp.stanford.edu/software/nndep.shtml" target="_blank" rel="noopener">《Neural Network Dependency Parser》</a></li></ul><p>介绍:基于神经网络的自然语言依存关系解析器(已集成至Stanford CoreNLP)，特点是超快、准确，目前可处理中英文语料，基于<a href="http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf" target="_blank" rel="noopener">《A Fast and Accurate Dependency Parser Using Neural Networks》</a> 思路实现.</p><ul><li><a href="http://www.flickering.cn/nlp/2015/03/%E6%88%91%E4%BB%AC%E6%98%AF%E8%BF%99%E6%A0%B7%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E7%9A%84-3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">《神经网络语言模型》</a></li></ul><p>介绍:本文根据神经网络的发展历程，详细讲解神经网络语言模型在各个阶段的形式，其中的模型包含NNLM[Bengio,2003]、Hierarchical NNLM[Bengio, 2005], Log-Bilinear[Hinton, 2007],SENNA等重要变形，总结的特别好.</p><ul><li><a href="http://www.elg.uottawa.ca/~nat/Courses/csi5387_Winter2014/paper13.pdf" target="_blank" rel="noopener">《Classifying Spam Emails using Text and Readability Features》</a></li></ul><p>介绍:经典问题的新研究：利用文本和可读性特征分类垃圾邮件。</p><ul><li><a href="https://github.com/alexandrebarachant/bci-challenge-ner-2015" target="_blank" rel="noopener">《BCI Challenge @ NER 2015》</a></li></ul><p>介绍:<a href="https://www.kaggle.com/c/inria-bci-challenge" target="_blank" rel="noopener">Kaggle脑控计算机交互(BCI)竞赛</a>优胜方案源码及文档，包括完整的数据处理流程，是学习Python数据处理和Kaggle经典参赛框架的绝佳实例</p><ul><li><a href="http://www.ipol.im/" target="_blank" rel="noopener">《IPOL Journal · Image Processing On Line》</a></li></ul><p>介绍:IPOL（在线图像处理）是图像处理和图像分析的研究期刊，每篇文章都包含一个算法及相应的代码、Demo和实验文档。文本和源码是经过了同行评审的。IPOL是开放的科学和可重复的研究期刊。我一直想做点类似的工作，拉近产品和技术之间的距离.</p><ul><li><a href="http://eprint.iacr.org/2014/331" target="_blank" rel="noopener">《Machine learning classification over encrypted data》</a></li></ul><p>介绍:出自MIT，研究加密数据高效分类问题.</p><ul><li><a href="https://github.com/purine/purine2" target="_blank" rel="noopener">《purine2》</a></li></ul><p>介绍:新加坡LV实验室的神经网络并行框架<a href="http://arxiv.org/abs/1412.6249" target="_blank" rel="noopener">Purine: A bi-graph based deep learning framework</a>,支持构建各种并行的架构，在多机多卡，同步更新参数的情况下基本达到线性加速。12块Titan 20小时可以完成Googlenet的训练。</p><ul><li><a href="http://michal.io/machine-learning-resources/" target="_blank" rel="noopener">《Machine Learning Resources》</a></li></ul><p>介绍:这是一个机器学习资源库,虽然比较少.但蚊子再小也是肉.有突出部分.此外还有一个由<a href="http://zhengrui.github.io/zerryland/ML-CV-Resource.html" target="_blank" rel="noopener">zheng Rui整理的机器学习资源</a>.</p><ul><li><a href="https://github.com/cjdd3b/nicar2015/tree/master/machine-learning" target="_blank" rel="noopener">《Hands-on with machine learning》</a></li></ul><p>介绍:Chase Davis在NICAR15上的主题报告材料，用Scikit-Learn做监督学习的入门例子.</p><ul><li><a href="http://www.cse.unsw.edu.au/~billw/nlpdict.html" target="_blank" rel="noopener">《The Natural Language Processing Dictionary》</a></li></ul><p>介绍:这是一本自然语言处理的词典,从1998年开始到目前积累了成千上万的专业词语解释,如果你是一位刚入门的朋友.可以借这本词典让自己成长更快.</p><ul><li><a href="http://arxiv.org/abs/1503.01331" target="_blank" rel="noopener">《PageRank Approach to Ranking National Football Teams》</a></li></ul><p>介绍:通过分析1930年至今的比赛数据，用PageRank计算世界杯参赛球队排行榜.</p><ul><li><a href="http://cyclismo.org/tutorial/R/" target="_blank" rel="noopener">《R Tutorial》</a></li></ul><p>介绍:R语言教程,此外还推荐一个R语言教程<a href="http://cran.r-project.org/doc/manuals/R-intro.html" target="_blank" rel="noopener">An Introduction to R</a>.</p><ul><li><a href="http://arxiv.org/abs/0803.0476" target="_blank" rel="noopener">《Fast unfolding of communities in large networks》</a></li></ul><p>介绍:经典老文，复杂网络社区发现的高效算法，Gephi中的<a href="The Louvain method for community detection in large networks">Community detection</a>即基于此.</p><ul><li><a href="http://numl.net/" target="_blank" rel="noopener">《NUML》</a></li></ul><p>介绍: 一个面向 .net 的开源机器学习库,<a href="https://github.com/sethjuarez/numl" target="_blank" rel="noopener">github地址</a></p><ul><li><a href="http://synaptic.juancazala.com/" target="_blank" rel="noopener">《synaptic.Js》</a></li></ul><p>介绍: 支持node.js的JS神经网络库，可在客户端浏览器中运行，支持LSTM等 <a href="https://github.com/cazala/synaptic" target="_blank" rel="noopener">github地址</a></p><ul><li><a href="http://tjo-en.hatenablog.com/entry/2015/03/20/191614" target="_blank" rel="noopener">《Machine learning for package users with R (1): Decision Tree》</a></li></ul><p>介绍: 决策树</p><ul><li><a href="http://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html" target="_blank" rel="noopener">《Deep Learning, The Curse of Dimensionality, and Autoencoders》</a></li></ul><p>介绍:  讨论深度学习自动编码器如何有效应对维数灾难,<a href="http://www.36dsj.com/archives/26223" target="_blank" rel="noopener">国内翻译</a></p><ul><li><a href="http://www.cs.cmu.edu/~suvrit/teach/" target="_blank" rel="noopener">《Advanced Optimization and Randomized Methods》</a></li></ul><p>介绍: CMU的优化与随机方法课程，由A. Smola和S. Sra主讲，优化理论是机器学习的基石，值得深入学习 <a href="http://pan.baidu.com/s/1c0cZtQC" target="_blank" rel="noopener">国内云(视频)</a></p><ul><li><a href="http://cs231n.stanford.edu/reports.html" target="_blank" rel="noopener">《CS231n: Convolutional Neural Networks for Visual Recognition》</a></li></ul><p>介绍: “面向视觉识别的CNN”课程设计报告集锦.近百篇，内容涉及图像识别应用的各个方面</p><ul><li><a href="http://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html" target="_blank" rel="noopener">《Topic modeling with LDA: MLlib meets GraphX》</a></li></ul><p>介绍:用Spark的MLlib+GraphX做大规模LDA主题抽取.</p><ul><li><a href="http://arxiv.org/abs/1502.05988" target="_blank" rel="noopener">《Deep Learning for Multi-label Classification》</a></li></ul><p>介绍: 基于深度学习的多标签分类,用基于RBM的DBN解决多标签分类(特征)问题</p><ul><li><a href="http://deepmind.com/publications.html" target="_blank" rel="noopener">《Google DeepMind publications》</a></li></ul><p>介绍: DeepMind论文集锦</p><ul><li><a href="http://kaldi-asr.org/" target="_blank" rel="noopener">《kaldi》</a></li></ul><p>介绍: 一个开源语音识别工具包,它目前托管在<a href="http://sourceforge.net/projects/kaldi/" target="_blank" rel="noopener">sourceforge</a>上面</p><ul><li><a href="http://datajournalismhandbook.org/" target="_blank" rel="noopener">《Data Journalism Handbook》</a></li></ul><p>介绍: 免费电子书《数据新闻手册》, 国内有热心的朋友翻译了<a href="http://datajournalismhandbook.org/chinese/index.html" target="_blank" rel="noopener">中文版</a>,大家也可以<a href="http://datajournalismhandbook.org/1.0/en/" target="_blank" rel="noopener">在线阅读</a></p><ul><li><a href="https://highlyscalable.wordpress.com/2015/03/10/data-mining-problems-in-retail/" target="_blank" rel="noopener">《Data Mining Problems in Retail》</a></li></ul><p>介绍: 零售领域的数据挖掘文章.</p><ul><li><a href="https://timdettmers.wordpress.com/2015/03/26/convolution-deep-learning/" target="_blank" rel="noopener">《Understanding Convolution in Deep Learning》</a></li></ul><p>介绍: 深度学习卷积概念详解,深入浅出.</p><ul><li><a href="http://pandas.pydata.org/" target="_blank" rel="noopener">《pandas: powerful Python data analysis toolkit》</a></li></ul><p>介绍: 非常强大的Python的数据分析工具包.</p><ul><li><a href="http://breakthroughanalysis.com/2015/03/23/text-analytics-2015/" target="_blank" rel="noopener">《Text Analytics 2015》</a></li></ul><p>介绍: 2015文本分析(商业)应用综述.</p><ul><li><a href="http://www.slideshare.net/VincenzoLomonaco/deep-learning-libraries-and-rst-experiments-with-theano" target="_blank" rel="noopener">《Deep Learning libraries and ﬁrst experiments with Theano》</a></li></ul><p>介绍: 深度学习框架、库调研及Theano的初步测试体会报告.</p><ul><li><a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">《DEEP learning》</a></li></ul><p>介绍:  MIT的Yoshua Bengio, Ian Goodfellow, Aaron Courville著等人讲深度学习的新书，还未定稿，线上提供Draft chapters收集反馈，超赞！强烈推荐.</p><ul><li><a href="https://github.com/hickeroar/simplebayes" target="_blank" rel="noopener">《simplebayes》</a></li></ul><p>介绍: Python下开源可持久化朴素贝叶斯分类库.</p><ul><li><a href="http://paracel.io/" target="_blank" rel="noopener">《Paracel》</a></li></ul><p>介绍:Paracel is a distributed computational framework designed for machine learning problems, graph algorithms and scientific computing in C++.</p><ul><li><a href="http://hanlp.linrunsoft.com/" target="_blank" rel="noopener">《HanLP:Han Language processing》</a></li></ul><p>介绍: 开源汉语言处理包.</p><ul><li><a href="http://www.rubylab.io/2015/03/18/simple-neural-network-implenentation-in-ruby/" target="_blank" rel="noopener">《Simple Neural Network implementation in Ruby》</a></li></ul><p>介绍: 使用Ruby实现简单的神经网络例子.</p><ul><li><a href="https://karpathy.github.io/neuralnets/" target="_blank" rel="noopener">《Hacker’s guide to Neural Networks》</a></li></ul><p>介绍:神经网络黑客入门.</p><ul><li><a href="http://datasciencemasters.org/" target="_blank" rel="noopener">《The Open-Source Data Science Masters》</a></li></ul><p>介绍:好多数据科学家名人推荐,还有资料.</p><ul><li><a href="http://arxiv.org/abs/1502.01710" target="_blank" rel="noopener">《Text Understanding from Scratch》</a></li></ul><p>介绍:实现项目已经开源在github上面<a href="https://github.com/zhangxiangxiao/Crepe" target="_blank" rel="noopener">Crepe</a></p><ul><li><a href="https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf" target="_blank" rel="noopener">《 Improving Distributional Similarity with Lessons Learned from Word Embeddings》</a></li></ul><p>介绍:作者发现，经过调参，传统的方法也能和word2vec取得差不多的效果。另外，无论作者怎么试，GloVe都比不过word2vec.</p><ul><li><a href="http://cs224d.stanford.edu/index.html" target="_blank" rel="noopener">《CS224d: Deep Learning for Natural Language Processing》</a></li></ul><p>介绍:Stanford深度学习与自然语言处理课程,Richard Socher主讲.</p><ul><li><a href="http://courses.washington.edu/css490/2012.Winter/lecture_slides/02_math_essentials.pdf" target="_blank" rel="noopener">《Math Essentials in Machine Learning》</a></li></ul><p>介绍:机器学习中的重要数学概念.</p><ul><li><a href="http://arxiv.org/abs/1503.00007" target="_blank" rel="noopener">《Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks》</a></li></ul><p>介绍:用于改进语义表示的树型LSTM递归神经网络,句子级相关性判断和情感分类效果很好.<a href="https://github.com/stanfordnlp/treelstm" target="_blank" rel="noopener">实现代码</a>.</p><ul><li><a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="noopener">《Statistical Machine Learning》</a></li></ul><p>介绍:卡耐基梅隆Ryan Tibshirani和Larry Wasserman开设的机器学习课程，先修课程为机器学习(10-715)和中级统计学(36-705)，聚焦统计理论和方法在机器学习领域应用.</p><ul><li><a href="http://am207.org/" target="_blank" rel="noopener">《AM207: Monte Carlo Methods, Stochastic Optimization》</a></li></ul><p>介绍:《哈佛大学蒙特卡洛方法与随机优化课程》是哈佛应用数学研究生课程，由V Kaynig-Fittkau、P Protopapas主讲，Python程序示例，对贝叶斯推理感兴趣的朋友一定要看看，提供授<a href="http://nbviewer.ipython.org/github/AM207/2015/tree/master/Lectures/" target="_blank" rel="noopener">课视频及课上IPN讲义</a>.</p><ul><li><a href="http://spark-summit.org/wp-content/uploads/2015/03/SSE15-40-Danford.pdf" target="_blank" rel="noopener">《生物医学的SPARK大数据应用》</a></li></ul><p>介绍:生物医学的SPARK大数据应用.并且伯克利开源了他们的big data genomics系统<a href="https://github.com/bigdatagenomics/adam" target="_blank" rel="noopener">ADAM</a>，其他的内容可以关注一下<a href="http://spark-summit.org/" target="_blank" rel="noopener">官方主页</a>.</p><ul><li><a href="http://aclanthology.info/" target="_blank" rel="noopener">《ACL Anthology》</a></li></ul><p>介绍:对自然语言处理技术或者机器翻译技术感兴趣的亲们，请在提出自己牛逼到无以伦比的idea（自动归纳翻译规律、自动理解语境、自动识别语义等等）之前，请通过谷歌学术简单搜一下，如果谷歌不可用，这个网址有这个领域几大顶会的论文列表,切不可断章取义,胡乱假设.</p><ul><li><a href="http://www.uni-weimar.de/medien/webis/publications/papers/stein_2015b.pdf" target="_blank" rel="noopener">《Twitter Sentiment Detection via Ensemble Classification Using Averaged Confidence Scores》</a></li></ul><p>介绍:论文+代码:基于集成方法的Twitter情感分类,<a href="https://github.com/webis-de/ECIR-2015-and-SEMEVAL-2015" target="_blank" rel="noopener">实现代码</a>.</p><ul><li><a href="http://ciml.chalearn.org/schedule" target="_blank" rel="noopener">《NIPS 2014 CIML workshop》</a></li></ul><p>介绍:NIPS CiML 2014的PPT,NIPS是神经信息处理系统进展大会的英文简称.</p><ul><li><a href="http://cs231n.stanford.edu/reports.html" target="_blank" rel="noopener">《CS231n: Convolutional Neural Networks for Visual Recognition》</a></li></ul><p>介绍:斯坦福的深度学习课程的Projects 每个人都要写一个论文级别的报告 里面有一些很有意思的应用 大家可以看看 .</p><ul><li><a href="http://www.sumsar.net/blog/2015/03/a-speed-comparison-between-flexible-linear-regression-alternatives-in-r/" target="_blank" rel="noopener">《A Speed Comparison Between Flexible Linear Regression Alternatives in R》</a></li></ul><p>介绍:R语言线性回归多方案速度比较具体方案包括lm()、nls()、glm()、bayesglm()、nls()、mle2()、optim()和Stan’s optimizing()等.</p><ul><li><a href="http://www.allthingsdistributed.com/2015/04/machine-learning.html" target="_blank" rel="noopener">《Back-to-Basics Weekend Reading - Machine Learning》</a></li></ul><p>介绍:文中提到的三篇论文（机器学习那些事、无监督聚类综述、监督分类综述）都很经典，Domnigos的机器学习课也很精彩</p><ul><li><a href="http://arxiv.org/abs/1504.00641" target="_blank" rel="noopener">《A Probabilistic Theory of Deep Learning》</a></li></ul><p>介绍:莱斯大学（Rice University）的深度学习的概率理论.</p><ul><li><a href="http://www.gregreda.com/2015/03/30/beer-review-markov-chains/" target="_blank" rel="noopener">《Nonsensical beer reviews via Markov chains》</a></li></ul><p>介绍:基于马尔可夫链自动生成啤酒评论的开源Twitter机器人,<a href="https://github.com/gjreda/beer-snob-says" target="_blank" rel="noopener">github地址</a>.</p><ul><li><a href="http://nlp.stanford.edu/courses/NAACL2013/" target="_blank" rel="noopener">《Deep Learning for Natural Language Processing (without Magic)》</a></li></ul><p>介绍:视频+讲义:深度学习用于自然语言处理教程(NAACL13).</p><ul><li><a href="https://www.youtube.com/watch?v=U4IYsLgNgoY&amp;hd=1" target="_blank" rel="noopener">《Introduction to Data Analysis using Machine Learning》</a></li></ul><p>介绍:用机器学习做数据分析,David Taylor最近在McGill University研讨会上的报告，还提供了一系列讲机器学习方法的ipn，很有价值 <a href="https://github.com/Prooffreader/intro_machine_learning" target="_blank" rel="noopener">GitHub</a>.<a href="http://pan.baidu.com/s/1mgtE9te" target="_blank" rel="noopener">国内</a></p><ul><li><a href="http://arxiv.org/abs/1503.08909" target="_blank" rel="noopener">《Beyond Short Snippets: Deep Networks for Video Classification》</a></li></ul><p>介绍:基于CNN+LSTM的视频分类,<a href="http://pan.baidu.com/s/1c0cZS9E" target="_blank" rel="noopener">google演示</a>.</p><ul><li><a href="http://www.quora.com/How-does-Quora-use-machine-learning-in-2015/answer/Xavier-Amatriain" target="_blank" rel="noopener">《How does Quora use machine learning in 2015?》</a></li></ul><p>介绍:Quora怎么用机器学习.</p><ul><li><a href="https://aws.amazon.com/cn/blogs/aws/amazon-machine-learning-make-data-driven-decisions-at-scale/" target="_blank" rel="noopener">《Amazon Machine Learning – Make Data-Driven Decisions at Scale》</a></li></ul><p>介绍:亚马逊在机器学习上面的一些应用,<a href="https://github.com/awslabs/machine-learning-samples" target="_blank" rel="noopener">代码示例</a>.</p><ul><li><a href="https://github.com/ogrisel/parallel_ml_tutorial" target="_blank" rel="noopener">《Parallel Machine Learning with scikit-learn and IPython》</a></li></ul><p>介绍:并行机器学习指南(基于scikit-learn和IPython).<a href="http://nbviewer.ipython.org/github/ogrisel/parallel_ml_tutorial/tree/master/notebooks/" target="_blank" rel="noopener">notebook</a></p><ul><li><a href="http://blog.kaggle.com/2015/04/08/new-video-series-introduction-to-machine-learning-with-scikit-learn/" target="_blank" rel="noopener">《Intro to machine learning with scikit-learn》</a></li></ul><p>介绍:DataSchool的机器学习基本概念教学.</p><ul><li><a href="https://github.com/hughperkins/DeepCL" target="_blank" rel="noopener">《DeepCLn》</a></li></ul><p>介绍:一个基于OpenGL实现的卷积神经网络，支持Linux及Windows系.</p><ul><li><a href="https://www.mapr.com/blog/inside-look-at-components-of-recommendation-engine" target="_blank" rel="noopener">《An Inside Look at the Components of a Recommendation Engine》</a></li></ul><p>介绍:基于Mahout和Elasticsearch的推荐系统.</p><ul><li><a href="http://www.ssc.upenn.edu/~fdiebold/Teaching221/econ221.html" target="_blank" rel="noopener">《Forecasting in Economics, Business, Finance and Beyond》</a></li></ul><p>介绍:Francis X. Diebold的《(经济|商业|金融等领域)预测方法.</p><ul><li><a href="http://www.ssc.upenn.edu/~fdiebold/Teaching706/econ706Penn.html" target="_blank" rel="noopener">《Time Series Econometrics - A Concise Course》</a></li></ul><p>介绍:Francis X. Diebold的《时序计量经济学》.</p><ul><li><a href="http://fotiad.is/blog/sentiment-analysis-comparison/" target="_blank" rel="noopener">《A comparison of open source tools for sentiment analysis》</a></li></ul><p>介绍:基于Yelp数据集的开源<a href="https://github.com/sfotiadis/yenlp" target="_blank" rel="noopener">情感分析工具</a>比较,评测覆盖Naive Bayes、SentiWordNet、CoreNLP等 .</p><ul><li><a href="http://vdisk.weibo.com/s/ayG13we2u_sAZ" target="_blank" rel="noopener">《Pattern Recognition And Machine Learning》</a></li></ul><p>介绍:国内Pattern Recognition And Machine Learning读书会资源汇总,<a href="http://vdisk.weibo.com/u/1841149974" target="_blank" rel="noopener">各章pdf讲稿</a>,<a href="http://www.cnblogs.com/Nietzsche/" target="_blank" rel="noopener">博客</a>.</p><ul><li><a href="https://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/" target="_blank" rel="noopener">《Probabilistic Data Structures for Web Analytics and Data Mining 》</a></li></ul><p>介绍:用于Web分析和数据挖掘的概率数据结构.</p><ul><li><a href="https://blindmotion.github.io/2015/04/11/ml-in-navigation/" target="_blank" rel="noopener">《Machine learning in navigation devices: detect maneuvers using accelerometer and gyroscope》</a></li></ul><p>介绍:机器学习在导航上面的应用.</p><ul><li><a href="https://www.youtube.com/user/Taylorns34/videos" target="_blank" rel="noopener">《Neural Networks Demystified 》</a></li></ul><p>介绍:Neural Networks Demystified系列视频，Stephen Welch制作，纯手绘风格，浅显易懂,<a href="http://pan.baidu.com/s/1i3AFURj" target="_blank" rel="noopener">国内云</a>.</p><ul><li><a href="https://www.datacamp.com/swirl-r-tutorial" target="_blank" rel="noopener">《swirl + DataCamp 》</a></li></ul><p>介绍:{swirl}数据训练营:R&amp;数据科学在线交互教程.</p><ul><li><a href="http://blog.terminal.com/recurrent-neural-networks-deep-net-optimization-lstm/" target="_blank" rel="noopener">《Learning to Read with Recurrent Neural Networks 》</a></li></ul><p>介绍:关于深度学习和RNN的讨论 <a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>.</p><ul><li><a href="http://wanghaitao8118.blog.163.com/blog/static/13986977220153811210319/" target="_blank" rel="noopener">《深度强化学习（Deep Reinforcement Learning）的资源》</a></li></ul><p>介绍:Deep Reinforcement Learning.</p><ul><li><a href="https://github.com/jakevdp/sklearn_pycon2015" target="_blank" rel="noopener">《Machine Learning with Scikit-Learn》</a></li></ul><p>介绍:(PyCon2015)Scikit-Learn机器学习教程,<a href="https://github.com/ogrisel/parallel_ml_tutorial" target="_blank" rel="noopener">Parallel Machine Learning with scikit-learn and IPython</a>.</p><ul><li><a href="http://www.cs.cmu.edu/~ymiao/pdnntk.html" target="_blank" rel="noopener">《PDNN》</a></li></ul><p>介绍:PDNN: A Python Toolkit for Deep Learning.</p><ul><li><a href="http://alex.smola.org/teaching/10-701-15/index.html" target="_blank" rel="noopener">《Introduction to Machine Learning》</a></li></ul><p>介绍:15年春季学期CMU的机器学习课程，由Alex Smola主讲，提供讲义及授课视频，很不错.<a href="http://pan.baidu.com/s/1pJxBePX" target="_blank" rel="noopener">国内镜像</a>.</p><ul><li><a href="http://www.st.ewi.tudelft.nl/~hauff/TI2736-B.html" target="_blank" rel="noopener">《Big Data Processing》</a></li></ul><p>介绍:大数据处理课.内容覆盖流处理、MapReduce、图算法等.</p><ul><li><a href="https://www.hakkalabs.co/articles/spark-mllib-making-practical-machine-learning-easy-and-scalable" target="_blank" rel="noopener">《Spark MLlib: Making Practical Machine Learning Easy and Scalable》</a></li></ul><p>介绍:用Spark MLlib实现易用可扩展的机器学习,<a href="http://pan.baidu.com/s/1gdxSOZh" target="_blank" rel="noopener">国内镜像</a>.</p><ul><li><a href="http://mrkulk.github.io/www_cvpr15/" target="_blank" rel="noopener">《Picture: A Probabilistic Programming Language for Scene Perception》</a></li></ul><p>介绍:以往上千行代码概率编程(语言)实现只需50行.</p><ul><li><a href="http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/" target="_blank" rel="noopener">《Beautiful plotting in R: A ggplot2 cheatsheet》</a></li></ul><p>介绍:ggplot2速查小册子,<a href="http://www.ling.upenn.edu/~joseff/avml2012/" target="_blank" rel="noopener">另外一个</a>,此外还推荐<a href="http://zevross.com/blog/2015/01/13/a-new-data-processing-workflow-for-r-dplyr-magrittr-tidyr-ggplot2/" target="_blank" rel="noopener">《A new data processing workflow for R: dplyr, magrittr, tidyr, ggplot2》</a>.</p><ul><li><a href="http://emnlp2014.org/papers/pdf/EMNLP2014148.pdf" target="_blank" rel="noopener">《Using Structured Events to Predict Stock Price Movement: An Empirical Investigation》</a></li></ul><p>介绍:用结构化模型来预测实时股票行情.</p><ul><li><a href="http://ijcai-15.org/index.php/accepted-papers" target="_blank" rel="noopener">《International Joint Conference on Artificial Intelligence Accepted paper》</a></li></ul><p>介绍:<a href="http://ijcai.org/" target="_blank" rel="noopener">国际人工智能联合会议</a>录取论文列表,大部分论文可使用Google找到.</p><ul><li><a href="http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/" target="_blank" rel="noopener">《Why GEMM is at the heart of deep learning》</a></li></ul><p>介绍:一般矩阵乘法(GEMM)对深度学习的重要性.</p><ul><li><a href="https://github.com/dmlc" target="_blank" rel="noopener">《Distributed (Deep) Machine Learning Common》</a></li></ul><p>介绍:A Community of awesome Distributed Machine Learning C++ projects.</p><ul><li><a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" target="_blank" rel="noopener">《Reinforcement Learning: An Introduction》</a></li></ul><p>介绍:免费电子书&lt;强化学习介绍&gt;,<a href="http://pan.baidu.com/s/1jkaMq" target="_blank" rel="noopener">第一版(1998)</a>,<a href="http://pan.baidu.com/s/1dDnNEnR" target="_blank" rel="noopener">第二版(2015草稿)</a>,相关课程<a href="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html" target="_blank" rel="noopener">资料</a>,<a href="http://www.inf.ed.ac.uk/teaching/courses/rl/" target="_blank" rel="noopener">Reinforcement Learning</a>.</p><ul><li><a href="http://blogs.msdn.com/b/microsoft_press/archive/2015/04/15/free-ebook-microsoft-azure-essentials-azure-machine-learning.aspx" target="_blank" rel="noopener">《Free ebook: Microsoft Azure Essentials: Azure Machine Learning》</a></li></ul><p>介绍:免费书:Azure ML使用精要.</p><ul><li><a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks" target="_blank" rel="noopener">《A Deep Learning Tutorial: From Perceptrons to Deep Networks》</a></li></ul><p>介绍:A Deep Learning Tutorial: From Perceptrons to Deep Networks.</p><ul><li><a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471" target="_blank" rel="noopener">《Machine Learning is Fun! - The world’s easiest introduction to Machine Learning》</a></li></ul><p>介绍:有趣的机器学习：最简明入门指南,<a href="http://blog.jobbole.com/67616/" target="_blank" rel="noopener">中文版</a>.</p><ul><li><a href="yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">《A Brief Overview of Deep Learning》</a></li></ul><p>介绍:深度学习简明介绍,<a href="http://xhrwang.me/2015/01/16/a-brief-overview-of-deep-learning.html" target="_blank" rel="noopener">中文版</a>.</p><ul><li><a href="https://github.com/dmlc/wormhole" target="_blank" rel="noopener">《Wormhole》</a></li></ul><p>介绍:Portable, scalable and reliable distributed machine learning.</p><ul><li><a href="https://github.com/soumith/convnet-benchmarks" target="_blank" rel="noopener">《convnet-benchmarks》</a></li></ul><p>介绍:CNN开源实现横向评测,参评框架包括Caffe 、Torch-7、CuDNN 、cudaconvnet2 、fbfft、Nervana Systems等，NervanaSys表现突出.</p><ul><li><a href="http://islpc21.is.cs.cmu.edu:3000/lti_catalogue" target="_blank" rel="noopener">《This catalogue lists resources developed by faculty and students of the Language Technologies Institute.》</a></li></ul><p>介绍:卡耐基梅隆大学计算机学院语言技术系的资源大全,包括大量的NLP开源软件工具包，基础数据集，论文集，数据挖掘教程，机器学习资源.</p><ul><li><a href="https://github.com/mayank93/Twitter-Sentiment-Analysis" target="_blank" rel="noopener">《Sentiment Analysis on Twitter》</a></li></ul><p>介绍:Twitter情感分析工具SentiTweet,<a href="http://pan.baidu.com/s/1i3kXPlj" target="_blank" rel="noopener">视频+讲义</a>.</p><ul><li><a href="http://machinelearning.wustl.edu/mlpapers/venues" target="_blank" rel="noopener">《Machine Learning Repository @ Wash U》</a></li></ul><p>介绍:华盛顿大学的Machine Learning Paper Repository.</p><ul><li><a href="https://github.com/soulmachine/machine-learning-cheat-sheet" target="_blank" rel="noopener">《Machine learning cheat sheet》</a></li></ul><p>介绍:机器学习速查表.</p><ul><li><a href="http://spark-summit.org/east" target="_blank" rel="noopener">《Spark summit east 2015 agenda》</a></li></ul><p>介绍:最新的Spark summit会议资料.</p><ul><li><a href="http://spark-summit.org/east" target="_blank" rel="noopener">《Spark summit east 2015 agenda》</a></li></ul><p>介绍:最新的Spark summit会议资料.</p><ul><li><a href="http://pan.baidu.com/s/1eQkybJG" target="_blank" rel="noopener">《Learning Spark》</a></li></ul><p>介绍:Ebook Learning Spark.</p><ul><li><a href="http://pan.baidu.com/s/1jGot9qe" target="_blank" rel="noopener">《Advanced Analytics with Spark, Early Release Edition》</a></li></ul><p>介绍:Ebook Advanced Analytics with Spark, Early Release Edition.</p><ul><li><a href="http://keg.cs.tsinghua.edu.cn/jietang/" target="_blank" rel="noopener">《国内机器学习算法及应用领域人物篇:唐杰》</a></li></ul><p>介绍:清华大学副教授，是图挖掘方面的专家。他主持设计和实现的Arnetminer是国内领先的图挖掘系统，该系统也是多个会议的支持商.</p><ul><li><a href="http://www.cse.ust.hk/~qyang/" target="_blank" rel="noopener">《国内机器学习算法及应用领域人物篇:杨强》</a></li></ul><p>介绍:迁移学习的国际领军人物.</p><ul><li><a href="http://cs.nju.edu.cn/zhouzh/" target="_blank" rel="noopener">《国内机器学习算法及应用领域人物篇:周志华》</a></li></ul><p>介绍:在半监督学习，multi-label学习和集成学习方面在国际上有一定的影响力.</p><ul><li><a href="http://ir.hit.edu.cn/~wanghaifeng/whf_pub.htm" target="_blank" rel="noopener">《国内机器学习算法及应用领域人物篇:王海峰》</a></li></ul><p>介绍:信息检索，自然语言处理，机器翻译方面的专家.</p><ul><li><a href="http://www.cs.jhu.edu/~junwu/" target="_blank" rel="noopener">《国内机器学习算法及应用领域人物篇:吴军》</a></li></ul><p>介绍:吴军博士是当前Google中日韩文搜索算法的主要设计者。在Google其间，他领导了许多研发项目，包括许多与中文相关的产品和自然语言处理的项目,他的<a href="https://sites.google.com/site/junwu02" target="_blank" rel="noopener">新个人主页</a>.</p><ul><li><a href="http://www.eecs.berkeley.edu/~junyanz/cat/cat_papers.html" target="_blank" rel="noopener">《Cat Paper Collection》</a></li></ul><p>介绍:喵星人相关论文集.</p><ul><li><a href="http://blog.dato.com/how-to-evaluate-machine-learning-models-part-1-orientation" target="_blank" rel="noopener">《How to Evaluate Machine Learning Models, Part 1: Orientation》</a></li></ul><p>介绍:如何评价机器学习模型系列文章,<a href="http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics" target="_blank" rel="noopener">How to Evaluate Machine Learning Models, Part 2a: Classification Metrics</a>,<a href="http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2b-ranking-and-regression-metrics" target="_blank" rel="noopener">How to Evaluate Machine Learning Models, Part 2b: Ranking and Regression Metrics</a>.</p><ul><li><a href="https://blog.twitter.com/2015/building-a-new-trends-experience" target="_blank" rel="noopener">《Building a new trends experience》</a></li></ul><p>介绍:Twitter新trends的基本实现框架.</p><ul><li><a href="https://www.packtpub.com/big-data-and-business-intelligence/storm-blueprints-patterns-distributed-real-time-computation" target="_blank" rel="noopener">《Storm Blueprints: Patterns for Distributed Real-time Computation》</a></li></ul><p>介绍:Storm手册，国内有<a href="https://github.com/cjie888/storm-trident" target="_blank" rel="noopener">中文翻译版本</a>,谢谢作者.</p><ul><li><a href="https://github.com/haifengl/smile" target="_blank" rel="noopener">《SmileMiner》</a></li></ul><p>介绍:Java机器学习算法库SmileMiner.</p><ul><li><a href="http://nlp.csai.tsinghua.edu.cn/~ly/talks/cwmt14_tut.pdf" target="_blank" rel="noopener">《机器翻译学术论文写作方法和技巧》</a></li></ul><p>介绍:机器翻译学术论文写作方法和技巧，Simon Peyton Jones的<a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm" target="_blank" rel="noopener">How to write a good research paper</a>同类视频<a href="https://www.youtube.com/watch?v=g3dkRsTqdDA" target="_blank" rel="noopener">How to Write a Great Research Paper</a>,<a href="http://vdisk.weibo.com/s/ayG13we2volht" target="_blank" rel="noopener">how to paper talk</a>.</p><ul><li><a href="http://blog.csdn.net/zouxy09/article/details/45288129" target="_blank" rel="noopener">《神经网络训练中的Tricks之高效BP（反向传播算法）》</a></li></ul><p>介绍:神经网络训练中的Tricks之高效BP,博主的其他博客也挺精彩的.</p><ul><li><a href="http://www.52cs.org/?p=499" target="_blank" rel="noopener">《我和NLP的故事》</a></li></ul><p>介绍:作者是NLP方向的硕士，短短几年内研究成果颇丰,推荐新入门的朋友阅读.</p><ul><li><a href="http://www.cs.ucla.edu/~palsberg/h-number.html" target="_blank" rel="noopener">《The h Index for Computer Science 》</a></li></ul><p>介绍:UCLA的Jens Palsberg根据Google Scholar建立了一个计算机领域的H-index牛人列表,我们熟悉的各个领域的大牛绝大多数都在榜上，包括1位诺贝尔奖得主，35位图灵奖得主，近百位美国工程院/科学院院士，300多位ACM Fellow,在这里推荐的原因是大家可以在google通过搜索牛人的名字来获取更多的资源,这份资料很宝贵.</p><ul><li><a href="http://ttic.uchicago.edu/~mbansal/papers/acl14_structuredTaxonomy.pdf" target="_blank" rel="noopener">《Structured Learning for Taxonomy Induction with Belief Propagation》</a></li></ul><p>介绍:用大型语料库学习概念的层次关系，如鸟是鹦鹉的上级，鹦鹉是虎皮鹦鹉的上级。创新性在于模型构造，用因子图刻画概念之间依存关系，因引入兄弟关系，图有环，所以用有环扩散（loopy propagation）迭代计算边际概率（marginal probability）.</p><ul><li><a href="http://www.stata.com/stata14/bayesian-analysis/" target="_blank" rel="noopener">《Bayesian analysis》</a></li></ul><p>介绍: 这是一款贝叶斯分析的商业软件,官方写的<a href="http://www.stata.com/manuals14/bayes.pdf" target="_blank" rel="noopener">贝叶斯分析的手册</a>有250多页,虽然R语言 已经有类似的<a href="http://cran.r-project.org/web/views/Bayesian.html" target="_blank" rel="noopener">项目</a>,但毕竟可以增加一个可选项.</p><ul><li><a href="http://www.quora.com/Boris-Babenko/Posts/deep-net-highlights-from-2014" target="_blank" rel="noopener">《deep net highlights from 2014》</a></li></ul><p>介绍:deep net highlights from 2014.</p><ul><li><a href="http://arxiv.org/pdf/1504.08083v1.pdf" target="_blank" rel="noopener">《Fast R-CNN》</a></li></ul><p>介绍:This paper proposes Fast R-CNN, a clean and fast framework for object detection.</p><ul><li><a href="https://realpython.com/blog/python/fingerprinting-images-for-near-duplicate-detection/" target="_blank" rel="noopener">《Fingerprinting Images for Near-Duplicate Detection》</a></li></ul><p>介绍:图像指纹的重复识别,作者<a href="https://github.com/realpython/image-fingerprinting/blob/master/code/output.csv" target="_blank" rel="noopener">源码</a>,国内<a href="http://www.cnblogs.com/wing1995/p/4471034.html" target="_blank" rel="noopener">翻译版本</a>.</p><ul><li><a href="http://www.cs.ubc.ca/~lowe/vision.html" target="_blank" rel="noopener">《The Computer Vision Industry 》</a></li></ul><p>介绍:提供计算机视觉、机器视觉应用的公司信息汇总.应用领域包括：自动辅助驾驶和交通管理、眼球和头部跟踪、影视运动分析、影视业、手势识别、通用视觉系统、各种工业自动化和检验、医药和生物、移动设备目标识别和AR、人群跟踪、摄像、安全监控、生物监控、三维建模、web和云应用.</p><ul><li><a href="https://github.com/mwaskom/seaborn" target="_blank" rel="noopener">《Seaborn: statistical data visualization》</a></li></ul><p>介绍:Python版可视化数据统计开源库.</p><ul><li><a href="http://www.juanklopper.com/opencourseware/mathematics-2/ipython-lecture-notes/" target="_blank" rel="noopener">《IPython lecture notes for OCW MIT 18.06》</a></li></ul><p>介绍:麻省理工Gilbert Strang线性代数课程笔记,Gilbert Strang《Linear Algebra》课程主页<a href="http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm" target="_blank" rel="noopener">视频+讲义</a>.</p><ul><li><a href="http://deeplearning4j.org/canova.html" target="_blank" rel="noopener">《Canova: A Vectorization Lib for ML》</a></li></ul><p>介绍:面向机器学习/深度学习的数据向量化工具Canova,<a href="https://github.com/deeplearning4j/Canova" target="_blank" rel="noopener">github</a>, 支持CSV文件、MNIST数据、TF-IDF/Bag of Words/word2vec文本向量化.</p><ul><li><a href="http://java.dzone.com/articles/dzone-refcardz-distributed" target="_blank" rel="noopener">《DZone Refcardz: Distributed Machine Learning with Apache Mahout》</a></li></ul><p>介绍:快速入门：基于Apache Mahout的分布式机器学习.</p><ul><li><a href="http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/tree/master/" target="_blank" rel="noopener">《Learning scikit-learn: Machine Learning in Python》</a></li></ul><p>介绍:基于scikit-learn讲解了一些机器学习技术，如SVM，NB，PCA，DT，以及特征工程、特征选择和模型选择问题.</p><ul><li><a href="https://speakerdeck.com/nivdul/lightning-fast-machine-learning-with-spark" target="_blank" rel="noopener">《Lightning fast Machine Learning with Spark》</a></li></ul><p>介绍:基于Spark的高效机器学习,<a href="https://www.parleys.com/tutorial/lightning-fast-machine-learning-spark" target="_blank" rel="noopener">视频地址</a>.</p><ul><li><a href="http://blog.wepay.com/how-were-using-machine-learning-to-fight-shell-selling/" target="_blank" rel="noopener">《How we’re using machine learning to fight shell selling》</a></li></ul><p>介绍:WePay用机器学习对抗信用卡”shell selling”诈骗.</p><ul><li><a href="http://www.datasciencecentral.com/profiles/blog/show?id=6448529:BlogPost:273276" target="_blank" rel="noopener">《Data Scientists Thoughts that Inspired Me》</a></li></ul><p>介绍:16位数据科学家语录精选.</p><ul><li><a href="http://www.journalofbigdata.com/content/2/1/1" target="_blank" rel="noopener">《Deep learning applications and challenges in big data analytics》</a></li></ul><p>介绍:深度学习在大数据分析领域的应用和挑战.</p><ul><li><a href="http://resrc.io/list/10/list-of-free-programming-books/#machine-learning" target="_blank" rel="noopener">《Free book:Machine Learning,Mathematics》</a></li></ul><p>介绍:免费的机器学习与数学书籍,除此之外还有其他的<a href="https://github.com/vhf/resrc" target="_blank" rel="noopener">免费编程书籍</a>,编程语言,设计,操作系统等.</p><ul><li><a href="http://arxiv.org/pdf/1505.01749.pdf" target="_blank" rel="noopener">《Object detection via a multi-region &amp; semantic segmentation-aware CNN model》</a></li></ul><p>介绍:一篇关于CNN模型对象识别Paper.</p><ul><li><a href="http://blog.shakirm.com/2015/05/a-statistical-view-of-deep-learning-v-generalisation-and-regularisation/" target="_blank" rel="noopener">《A Statistical View of Deep Learning (V): Generalisation and Regularisation》</a></li></ul><p>介绍:深度学习的统计分析V:泛化和正则化.</p><ul><li><a href="http://arxiv.org/abs/1505.00387" target="_blank" rel="noopener">《Highway Networks》</a></li></ul><p>介绍:用SGD能高效完成训练的大规模(多层)深度网络HN.</p><ul><li><a href="http://www.erogol.com/what-i-read-for-deep-learning/" target="_blank" rel="noopener">《What I Read For Deep-Learning》</a></li></ul><p>介绍:深度学习解读文章.</p><ul><li><a href="http://dataconomy.com/an-introduction-to-recommendation-engines" target="_blank" rel="noopener">《An Introduction to Recommendation Engines》</a></li></ul><p>介绍:Coursera上的推荐系统导论（Introduction to Recommender Systems）公开课.</p><ul><li><a href="http://www.holehouse.org/mlclass/index.html" target="_blank" rel="noopener">《Stanford Machine Learning》</a></li></ul><p>介绍:Andrew Ng经典机器学习课程笔记.</p><ul><li><a href="http://yaroslavvb.blogspot.de/2015/05/iclr-2015_12.html" target="_blank" rel="noopener">《ICLR 2015》</a></li></ul><p>介绍:ICLR 2015见闻录,<a href="http://yaroslavvb.blogspot.de/" target="_blank" rel="noopener">博客</a>的其他机器学习文章也不错.</p><ul><li><a href="http://www.cripac.ia.ac.cn/People/sw/Xu2015PSR.pdf" target="_blank" rel="noopener">《Stanford Machine Learning》</a></li></ul><p>介绍:推荐系统”个性化语义排序”模型.</p><ul><li><a href="http://senseable.mit.edu/tweetbursts/" target="_blank" rel="noopener">《The More Excited We Are, The Shorter We Tweet》</a></li></ul><p>介绍:激情时分更惜字——MIT的最新Twitter研究结果.</p><ul><li><a href="http://hlt.suda.edu.cn/paper.html" target="_blank" rel="noopener">《苏州大学人类语言技术研究论文主页》</a></li></ul><p>介绍:苏州大学人类语言技术研究相关论文.</p><ul><li><a href="http://arxiv.org/abs/1505.00387" target="_blank" rel="noopener">《Neural Turing Machines implementation》</a></li></ul><p>介绍:实现神经图灵机(NTM),<a href="https://github.com/fumin/ntm" target="_blank" rel="noopener">项目地址</a>,此外推荐相关神经图灵机<a href="http://www.i-programmer.info/news/105-artificial-intelligence/7923-neural-turing-machines-learn-their-algorithms.html" target="_blank" rel="noopener">算法</a>.</p><ul><li><a href="http://www.cse.wustl.edu/~furukawa/cse559a/2015_spring/" target="_blank" rel="noopener">《Computer Vision - CSE 559A, Spring 2015》</a></li></ul><p>介绍:华盛顿大学的机器视觉(2015),参考资料<a href="http://szeliski.org/Book/" target="_blank" rel="noopener">Computer Vision: Algorithms and Applications</a>.</p><ul><li><a href="http://www.mmds.org/" target="_blank" rel="noopener">《Mining of Massive Datasets》</a></li></ul><p>介绍:”Mining of Massive Datasets”发布第二版,Jure Leskovec, Anand Rajaraman, Jeff Ullman 新版增加Jure Leskovec作为合作作者，新增社交网络图数据挖掘、降维和大规模机器学习三章,<a href="http://pan.baidu.com/s/1GvtpG" target="_blank" rel="noopener">电子版</a>依旧免费.</p><ul><li><a href="http://rt.dgyblog.com/ref/ref-learning-deep-learning.html" target="_blank" rel="noopener">《Learning Deep Learning》</a></li></ul><p>介绍:一个深度学习资源页,资料很丰富.</p><ul><li><a href="http://vdisk.weibo.com/s/ayG13we2ler9b" target="_blank" rel="noopener">《Learning Deep Learning》</a></li></ul><p>介绍:免费电子书”Learning Deep Learning”.</p><ul><li><a href="http://www.astroml.org/sklearn_tutorial/index.html" target="_blank" rel="noopener">《Tutorial: Machine Learning for Astronomy with Scikit-learn》</a></li></ul><p>介绍:Machine Learning for Astronomy with scikit-learn.</p><ul><li><a href="http://info.salford-systems.com/an-introduction-to-random-forests-for-beginners" target="_blank" rel="noopener">《An Introduction to Random Forests for Beginners》</a></li></ul><p>介绍:免费电子书”随机森林入门指南”.</p><ul><li><a href="http://rayli.net/blog/data/top-10-data-mining-algorithms-in-plain-english/" target="_blank" rel="noopener">《Top 10 data mining algorithms in plain English》</a></li></ul><p>介绍:白话数据挖掘十大算法.</p><ul><li><a href="https://www.mapr.com/blog/inside-look-at-components-of-recommendation-engine#.VVmZ5vmqqko" target="_blank" rel="noopener">《An Inside Look at the Components of a Recommendation Engine》</a></li></ul><p>介绍:基于Mahout和Elasticsearch的推荐系统,<a href="http://www.csdn.net/article/2015-05-14/2824676" target="_blank" rel="noopener">国内译版</a>.</p><ul><li><a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/15585/isbn9789526061498.pdf" target="_blank" rel="noopener">《Advances in Extreme Learning Machines》</a></li></ul><p>介绍:博士学位论文:ELM研究进展.</p><ul><li><a href="https://vimeo.com/59324550" target="_blank" rel="noopener">《10-minute tour of pandas》</a></li></ul><p>介绍:Pandas十分钟速览,<a href="http://nbviewer.ipython.org/urls/gist.github.com/wesm/4757075/raw/a72d3450ad4924d0e74fb57c9f62d1d895ea4574/PandasTour.ipynb" target="_blank" rel="noopener">ipn</a>.</p><ul><li><a href="http://pudo.org/blog/2015/05/15/document-mining.html" target="_blank" rel="noopener">《Data doesn’t grow in tables: harvesting journalistic insight from documents》</a></li></ul><p>介绍:面向数据新闻的文本挖掘.</p><ul><li><a href="http://grail.cs.washington.edu/projects/timelapse/" target="_blank" rel="noopener">《Time-lapse Mining from Internet Photos》</a></li></ul><p>介绍:用网络图片合成延时视频(SIGGRAPH 2015).</p><ul><li><a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" target="_blank" rel="noopener">《The Curse of Dimensionality in classification》</a></li></ul><p>介绍:分类系统的维数灾难.</p><ul><li><a href="http://www.computervisionblog.com/2015/05/deep-learning-vs-big-data-who-owns-what.html" target="_blank" rel="noopener">《Deep Learning vs Big Data: Who owns what?》</a></li></ul><p>介绍:深度学习vs.大数据——从数据到知识：版权的思考,[翻译版](<a href="http://www.csdn.net/article/2015-05-19/2824707" target="_blank" rel="noopener">http://www.csdn.net/article/2015-05-19/2824707</a></p><ul><li><a href="http://www.nature.com/ctg/journal/v5/n1/abs/ctg201319a.html" target="_blank" rel="noopener">《A Primer on Predictive Models》</a></li></ul><p>介绍:预测模型入门.</p><ul><li><a href="http://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/" target="_blank" rel="noopener">《Demistifying LSTM Neural Networks》</a></li></ul><p>介绍:深入浅出LSTM.</p><ul><li><a href="https://www.youtube.com/playlist?list=PLhiWXaTdsWB8PnrVZquVyqlRFWXM4ijYz" target="_blank" rel="noopener">《ICLR 2015》</a></li></ul><p>介绍:2015年ICLR会议<a href="http://pan.baidu.com/s/1bnbbRyR" target="_blank" rel="noopener">视频</a>与<a href="http://www.iclr.cc/doku.php?id=iclr2015:main" target="_blank" rel="noopener">讲义</a>.</p><ul><li><a href="http://dataremixed.com/2015/05/on-visualizing-data-well/" target="_blank" rel="noopener">《On Visualizing Data Well》</a></li></ul><p>介绍:Ben Jones的数据可视化建议.</p><ul><li><a href="http://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/" target="_blank" rel="noopener">《Decoding Dimensionality Reduction, PCA and SVD》</a></li></ul><p>介绍:解读数据降维/PCA/SVD.</p><ul><li><a href="http://ryancompton.net/assets/ml_cheat_sheet/supervised_learning.html" target="_blank" rel="noopener">《Supervised learning superstitions cheat sheet》</a></li></ul><p>介绍:IPN:监督学习方法示例/对比参考表,覆盖logistic回归, 决策树, SVM, KNN, Naive Bayes等方法.</p><ul><li><a href="http://arxiv.org/abs/1505.04771" target="_blank" rel="noopener">《DopeLearning: A Computational Approach to Rap Lyrics Generation》</a></li></ul><p>介绍:基于RankSVM和DNN自动(重组)生成Rap歌词.</p><ul><li><a href="https://www.sics.se/~mange/papers/RI_intro.pdf" target="_blank" rel="noopener">《An Introduction to Random Indexing》</a></li></ul><p>介绍:随机索引RI词空间模型专题.</p><ul><li><a href="http://www.vdiscover.org/" target="_blank" rel="noopener">《VDiscover》</a></li></ul><p>介绍:基于机器学习的漏洞检测工具VDiscover.</p><ul><li><a href="https://github.com/dmlc/minerva" target="_blank" rel="noopener">《Minerva》</a></li></ul><p>介绍:深度学习系统minerva。拥有python编程接口。多GPU几乎达到线性加速。在4块GPU上能在4天内将GoogLeNet训练到68.7%的top-1以及89.0%的top-5准确率。和同为dmlc项目的cxxnet相比，采用动态数据流引擎，提供更多灵活性。未来将和cxxnet一起整合为mxnet项目，互取优势.</p><ul><li><a href="http://www.cv-foundation.org/openaccess/CVPR2015.py" target="_blank" rel="noopener">《CVPR 2015 paper》</a></li></ul><p>介绍:2015年国际计算机视觉与模式识别会议paper.</p><ul><li><a href="http://www.quora.com/What-are-the-advantages-of-different-classification-algorithms/answer/Xavier-Amatriain" target="_blank" rel="noopener">《What are the advantages of different classification algorithms?》</a></li></ul><p>介绍:Netflix工程总监眼中的分类算法：深度学习优先级最低,<a href="http://www.csdn.net/article/2015-05-24/2824758" target="_blank" rel="noopener">中文版</a>.</p><ul><li><a href="https://www.codalab.org/competitions/3221#results" target="_blank" rel="noopener">《Results for Microsoft COCO Image Captioning Challenge》</a></li></ul><p>介绍:Codalab图像标注竞赛排行+各家论文,Reddit上flukeskywalker整理了各家技术<a href="http://www.reddit.com/r/MachineLearning/comments/376b28/comparison_of_official_test_scores_of_current/" target="_blank" rel="noopener">相关论文</a>.</p><ul><li><a href="http://arxiv.org/abs/1504.04343" target="_blank" rel="noopener">《Caffe con Troll: Shallow Ideas to Speed Up Deep Learning》</a></li></ul><p>介绍:基于Caffe的加速深度学习系统CcT.</p><ul><li><a href="http://arxiv.org/abs/1412.7024" target="_blank" rel="noopener">《Low precision storage for deep learning》</a></li></ul><p>介绍:深度学习(模型)低精度(训练与)存储.</p><ul><li><a href="http://www.mbmlbook.com/index.html" target="_blank" rel="noopener">《Model-Based Machine Learning (Early Access)》</a></li></ul><p>介绍:新书预览:模型机器学习.</p><ul><li><a href="http://www.princeton.edu/~sbubeck/SurveyBCB12.pdf" target="_blank" rel="noopener">《Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems》</a></li></ul><p>介绍:免费电子书多臂老虎机,此外推荐<a href="https://sites.google.com/site/banditstutorial/" target="_blank" rel="noopener">Introduction to Bandits: Algorithms and Theory</a>.</p><ul><li><a href="https://www.datacamp.com/courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic" target="_blank" rel="noopener">《Kaggle R Tutorial on Machine Learing》</a></li></ul><p>介绍:基于Kaggle’s Titanic Competition的交互式R机器学习教程,介绍<a href="http://blog.kaggle.com/2015/05/27/interactive-r-tutorial-machine-learning-for-the-titanic-competition/" target="_blank" rel="noopener">《Interactive R Tutorial: Machine Learning for the Titanic Competition》</a>.</p><ul><li><a href="http://suanfazu.com/t/deep-learning/9401" target="_blank" rel="noopener">《Deep Learning（深度学习）学习笔记整理系列》</a></li></ul><p>介绍:Deep Learning（深度学习）学习笔记整理系列.</p><ul><li><a href="http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/" target="_blank" rel="noopener">《Introduction to Neural Machine Translation with GPUs 》</a></li></ul><p>介绍:神经(感知)机器翻译介绍.</p><ul><li><a href="https://www.youtube.com/watch?v=n1ViNeWhC24&amp;hd=1" target="_blank" rel="noopener">《Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning》</a></li></ul><p>介绍:Andrew Ng关于深度学习/自学习/无监督特征学习的报告,<a href="http://pan.baidu.com/s/1jG8DUN8" target="_blank" rel="noopener">国内云</a>.</p><ul><li><a href="http://arxiv.org/abs/1505.04630" target="_blank" rel="noopener">《Recurrent Neural Network Training with Dark Knowledge Transfer》</a></li></ul><p>介绍:论文:通过潜在知识迁移训练RNN.</p><ul><li><a href="https://github.com/chrischris292/ShowMeTheMoney" target="_blank" rel="noopener">《Show Me The Money》</a></li></ul><p>介绍:面向金融数据的情感分析工具.</p><ul><li><a href="https://github.com/bmabey/pyLDAvis" target="_blank" rel="noopener">《pyLDAvis》</a></li></ul><p>介绍:(Python)主题模型交互可视化库pyLDAvis.</p><ul><li><a href="http://nbviewer.ipython.org/github/tfolkman/learningwithdata/blob/master/Logistic%20Gradient%20Descent.ipynb" target="_blank" rel="noopener">《Logistic Regression and Gradient Descent》</a></li></ul><p>介绍:Logistic回归与优化实例教程.</p><ul><li><a href="http://pan.baidu.com/s/1dDGVL53" target="_blank" rel="noopener">《贾扬清微信讲座记录》</a></li></ul><p>介绍:贾扬清（谷歌大脑科学家、caffe缔造者）微信讲座记录.</p><ul><li><a href="https://github.com/udibr/sketch" target="_blank" rel="noopener">《sketch》</a></li></ul><p>介绍:Theano/Blocks实现RNN手写字符串生成sketch.</p><ul><li><a href="http://chris.de-vries.id.au/2015/05/web-scale-document-clustering.html" target="_blank" rel="noopener">《Web Scale Document Clustering: Clustering 733 Million Web Pages》</a></li></ul><p>介绍:基于TopSig的海量(7亿+)网页聚类.</p><ul><li><a href="http://aclweb.org/anthology/N/N15/" target="_blank" rel="noopener">《NAACL 2015 Proceedings on ACL Anthology》</a></li></ul><p>介绍:NAACL 2015 论文papers.</p><ul><li><a href="http://www.anlytcs.com/2015/05/stock-forecasting-with-machine-learning.html" target="_blank" rel="noopener">《Stock Forecasting With Machine Learning - Seven Possible Errors》</a></li></ul><p>介绍:机器学习预测股市的七个问题.</p><ul><li><a href="http://www.reddit.com/r/MachineLearning/comments/378but/are_there_any_good_resources_for_learning_about/" target="_blank" rel="noopener">《Are there any good resources for learning about neural networks?》</a></li></ul><p>介绍:神经网络学习资料推荐.</p><ul><li><a href="http://arxiv.org/abs/1506.00019v1" target="_blank" rel="noopener">《A Critical Review of Recurrent Neural Networks for Sequence Learning》</a></li></ul><p>介绍:面向序列学习的RNN综述.</p><ul><li><a href="http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf" target="_blank" rel="noopener">《Handling and Processing Strings in R》</a></li></ul><p>介绍:R文本处理手册.</p><ul><li><a href="https://github.com/s16h/py-must-watch" target="_blank" rel="noopener">《Must-watch videos about Python》</a></li></ul><p>介绍:“必看”的Python视频集锦.</p><ul><li><a href="http://malteschwarzkopf.de/research/assets/google-stack.pdf" target="_blank" rel="noopener">《The Google Stack》</a></li></ul><p>介绍:Google(基础结构)栈.</p><ul><li><a href="http://cs.stanford.edu/people/mmahoney/f13-stat260-cs294/" target="_blank" rel="noopener">《Randomized Algorithms for Matrices and Data》</a></li></ul><p>介绍:矩阵和数据的随机算法(UC Berkeley 2013).</p><ul><li><a href="https://www.datacamp.com/courses/intermediate-r" target="_blank" rel="noopener">《Intermediate R》</a></li></ul><p>介绍:DataCamp中级R语言教程.</p><ul><li><a href="http://www.topologywithouttears.net/" target="_blank" rel="noopener">《Topology Without Tears》</a></li></ul><p>介绍:免费电子书:轻松掌握拓扑学,<a href="http://www.topologywithouttears.net/topbookchinese.pdf" target="_blank" rel="noopener">中文版</a>.</p><ul><li><a href="http://www.inference.phy.cam.ac.uk/itprnn_lectures/" target="_blank" rel="noopener">《Information Theory, Pattern Recognition, and Neural Networks》</a></li></ul><p>介绍:<a href="http://www.inference.phy.cam.ac.uk/itprnn/book.pdf" target="_blank" rel="noopener">Book</a>,<a href="https://www.youtube.com/user/jakobfoerster/videos" target="_blank" rel="noopener">video</a>.</p><ul><li><a href="www.github.com/scikit-learn/scikit-learn">《Scikit-learn》</a></li></ul><p>介绍:Scikit-learn 是基于Scipy为机器学习建造的的一个Python模块，他的特色就是多样化的分类，回归和聚类的算法包括支持向量机，逻辑回归，朴素贝叶斯分类器，随机森林，Gradient Boosting，聚类算法和DBSCAN。而且也设计出了Python numerical和scientific libraries Numpy and Scipy</p><ul><li><a href="www.github.com/lisa-lab/pylearn2">《Pylearn2》</a></li></ul><p>介绍:Pylearn是一个让机器学习研究简单化的基于Theano的库程序。</p><ul><li><a href="www.github.com/numenta/nupic">《NuPIC》</a></li></ul><p>介绍:NuPIC是一个以HTM学习算法为工具的机器智能平台。HTM是皮层的精确计算方法。HTM的核心是基于时间的持续学习算法和储存和撤销的时空模式。NuPIC适合于各种各样的问题,尤其是检测异常和预测的流数据来源。</p><ul><li><a href="www.github.com/nilearn/nilearn">《Nilearn》</a></li></ul><p>介绍:Nilearn 是一个能够快速统计学习神经影像数据的Python模块。它利用Python语言中的scikit-learn 工具箱和一些进行预测建模，分类，解码，连通性分析的应用程序来进行多元的统计。</p><ul><li><a href="www.github.com/pybrain/pybrain">《PyBrain》</a></li></ul><p>介绍:Pybrain是基于Python语言强化学习，人工智能，神经网络库的简称。 它的目标是提供灵活、容易使用并且强大的机器学习算法和进行各种各样的预定义的环境中测试来比较你的算法。</p><ul><li><a href="www.github.com/clips/pattern">《Pattern》</a></li></ul><p>介绍:Pattern 是Python语言下的一个网络挖掘模块。它为数据挖掘，自然语言处理，网络分析和机器学习提供工具。它支持向量空间模型、聚类、支持向量机和感知机并且用KNN分类法进行分类。</p><ul><li><a href="www.github.com/mila-udem/fuel">《Fuel》</a></li></ul><p>介绍:Fuel为你的机器学习模型提供数据。他有一个共享如MNIST, CIFAR-10 (图片数据集), Google’s One Billion Words (文字)这类数据集的接口。你使用他来通过很多种的方式来替代自己的数据。</p><ul><li><a href="www.github.com/idiap/bob">《Bob》</a></li></ul><p>介绍:Bob是一个免费的信号处理和机器学习的工具。它的工具箱是用Python和C++语言共同编写的，它的设计目的是变得更加高效并且减少开发时间，它是由处理图像工具,音频和视频处理、机器学习和模式识别的大量软件包构成的。</p><ul><li><a href="www.github.com/jaberg/skdata">《Skdata》</a></li></ul><p>介绍:Skdata是机器学习和统计的数据集的库程序。这个模块对于玩具问题，流行的计算机视觉和自然语言的数据集提供标准的Python语言的使用。</p><ul><li><a href="www.github.com/luispedro/milk">《MILK》</a></li></ul><p>介绍:MILK是Python语言下的机器学习工具包。它主要是在很多可得到的分类比如SVMS,K-NN,随机森林，决策树中使用监督分类法。 它还执行特征选择。 这些分类器在许多方面相结合,可以形成不同的例如无监督学习、密切关系金传播和由MILK支持的K-means聚类等分类系统。</p><ul><li><a href="www.github.com/machinalis/iepy">《IEPY》</a></li></ul><p>介绍:IEPY是一个专注于关系抽取的开源性信息抽取工具。它主要针对的是需要对大型数据集进行信息提取的用户和想要尝试新的算法的科学家。</p><ul><li><a href="www.github.com/machinalis/quepy">《Quepy》</a></li></ul><p>介绍:Quepy是通过改变自然语言问题从而在数据库查询语言中进行查询的一个Python框架。他可以简单的被定义为在自然语言和数据库查询中不同类型的问题。所以，你不用编码就可以建立你自己的一个用自然语言进入你的数据库的系统。现在Quepy提供对于Sparql和MQL查询语言的支持。并且计划将它延伸到其他的数据库查询语言。</p><ul><li><a href="www.github.com/hannes-brt/hebel">《Hebel》</a></li></ul><p>介绍:Hebel是在Python语言中对于神经网络的深度学习的一个库程序，它使用的是通过PyCUDA来进行GPU和CUDA的加速。它是最重要的神经网络模型的类型的工具而且能提供一些不同的活动函数的激活功能，例如动力，涅斯捷罗夫动力，信号丢失和停止法。</p><ul><li><a href="www.github.com/rasbt/mlxtend">《mlxtend》</a></li></ul><p>介绍:它是一个由有用的工具和日常数据科学任务的扩展组成的一个库程序。</p><ul><li><a href="www.github.com/dnouri/nolearn">《nolearn》</a></li></ul><p>介绍:这个程序包容纳了大量能对你完成机器学习任务有帮助的实用程序模块。其中大量的模块和scikit-learn一起工作，其它的通常更有用。</p><ul><li><a href="www.github.com/kvh/ramp">《Ramp》</a></li></ul><p>介绍:Ramp是一个在Python语言下制定机器学习中加快原型设计的解决方案的库程序。他是一个轻型的pandas-based机器学习中可插入的框架，它现存的Python语言下的机器学习和统计工具（比如scikit-learn,rpy2等）Ramp提供了一个简单的声明性语法探索功能从而能够快速有效地实施算法和转换。</p><ul><li><a href="www.github.com/machinalis/featureforge">《Feature Forge》</a></li></ul><p>介绍:这一系列工具通过与scikit-learn兼容的API，来创建和测试机器学习功能。这个库程序提供了一组工具，它会让你在许多机器学习程序使用中很受用。当你使用scikit-learn这个工具时，你会感觉到受到了很大的帮助。（虽然这只能在你有不同的算法时起作用。）</p><ul><li><a href="www.github.com/yandex/rep">《REP》</a></li></ul><p>介绍:REP是以一种和谐、可再生的方式为指挥数据移动驱动所提供的一种环境。它有一个统一的分类器包装来提供各种各样的操作，例如TMVA, Sklearn, XGBoost, uBoost等等。并且它可以在一个群体以平行的方式训练分类器。同时它也提供了一个交互式的情节。</p><ul><li><a href="www.github.com/awslabs/machine-learning-samples">《Python 学习机器样品》</a></li></ul><p>介绍:用亚马逊的机器学习建造的简单软件收集。</p><ul><li><a href="www.github.com/dclambert/Python-ELM">《Python-ELM》</a></li></ul><p>介绍:这是一个在Python语言下基于scikit-learn的极端学习机器的实现。</p><ul><li><a href="http://forum.memect.com/thread/dimension-reduction/" target="_blank" rel="noopener">《Dimension Reduction》</a></li></ul><p>介绍:电子书降维方法,此外还推荐<a href="http://www.stat.washington.edu/courses/stat539/spring14/Resources/tutorial_nonlin-dim-red.pdf" target="_blank" rel="noopener">Dimensionality Reduction A Short Tutorial</a>、<a href="http://lvdmaaten.github.io/drtoolbox/" target="_blank" rel="noopener">Matlab Toolbox for Dimensionality Reduction</a>、<a href="http://www.cs.berkeley.edu/~jordan/papers/wang-sha-jordan-nips11.pdf" target="_blank" rel="noopener">Unsupervised Kernel Dimension Reduction</a></p><ul><li><a href="http://deeplearning.net/datasets/" target="_blank" rel="noopener">《Datasets Used For Benchmarking Deep Learning Algorithms》</a></li></ul><p>介绍:deeplearning.net整理的深度学习数据集列表.</p><ul><li><a href="https://github.com/advancedlogic/go-freeling" target="_blank" rel="noopener">《Golang Natural Language Processing》</a></li></ul><p>介绍:Go语言编写的自然语言处理工具.</p><ul><li><a href="http://arxiv.org/abs/1412.4930" target="_blank" rel="noopener">《Rehabilitation of Count-based Models for Word Vector Representations》</a></li></ul><p>介绍:词频模型对词向量的反击,参考<a href="https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf" target="_blank" rel="noopener">Improving Distributional Similarity with Lessons Learned from Word Embeddings </a>。</p><ul><li><a href="http://static1.squarespace.com/static/51156277e4b0b8b2ffe11c00/t/55344152e4b0ff30bb9ec163/1429487954122/ASA_Kuhn.pdf" target="_blank" rel="noopener">《Three Aspects of Predictive Modeling》</a></li></ul><p>介绍:预测模型的三个方面.</p><ul><li><a href="http://cs224d.stanford.edu/" target="_blank" rel="noopener">《CS224d: Deep Learning for Natural Language Processing》</a></li></ul><p>介绍:斯坦福大学深度学习与自然语言处理课程,部分课程笔记<a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F" target="_blank" rel="noopener">词向量</a>、<a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%B8%80%E8%AE%B2%E5%BC%95%E8%A8%80" target="_blank" rel="noopener">引言</a></p><ul><li><a href="http://googleresearch.blogspot.jp/2015/06/google-computer-vision-research-at-cvpr.html" target="_blank" rel="noopener">《Google Computer Vision research at CVPR 2015》</a></li></ul><p>介绍:CVPR2015上Google的CV研究列表.</p><ul><li><a href="http://public.hudl.com/bits/archives/2015/06/05/highlights/" target="_blank" rel="noopener">《Using Deep Learning to Find Basketball Highlights》</a></li></ul><p>介绍:利用(Metamind)深度学习自动发现篮球赛精彩片段.</p><ul><li><a href="http://arxiv.org/abs/1512.04150" target="_blank" rel="noopener">《Learning Deep Features for Discriminative Localization》</a></li></ul><p>介绍:对本土化特征学习的分析</p>]]></content>
      
      
      <categories>
          
          <category> 资料整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLTK基础教程】02  何为文本歧义</title>
      <link href="/2018/01/22/%E3%80%90NLTK%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E3%80%9102-1%20%E4%BD%95%E4%B8%BA%E6%96%87%E6%9C%AC%E6%AD%A7%E4%B9%89/"/>
      <url>/2018/01/22/%E3%80%90NLTK%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E3%80%9102-1%20%E4%BD%95%E4%B8%BA%E6%96%87%E6%9C%AC%E6%AD%A7%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<p>文本歧义，书中的定义式从原生数据中获取一段机器可读的已经格式化文本之前所要做的所有预处理工作，以及所有繁复的任务。该过程涉及到数据再加工，文本清理，特定项处理，标识化处理，词干提取或词型还原以及停用词移除等操作。<br><a id="more"></a><br>好吧，书中将文本歧义定义为数据预处理这一些列工作，难到文本歧义不应该是一个文本，多个意思，从而有歧义这个意思吗？没搞懂（问号脸.jpg）.如果是把文本歧义理解成通过一系列数据预处理工作，消除文本歧义，好吧，貌似说得通。那也不纠结那么多了，下面看一个例子，解析一个csv文件。</p><pre><code>import csvwith open(&#39;example.csv&#39;) as f:    reader=csv.reader(f,delimiter=&#39;,&#39;,quotechar=&#39;&quot;&#39;)    for line in reader:        print line[1]</code></pre><p>代码说明：<br>这几句代码整体上是没有什么问题的，这里只是提下csv的reader方法的参数：<br>①delimiter：一行中的分隔符<br>②quotechar：每个字段用的类型符号<br>这里就会涉及到处理文档类型的一般流程，具体见下图：<br><img src="http://img.blog.csdn.net/20180122210720461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHl6MTU4NDE3MjgwOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="image"><br>在大多数情况下，我们所遇到的这些数据中的某一个，而python中也有对于这些数据格式最常见的封装格式。通过该模块，我们可以使用各种不同的分离器和引用符等工具。<br>接下来，我们再来看一个json文件示例：<br>①json数据为：</p><pre><code>{    &quot;array&quot;:[1,2,3,4],    &quot;boolean&quot;:True,    &quot;object&quot;:{        &quot;a&quot;:&quot;b&quot;    },    &quot;string&quot;:&quot;hello world&quot;}</code></pre><p>②处理该字符串的解析代码如下：</p><pre><code>import jsonjsonfile=open(&quot;example.json&quot;)data=json.load(jsonfile)print(data[&#39;string&#39;])</code></pre><p>好吧，这个就记录到这里吧！</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLTK基础教程 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLTK基础教程】01-02 利用nltk统计词频</title>
      <link href="/2018/01/21/%E3%80%90NLTK%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E3%80%9101-02%20%E5%88%A9%E7%94%A8nltk%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91(1)/"/>
      <url>/2018/01/21/%E3%80%90NLTK%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E3%80%9101-02%20%E5%88%A9%E7%94%A8nltk%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91(1)/</url>
      
        <content type="html"><![CDATA[<p>在上篇中，简单的介绍了三种获取有效文本的方法，那么接下来就利用nltk来统计这些文本中出现的次数。<br><a id="more"></a><br>我们首先来看下传统统计词频的方法：</p><pre><code>import operatorfreq_dis={}for tok in tokens:    if tok in freq_dis:        freq_dis[tok]+=1    else:        freq_dis[tok]=1sorted_freq_dist=sorted(freq_dis.items(),key=operator.itemgetter(1),reverse=True)print(sorted_freq_dist[:25])</code></pre><p>统计结果如下：</p><pre><code>[(&#39;Python&#39;, 59), (&#39;&gt;&gt;&gt;&#39;, 24), (&#39;the&#39;, 21), (&#39;and&#39;, 21), (&#39;to&#39;, 17), (&#39;is&#39;, 17), (&#39;of&#39;, 17), (&#39;=&#39;, 14), (&#39;for&#39;, 11), (&#39;News&#39;, 11), (&#39;Events&#39;, 11), (&#39;a&#39;, 10), (&#39;#&#39;, 9), (&#39;More&#39;, 9), (&#39;3&#39;, 8), (&#39;in&#39;, 8), (&#39;with&#39;, 7), (&#39;Community&#39;, 7), (&#39;...&#39;, 7), (&#39;Docs&#39;, 6), (&#39;Guide&#39;, 6), (&#39;Software&#39;, 6), (&#39;The&#39;, 5), (&#39;1&#39;, 5), (&#39;that&#39;, 5)]</code></pre><p>利用nltk来统计文本词频如下：</p><pre><code>import nltkFreq_dist_nltk=nltk.FreqDist(tokens)print(Freq_dist_nltk)for k,v in Freq_dist_nltk.items():    print(str(k)+&quot;:&quot;+str(v))Freq_dist_nltk.plot(50,cumulative=False)</code></pre><p>相比之下，利用nltk库来实现，确实便利了很多。<br><img src="http://img.blog.csdn.net/20180121203411270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHl6MTU4NDE3MjgwOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="image"><br>好吧，《NLTK基础教程》第一章基本上就结束了，这一章主要是简单介绍了python的语法，然后引出NLTK。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLTK基础教程 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【NLTK基础教程】01-1 三种获取网页中有效文本的方法</title>
      <link href="/2018/01/21/%E3%80%90NLTK%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E3%80%9101-01%20%E4%B8%89%E7%A7%8D%E8%8E%B7%E5%8F%96%E7%BD%91%E9%A1%B5%E4%B8%AD%E6%9C%89%E6%95%88%E6%96%87%E6%9C%AC%E7%9A%84%E6%96%B9%E6%B3%95(1)/"/>
      <url>/2018/01/21/%E3%80%90NLTK%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E3%80%9101-01%20%E4%B8%89%E7%A7%8D%E8%8E%B7%E5%8F%96%E7%BD%91%E9%A1%B5%E4%B8%AD%E6%9C%89%E6%95%88%E6%96%87%E6%9C%AC%E7%9A%84%E6%96%B9%E6%B3%95(1)/</url>
      
        <content type="html"><![CDATA[<p>本篇是《NLTK基础教程》第一章的第三节，向NLTK迈进中的一个例子，获取网页中的文本的三种方法。<br><a id="more"></a></p><h4 id="1-抓取网页"><a href="#1-抓取网页" class="headerlink" title="1 抓取网页"></a>1 抓取网页</h4><p>这里主要是用了urllib库中的函数来抓取指定网页，代码如下：</p><pre><code>import urllib.requestresponse=urllib.request.urlopen(&#39;http://python.org&#39;)html=response.read()print(len(html))</code></pre><p>由于python版本的关系，哈哈，这里有个坑，那我就指出来吧！以后如果有坑的地方，我还是先踩下！嘿嘿。</p><p>==【踩坑专业专用标记】==<br>（1）如果你的python版本是2.x，那么你的前两句代码应该如下：</p><pre><code>import urllib2response=urllib2.urlopen(&#39;http://python.org&#39;)</code></pre><p>(2)如果你的python版本是3.x，那么就使用最上面的代码就可以了，即：</p><pre><code>import urllib.requestresponse=urllib.request.urlopen(&#39;http://python.org&#39;)</code></pre><h4 id="2-获取网页中的文本"><a href="#2-获取网页中的文本" class="headerlink" title="2 获取网页中的文本"></a>2 获取网页中的文本</h4><p>好吧，前期抓取网页的准备工作完成了，那么接下来获取网页中的有效文本（也就是想办法去掉html标签和多余的字符），教材中简单的讲了三种方法，我感觉挺有用的，也会经常用到，于是就记录下来了。<br>（1）方法一：要清理掉html标签，一种可行的做法是只选取其中的标记，包括数字和字符，然后替换掉就可以了。</p><pre><code>tokens=[tok for tok in html.split()]print(&quot;total no of tokens :&quot;+str(len(tokens)))print(tokens[0:100])</code></pre><p>当然匹配这些无用字符还有更简洁的版本，也就是通过正则表达式来实现，见方法二。<br>（2）方法二：利用正则表达是来匹配无用字符。</p><pre><code>import retokens=re.split(&quot;\w+&quot;,html)print(len(tokens))print(tokens[0:100])</code></pre><p>以上两种方法都是从方面来获取有效文本。<br>（3）方法三；通过ntlk来获取网页中有效文本。<br>还是直接上代码吧，简单粗暴😏。</p><pre><code>import nltkfrom bs4 import BeautifulSoupsoup = BeautifulSoup(html)text = soup.get_text()tokens=[tok for tok in text.split()]print(tokens[0:100])</code></pre><p>好吧，这里也有个坑，也就是nltk中的clean_html()方法不可用了，需要用BeautifulSoup的 get_text()方法。</p><p>==【踩坑专业专用标记】==<br>因此教材上的代码：</p><pre><code>clean=nltk.clean_html(html)</code></pre><p>应该改为：</p><pre><code>from bs4 import BeautifulSoupsoup = BeautifulSoup(html)text = soup.get_text()</code></pre><p>好吧，到这里，这三种方法就介绍完了，如果有什么问题，欢迎留言。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLTK基础教程 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【leetcode】724. Find Pivot Index</title>
      <link href="/2018/01/18/%E3%80%90leetcode%E3%80%91724.%20Find%20Pivot%20Index/"/>
      <url>/2018/01/18/%E3%80%90leetcode%E3%80%91724.%20Find%20Pivot%20Index/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Given an array of integers nums, write a method that returns the “pivot” index of this array.<br>We define the pivot index as the index where the sum of the numbers to the left of the index is equal to the sum of the numbers to the right of the index.<br>If no such index exists, we should return -1. If there are multiple pivot indexes, you should return the left-most pivot index.<br><a id="more"></a></p></blockquote><h3 id="1-基本思路"><a href="#1-基本思路" class="headerlink" title="1 基本思路"></a>1 基本思路</h3><p>可以利用空间换时间，依次将从左边累加和从右边累加的结果分别保存在两个数组中，然后逐一比较这两个累加数组，当相等，则返回当前下标。<br>为什么两个累加数组中，相同索引i所对应的数组元素值想等，该下i标就是所要求的呢？因为左边累加的数组减去当前元素，就是i左边所有元素之和，而右累加数组减去当前元素，就是i右边元素之和，而减去的元素值是同一个，所有i满足条件。</p><h3 id="2-完整代码"><a href="#2-完整代码" class="headerlink" title="2 完整代码"></a>2 完整代码</h3><pre><code>class Solution {public:    int pivotIndex(vector&lt;int&gt;&amp; nums) {        //获取数组中元素个数        int len=nums.size();        int* a=new int[len];        int* b=new int[len];        if(len==0)            return -1;        if(len==1)            return 0;        int sumi=0;        int sumj=0;        int i=0;        int j=len-1;        for(i=0;i&lt;len;i++)        {            sumi=nums[i]+sumi;            a[i]=sumi;        }        for(j=len-1;j&gt;-1;j--)        {            sumj=sumj+nums[j];            b[j]=sumj;        }        for(i=0;i&lt;len;i++)        {            if(a[i]==b[i])                return i;        }        return -1;    }};</code></pre>]]></content>
      
      
      <categories>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【自然语言处理入门】03：利用线性回归对数据集进行分析预测（下）</title>
      <link href="/2017/12/21/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9103%EF%BC%9A%E5%88%A9%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
      <url>/2017/12/21/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9103%EF%BC%9A%E5%88%A9%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>上一篇中我们简单的介绍了<a href="https://xiongzongyang.github.io/2017/12/21/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9103%EF%BC%9A%E5%88%A9%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%EF%BC%88%E4%B8%8A%EF%BC%89/" target="_blank" rel="noopener">利用线性回归分析并预测波士顿房价数据集</a>，那么在这一篇中，将使用相同的模型来对红酒数据集进行分析。<br><a id="more"></a></p></blockquote><h3 id="1-基本要求"><a href="#1-基本要求" class="headerlink" title="1 基本要求"></a>1 基本要求</h3><p>利用线性回归，对红酒数据集进行分析。数据集<a href="https://github.com/xiongzongyang/hexo_photo" target="_blank" rel="noopener">下载地址</a>。</p><h3 id="2-完整代码"><a href="#2-完整代码" class="headerlink" title="2 完整代码"></a>2 完整代码</h3><pre><code>#-*- coding: UTF-8 -*-# @Time    : 2017/12/21 9:29# @Author  : xiongzongyang# @Software: PyCharmimport pandas as pdfrom pandas import Series, DataFrameimport numpy as npfrom sklearn.linear_model import LinearRegression #线性回归from sklearn.metrics import mean_squared_errorimport matplotlib as mplimport matplotlib.pyplot as plt#读取数据def read_data(data_path=&quot;./data/&quot;):    test_data = pd.read_csv(data_path+&quot;test.csv&quot;, header=None)    train_data = pd.read_csv(data_path+&quot;train.csv&quot;, header=None)    return train_data,test_data#数据处理def deal_data(pd_data):    #获取数据的行数,因为要出去字段名，所以要-1    row_cnt = pd_data.shape[0]-1    #计算列数，因为在读入数据时，没有指定分隔符，所以所有列数据都是作为一列数据来读入的，因此在计算列数时，将读入的每一行按照;来分开    column_cnt = len(pd_data.iloc[0, 0].split(&quot;;&quot;))    #empty 会创建一个没有使用特定值来初始化的数组。给这些方法传递一个元组作为形状来创建高维数组：    X = np.empty((row_cnt, column_cnt - 1))    Y = np.empty((row_cnt, 1))    column_name=pd_data.iloc[0, 0].split(&quot;;&quot;)    #开始获取数据    for i in range(0, row_cnt):        #逐一将每一行进行分割（按;空格分割）        row_array = pd_data.iloc[i+1, 0].split(&quot;;&quot;)        #x取前13个数据，X[i]是一个一维数组，则X相当于一个二维数组，Y同理        X[i] = np.array(row_array[0:-1])        #y取最后一个数据        Y[i] = np.array(row_array[-1])    return X, Y,column_name#把特征标准化为均匀分布def uniform_norm(X):    X_max = X.max(axis=0)    X_min = X.min(axis=0)    return (X - X_min) / (X_max - X_min), X_max, X_min#实现线性回归#画图def draw(pred,test_Y):    t = np.arange(len(pred))    mpl.rcParams[&#39;font.sans-serif&#39;] = [u&#39;simHei&#39;]    mpl.rcParams[&#39;axes.unicode_minus&#39;] = False    plt.figure(facecolor=&#39;w&#39;)    plt.plot(t, test_Y, &#39;r-&#39;, lw=2, label=u&#39;true value&#39;)    plt.plot(t, pred, &#39;b-&#39;, lw=2, label=u&#39;estimated&#39;)    plt.legend(loc=&#39;best&#39;)    plt.title(u&#39;wine quality&#39;, fontsize=18)    plt.xlabel(u&#39;case id&#39;, fontsize=15)    plt.ylabel(u&#39;quality&#39;, fontsize=15)    plt.grid()    plt.show()#模型评估def evaluate(unif_train_X,train_Y,unif_test_X,test_Y):    print(&quot;训练集上效果评估:&quot;)    pred_train = model.predict(unif_train_X)    print(&quot;R^2系数 &quot;, model.score(unif_train_X, train_Y))    print(&quot;均方误差 &quot;, mean_squared_error(train_Y, pred_train))    print(&quot;\n测试集上效果评估 :&quot;)    r2 = model.score(unif_test_X, test_Y)    print(&quot;R^2系数 &quot;, r2)    pred = model.predict(unif_test_X)    print(&quot;均方误差 &quot;, mean_squared_error(test_Y, pred))#主函数if __name__ == &quot;__main__&quot;:    #读取数据    train_data,test_data=read_data()    #数据处理    train_X, train_Y,column_name=deal_data(train_data)    # print(train_X.shape)    # print(train_Y.shape)    test_X, test_Y,column_name=deal_data(test_data)    # print(test_X.shape)    # print(test_Y.shape)    #把特征标准化为均匀分布    unif_train_X, max_X, min_X = uniform_norm(train_X)    unif_test_X = (test_X - min_X) / (max_X - min_X)    #实现线性回归    model = LinearRegression()    model.fit(unif_train_X, train_Y)    #在训练集上预测    pred_train = model.predict(unif_train_X)    #在测试集上预测    pred = model.predict(unif_test_X)    #画图    draw(pred,test_Y)    #模型评估    evaluate(unif_train_X,train_Y,unif_test_X,test_Y)</code></pre><p>注意：本数据集中，每一列都有列名，因此在数据预处理的过程中要去掉。</p><h3 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3 实验结果"></a>3 实验结果</h3><p><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/QQ%E5%9B%BE%E7%89%8720171221191054.png" alt="image"></p><h3 id="4-实验小结"><a href="#4-实验小结" class="headerlink" title="4 实验小结"></a>4 实验小结</h3><p>结果这两个实验可以看出，一般数据分析处理流程如下图所示。</p><pre><code>graph LR数据读取--&gt;数据预处理选择模型--&gt;训练模型数据预处理--&gt;训练模型训练模型--&gt;测试模型测试模型--&gt;模型评估</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 木豆课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【自然语言处理入门】03：利用线性回归对数据集进行分析预测（上）</title>
      <link href="/2017/12/21/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9103%EF%BC%9A%E5%88%A9%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
      <url>/2017/12/21/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9103%EF%BC%9A%E5%88%A9%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇笔记是《从自然语言处理到机器学习入门》课程第三次作业的上篇，主要是复现了老大课上讲的利用线性回归对波士顿房价进行预测的实验。在下篇中，将利用该模型对红酒数据集进行线性回归分析。<br><a id="more"></a></p><h3 id="1-基本要求"><a href="#1-基本要求" class="headerlink" title="1 基本要求"></a>1 基本要求</h3><p>利用提供的波士顿房价数据，对其进行分析。<a href="https://github.com/xiongzongyang/hexo_photo" target="_blank" rel="noopener">数据地址</a>,训练集合测试集已经分好了。</p><h3 id="2-完整代码"><a href="#2-完整代码" class="headerlink" title="2 完整代码"></a>2 完整代码</h3></blockquote><pre><code>#-*- coding: UTF-8 -*-# @Time    : 2017/12/21 9:29# @Author  : xiongzongyang# @Software: PyCharmimport pandas as pdfrom pandas import Series, DataFrameimport numpy as npimport matplotlib.pyplot as pltimport sklearn# from sklean.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegression #线性回归from sklearn.linear_model import Ridge #岭回归from sklearn.linear_model import Lassofrom sklearn.linear_model import ElasticNetCVfrom sklearn.metrics import mean_squared_errorimport matplotlib as mplimport matplotlib.pyplot as plt#读取数据def read_data(data_path=&quot;./data/&quot;):    test_data = pd.read_csv(data_path+&quot;boston_house.test&quot;, header=None)    train_data = pd.read_csv(data_path+&quot;boston_house.train&quot;, header=None)    #测试数据是否读取成功    # print(train_data)    # print(test_data.head())    # #查看数据的规模    # print(train_data.shape)    # print(test_data.shape)    return train_data,test_data#数据处理def deal_data(pd_data):    #获取数据的行数    row_cnt = pd_data.shape[0]    #print(row_cnt)    #计算列数，因为在读入数据时，没有指定分隔符，所以14列数据都是作为一列数据来读入的，因此在计算列数时，将读入的每一行按照空格来分开    column_cnt = len(pd_data.iloc[0, 0].split())    #print(column_cnt)    #empty 会创建一个没有使用特定值来初始化的数组。给这些方法传递一个元组作为形状来创建高维数组：    X = np.empty((row_cnt, column_cnt - 1))    # print(X)    #创建一个高位数组    Y = np.empty((row_cnt, 1))    for i in range(0, row_cnt):        #逐一将每一行进行分割（按空格分割）        row_array = pd_data.iloc[i, 0].split()        #x取前13个数据，X[i]是一个一维数组，则X相当于一个二维数组，Y同理        X[i] = np.array(row_array[0:-1])        #y取最后一个数据        Y[i] = np.array(row_array[-1])    return X, Y#把特征标准化为均匀分布def uniform_norm(X):    X_max = X.max(axis=0)    X_min = X.min(axis=0)    return (X - X_min) / (X_max - X_min), X_max, X_min#实现线性回归#画图def draw(pred,test_Y):    t = np.arange(len(pred))    mpl.rcParams[&#39;font.sans-serif&#39;] = [u&#39;simHei&#39;]    mpl.rcParams[&#39;axes.unicode_minus&#39;] = False    plt.figure(facecolor=&#39;w&#39;)    plt.plot(t, test_Y, &#39;r-&#39;, lw=2, label=u&#39;true value&#39;)    plt.plot(t, pred, &#39;b-&#39;, lw=2, label=u&#39;estimated&#39;)    plt.legend(loc=&#39;best&#39;)    plt.title(u&#39;Boston house price&#39;, fontsize=18)    plt.xlabel(u&#39;case id&#39;, fontsize=15)    plt.ylabel(u&#39;house price&#39;, fontsize=15)    plt.grid()    plt.show()#模型评估def evaluate(unif_train_X,train_Y,unif_test_X,test_Y):    print(&quot;训练集上效果评估:&quot;)    pred_train = model.predict(unif_train_X)    print(&quot;R^2系数 &quot;, model.score(unif_train_X, train_Y))    print(&quot;均方误差 &quot;, mean_squared_error(train_Y, pred_train))    print(&quot;\n测试集上效果评估 :&quot;)    r2 = model.score(unif_test_X, test_Y)    print(&quot;R^2系数 &quot;, r2)    pred = model.predict(unif_test_X)    print(&quot;均方误差 &quot;, mean_squared_error(test_Y, pred))#主函数if __name__ == &quot;__main__&quot;:    #读取数据    train_data,test_data=read_data()    #数据处理    train_X, train_Y=deal_data(train_data)    # print(train_X.shape)    # print(train_Y.shape)    test_X, test_Y=deal_data(test_data)    # print(test_X.shape)    # print(test_Y.shape)    #把特征标准化为均匀分布    unif_train_X, max_X, min_X = uniform_norm(train_X)    unif_test_X = (test_X - min_X) / (max_X - min_X)    #实现线性回归    model = LinearRegression()    model.fit(unif_train_X, train_Y)    #在训练集上预测    pred_train = model.predict(unif_train_X)    #在测试集上预测    pred = model.predict(unif_test_X)    #画图    draw(pred,test_Y)    #模型评估    evaluate(unif_train_X,train_Y,unif_test_X,test_Y)</code></pre><h3 id="3-实验效果"><a href="#3-实验效果" class="headerlink" title="3 实验效果"></a>3 实验效果</h3><p>（1）预测的效果图<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/QQ%E6%88%AA%E5%9B%BE20171221153748.jpg" alt="image"><br>（2）效果评估<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/QQ%E6%88%AA%E5%9B%BE20171221154931.jpg" alt="image"></p><h3 id="4-相关知识点"><a href="#4-相关知识点" class="headerlink" title="4 相关知识点"></a>4 相关知识点</h3><p>本个实验中主要涉及到了以下知识点：<br>（1）线性回归<br>（2）pandas，numpy，matplotlib等相关python科学计算库<br>（3）数据的预处理也非常重要，熟练掌握数据预处理方法对以后的工作中或者是在读别人代码时会轻松很多。</p><h3 id="5-相关参考"><a href="#5-相关参考" class="headerlink" title="5 相关参考"></a>5 相关参考</h3><p>（1）主要是参考了老大的代码，然后对代码进行了分块处理。<br>（2）<a href="https://www.cnblogs.com/pinard/p/6016029.html" target="_blank" rel="noopener">用scikit-learn和pandas学习线性回归</a></p><h4 id="ps-红酒数据集分析请看下篇。"><a href="#ps-红酒数据集分析请看下篇。" class="headerlink" title="ps:红酒数据集分析请看下篇。"></a>ps:红酒数据集分析请看下篇。</h4>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 木豆课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++中调用python程序</title>
      <link href="/2017/12/20/%E5%AE%9E%E9%AA%8C%E6%95%99%E7%A8%8B%EF%BC%9AC++%E4%B8%AD%E8%B0%83%E7%94%A8python%E7%A8%8B%E5%BA%8F/"/>
      <url>/2017/12/20/%E5%AE%9E%E9%AA%8C%E6%95%99%E7%A8%8B%EF%BC%9AC++%E4%B8%AD%E8%B0%83%E7%94%A8python%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>因项目需要，需要在c++中调用python文件，于是在网上查询相关资料，终于实现，大概搞了大半天，所以还是记录下这个过程<br><a id="more"></a></p><h3 id="1-相关介绍"><a href="#1-相关介绍" class="headerlink" title="1 相关介绍"></a>1 相关介绍</h3><p>（1）基本情况<br>++在C/C++中嵌入Python，可以使用Python提供的强大功能，通过嵌入Python可以替代动态链接库形式的接口，这样可以方便地根据需要修改脚本代码，而不用重新编译链接二进制的动态链接库。至少你可以把它当成文本形式的动态链接库，需要的时候还可以改一改，只要不改变接口， C++的程序一旦编译好了，再改就没那么方便了。<br>（2）C++调用Python有两种方式<br>第一种方式：通过找到Python模块，类，方法，构造参数来调用。<br>第二中方式，就是通过构造出一个Python的脚本，用python引擎来执行。<br>第一种方式可能更为优雅，符合大多数的反射调用的特点。（如c#的反射机制，c#调用Com+，c#调用javascript脚本等）。<br>一个问题：两种语言互相调用的时候，需要做数据结构（如基本类型，字符串，整数类型等，以及自定义的类等类型）间的转换，共享内存中的一个对象。比如，如何将C++的对象实例传入python中，并在python中使用。c++和python并不在一个进程中，因此可以使用boost的shared_ptr来实现。Python调用C++，换句话说就是需要把C++封装成Python可以“理解”的类型。同理可知C++怎么去调用Python脚本。<br>下面这个例子，主要是演示了c++调用python，可以在c++中形成一个python脚本，然后利用PyRun_SimpleString调用;并且，构造一个c++的对象，传入到python中，并在python的脚本中调用其函数。</p></blockquote><h3 id="2-实验环境"><a href="#2-实验环境" class="headerlink" title="2 实验环境"></a>2 实验环境</h3><p>（1）vs2017<br>具体配置见 <a href="http://blog.csdn.net/pipisorry/article/details/49532341" target="_blank" rel="noopener">vs安装配置</a><br>（2）clion</p><p>配置是修改cmakelist文件，添加：</p><pre><code>include_directories(/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7)set(CMAKE_LIBRARY_PATH &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/&quot;)link_libraries(python)</code></pre><h3 id="3-编写程序"><a href="#3-编写程序" class="headerlink" title="3 编写程序"></a>3 编写程序</h3><p>（1）方式一：，通过构造出一个Python的脚本，用python引擎来执行。<br>①主程序</p><pre><code>#include &quot;stdafx.h&quot;#include&lt;Python.h&gt;//前面所做的一切配置都是为了调用这个头文件和相关库#include&lt;iostream&gt;using namespace std;/**g++ -o callpy callpy.cpp -I/usr/include/python2.6 -L/usr/lib64/python2.6/config -lpython2.6**/int main(int argc, char** argv){    // 初始化Python      //在使用Python系统前，必须使用Py_Initialize对其      //进行初始化。它会载入Python的内建模块并添加系统路      //径到模块搜索路径中。这个函数没有返回值，检查系统      //是否初始化成功需要使用Py_IsInitialized。      Py_Initialize();    // 检查初始化是否成功      if (!Py_IsInitialized()) {        return -1;    }    // 添加当前路径      //把输入的字符串作为Python代码直接运行，返回0      //表示成功，-1表示有错。大多时候错误都是因为字符串      //中有语法错误。      PyRun_SimpleString(&quot;import sys&quot;);    PyRun_SimpleString(&quot;print(&#39;---import sys---&#39;)&quot;);    PyRun_SimpleString(&quot;sys.path.append(&#39;./&#39;)&quot;);    PyRun_SimpleString(&quot;import pytest&quot;);    PyRun_SimpleString(&quot;pytest.add()&quot;);    Py_Finalize();        system(&quot;pause&quot;);    return 0;}</code></pre><p>②python程序：pytest.py</p><pre><code>#test function  def add():      print(&quot;hello world&quot;)</code></pre><p>运行结果：</p><pre><code>---import sys---hello world请按任意键继续. . .</code></pre><p>（2）方式二：通过找到Python模块，类，方法，构造参数来调用。<br>这中方法我的电脑上没实验成功，我同学实验成功了。主要参照如下教程的代码：<br>①<a href="https://www.cnblogs.com/findumars/p/6142330.html" target="_blank" rel="noopener">浅析 C++ 调用 Python 模块</a><br>② <a href="http://blog.csdn.net/pipisorry/article/details/49532341" target="_blank" rel="noopener">C++调用python</a><br>③<a href="https://www.cnblogs.com/apexchu/p/5015961.html" target="_blank" rel="noopener">Python实例浅谈之三Python与C/C++相互调用</a></p><h3 id="4-相关注意"><a href="#4-相关注意" class="headerlink" title="4 相关注意"></a>4 相关注意</h3><p>（1）pystring_fromstring没有定义<br>造成这个问题的原因是：python3+中没有这个函数，所以这个实验只能在python2+的环境下进行。</p><h3 id="5-相关参考"><a href="#5-相关参考" class="headerlink" title="5 相关参考"></a>5 相关参考</h3><p>（1）<a href="https://www.cnblogs.com/findumars/p/6142330.html" target="_blank" rel="noopener">浅析 C++ 调用 Python 模块</a><br>（2） <a href="http://blog.csdn.net/pipisorry/article/details/49532341" target="_blank" rel="noopener">C++调用python</a><br>（3）<a href="https://www.cnblogs.com/apexchu/p/5015961.html" target="_blank" rel="noopener">Python实例浅谈之三Python与C/C++相互调用</a></p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
            <tag> C++ </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【自然语言处理入门】02：Kenlm语料库的制作与模型的训练</title>
      <link href="/2017/12/18/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9102%EF%BC%9AKenlm%E8%AF%AD%E6%96%99%E5%BA%93%E7%9A%84%E5%88%B6%E4%BD%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83/"/>
      <url>/2017/12/18/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9102%EF%BC%9AKenlm%E8%AF%AD%E6%96%99%E5%BA%93%E7%9A%84%E5%88%B6%E4%BD%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文是《从自然语言处理到机器学习入门》系列课程的第二次作业，由于我的作业环境没有配好（配了n次了还是不行T_T），但是为了保证这一系列作业的完整性，于是经罗曜强律师同意，<a href="www.aqinet.cn">人工智能A7论坛</a>授权，转载他的作业笔记。<br><a id="more"></a></p><h3 id="1-基本要求"><a href="#1-基本要求" class="headerlink" title="1 基本要求"></a>1 基本要求</h3><p>通过自己训练的语言模型编程，判断每句话中是否存在a an用错的问题(所谓用错 指a an用反了 比如 i have a apple是错误的； i have an apple  是正确的)</p><h3 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2 准备工作"></a>2 准备工作</h3></blockquote><p>（1）实验的环境Ubuntu16.04，Python 版本 2.7</p><p>（2）使用kenlm训练一个语言模型，首先要准备kenlm所需要的语料，按照<a href="http://kheafield.com/code/kenlm/官方文档上使用说明，训练的文件会被训练成.arpa的格式。" target="_blank" rel="noopener">http://kheafield.com/code/kenlm/官方文档上使用说明，训练的文件会被训练成.arpa的格式。</a></p><p>（3）训练模型：例如:我有名为test.txt的文件需要训练成kenlm指定的.arpa格式文件,训练后的文件为text.arpa，我需要在Ubuntu的Teminal终端使用如下命令进行训练：</p><pre><code>bin/lmplz -o 5 &lt;test.txt  &gt; text.arpa</code></pre><p>-o   Required. Order of the language model to estimate<br>-o 5 代表使用5ngram</p><p>将arpa文件转换为binary文件，这样可以对arpa文件进行压缩，提高后续在python中加载的速度。</p><pre><code>bin/build_binary -s text.arpa text.bin</code></pre><h3 id="3-具体实验"><a href="#3-具体实验" class="headerlink" title="3 具体实验"></a>3 具体实验</h3><p>做好上述前置准备工作后，接着就是在Python下运行text.arpa<br>主要分为以下几个步骤：</p><pre><code>#导入训练所需要的包import kenlmimport nltkfrom itertools import combinations, permutations#将文件导入kenlm语言模型model = kenlm.LanuageModel(text.bin)#判断a或者an在互换前的得分和互换后的得分，如果互换前的得分高于互换后的得分，则说明a或an没有错误，如果互换后的得分高于互换前的得分则说明a或者an语法错误def judge_a_or_an(sentence):#创建一个空list，用于存放sentences = [] #将句子进行分词“””Model.score函数输出的是对数概率，bos=False, eos=False意思是不自动添加句首和句末标记符“””pre_score = model.score(‘ ’.join(sentence), bos = True,  eos = True)#通过循环的方式替换a或an然后进行评分对比for word in sentence:#如果word里面有a，则把a换成anIf word == ‘a’:s.append(‘an’)#如果word里面有an，则把an换成aelif word == ‘an’:s.append(‘a’)#如果word里面没有a或者an，按照原句输出else:s.append(word)after_score = model.score(‘ ’.join(s), bos = True, eos = True)#对话置换前，置换后的得分，如果置换前得分高于置换后，则返回0，否则返回1if pre_score &gt; after_scorereturn 0else:return 1#打开文件inputs = open(‘text.arpa’, ’r’)outputs = open(‘text_after.txt’, ‘r’)for line in inputs:data = nltk.tokenize.word_tokenize(line)#调用judge_a_or_an函数label = judge_a_or_an(data)#格式化输出0或1print(line + ‘\t%d’ %(label))#关闭IO流outputs.close()inputs.close()</code></pre><h3 id="4-常见错误"><a href="#4-常见错误" class="headerlink" title="4 常见错误"></a>4 常见错误</h3><p>（1）ModuleNotFoundError: No module named ‘kenlm’<br>Kenlm安装错误，导致无法正常调用kenlm、<br>（2）打开文件后未关闭IO流致使文件无法正常输出<br>（3）其他语法错误</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 木豆课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scrapy-02：创建工程项目的简单步骤</title>
      <link href="/2017/12/16/scrapy-02%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4/"/>
      <url>/2017/12/16/scrapy-02%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4/</url>
      
        <content type="html"><![CDATA[<p>前面看了一个爬虫的小例子，那么接下来就；来看看创建一个爬虫的步骤有哪些。<br><a id="more"></a></p><h3 id="1-创建工程项目的简单步骤"><a href="#1-创建工程项目的简单步骤" class="headerlink" title="1 创建工程项目的简单步骤"></a>1 创建工程项目的简单步骤</h3><p>（1）开始一个爬虫项目<br>进入一个你要创建爬虫的目录下面，在命令行窗口中执行如下代码：</p><pre><code>scrapy startproject 项目名</code></pre><p>其中项目名改成你将要创建的项目的名称<br>项目创建好了以后，进入该项目，可以看到如下的目录结构（其中tutorial是创建的项目名）<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/QQ%E5%9B%BE%E7%89%8720171216191210.png" alt="image"><br>（2）创建一个spider文件模板<br>首先进入项目文件里，在项目文件里用命令行执行</p><pre><code>scrapy genspider abc_spider www.baidu.com</code></pre><p>命令行的第三个串是要创建的spider的名字，第四个是要爬取网页的链接地址。<br>这时，进入项目文件下的spider目录里面，就会发现一个abc_spider.py文件，这个就是创建的文件模板，用IDE打开，修改其内容。</p><pre><code># -*- coding: utf-8 -*-import scrapyclass AbcSpiderSpider(scrapy.Spider):    name = &#39;abc_spider&#39;    allowed_domains = [&#39;www.baidu.com&#39;]    start_urls = [&#39;http://www.baidu.com/&#39;]    def parse(self, response):        #定义文件名        filename=response.url.split(&#39;/&#39;)[-2]+&quot;html&quot;        #打开文件并写入源代码        with open(filename,&quot;wb&quot;) as fp:            fp.write(response.body)</code></pre><p>（3）运行爬虫</p><pre><code>scrapy crawl 爬虫名</code></pre><p>于是就能看到一个html的文件。</p><h3 id="2-自定义抓取项"><a href="#2-自定义抓取项" class="headerlink" title="2 自定义抓取项"></a>2 自定义抓取项</h3><p>（1）打开项目中的item.py文件，按照说明进行改写。</p><pre><code>class FirstspiderItem(scrapy.Item):    # define the fields for your item here like:    title = scrapy.Field()    desc = scrapy.Field()    link=scrapy.Field()</code></pre><p>（2）改写parse函数</p><pre><code># -*- coding: utf-8 -*-import scrapyfrom firstspider.items import FirstspiderItemclass AbcSpiderSpider(scrapy.Spider):    name = &#39;abc_spider&#39;    allowed_domains = [&#39;www.baidu.com&#39;]    start_urls = [&#39;http://www.baidu.com/&#39;]    def parse(self, response):        # #定义文件名        # filename=response.url.split(&#39;/&#39;)[-2]+&quot;html&quot;        # #打开文件并写入源代码        # with open(filename,&quot;wb&quot;) as fp:        #     fp.write(response.body)        #通过xpath获取要抓取的内容        lis=response.xpath(&#39;//*[@id=&quot;head&quot;]&#39;)        for li in lis:            #在这之前需要引入            item=FirstspiderItem()            #xpath百度首页不好举例            item[&#39;title&#39;]=li.xpath(&#39;xpath路径&#39;).extract()            item[&#39;link&#39;]=li.xpath(&#39;xpath路径&#39;).extract()            item[&#39;desc&#39;] = li.xpath(&#39;xpath路径&#39;).extract()            #返回item            yield item</code></pre><p>（3）改写pipeline文件<br>这里不做修改，只是返回item即可。</p><pre><code>class FirstspiderPipeline(object):    def process_item(self, item, spider):        return item</code></pre><p>（4）运行爬虫<br>可以通过</p><pre><code>scrapy list</code></pre><p>来获取项目里面所包含的爬虫，然后运行你要执行的爬虫。抓取得结果一般情况下是存放在数据库中。</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scrapy </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scrapy-01：简单实列</title>
      <link href="/2017/12/16/scrapy-01%EF%BC%9A%E7%AE%80%E5%8D%95%E5%AE%9E%E5%88%97/"/>
      <url>/2017/12/16/scrapy-01%EF%BC%9A%E7%AE%80%E5%8D%95%E5%AE%9E%E5%88%97/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近开始学习自然语言处理，在自然语言处理的过程中，获取语料，处理语料都是非常重要的一部，因此在开始入门学习自然语言处理的同时，也开始练习一些爬虫，暑假时学过一点，但是也忘得差不多了，于是重新开始，并记录下来、<br><a id="more"></a></p></blockquote><h4 id="1-scrapy库介绍"><a href="#1-scrapy库介绍" class="headerlink" title="1  scrapy库介绍"></a>1  scrapy库介绍</h4><p>（1）具体的介绍可以自行百度</p><p>（2）安装方法：</p><pre><code>#利用pip安装pip install scrapy#利用conda安装conda install scrapy</code></pre><h4 id="2-scrapy的简单使用"><a href="#2-scrapy的简单使用" class="headerlink" title="2 scrapy的简单使用"></a>2 scrapy的简单使用</h4><p>（1）代码实例</p><pre><code># coding: utf-8import scrapyfrom w3lib.html import remove_tags#定义一个类，继承了scrapy，spiderclass StackOvverflowSpider(scrapy.Spider):   #爬虫项目的名字，在整个项目中，这个名字需要唯一   name=&quot;stackoverflow&quot;   #指定爬虫开始的网址链接   start_urls=[&quot;http://stackoverflow.com/questions?sort=votes&quot;]   #爬虫项目的回调函数   def parse(self,response):       #通过css样式来获取需要遍历的链接，       for href in response.css(&quot;.question-summary h3 a::attr(href)&quot;):           #组装成完整的链接           full_url=response.urljoin(href.extract())           #对每一个完整的链接，传给回调函数，在回调函数中抓取页面内容           yield scrapy.Request(full_url,callback=self.parse_question)   def parse_question(self,response):       yield{           #指定要抓取的内容，title是抓取结果中保存的字段，后面对应的是他的页面上的内容，后面的css中的类名，需要事先打开网页源代码查看。           &#39;title&#39;:response.css(&#39;h1 a::text&#39;).extract()[0],           &#39;votes&#39;:response.css(&#39;.question .vote-count-post::text&#39;).extract()[0],           &#39;body&#39;:response.css(&#39;.question .post-text&#39;).extract()[0]       }</code></pre><p>（2）运行代码</p><pre><code>scrapy runspider  scrapy_1.py -o abc.csv</code></pre><p>（3）代码讲解：见代码中的注释</p>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scrapy </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【笔记】01：《统计学习方法》-感知机模型学习笔记</title>
      <link href="/2017/12/13/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2017/12/13/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><a id="more"></a><strong>感知机模型描述</strong><br>:   这一部分主要介绍感知机是什么？</p><blockquote><p>假设输入空间（输入空间即为特征空间，由实例的各种特征组成）是χ⊆R*，输出空间是Y={-1,+1}。当输入x∈χ表示实例的特征向量，对应于输入空间（特征向量）的点，输出y∈Y表示实例的类别。由输入空间到输出空间的如下函数：</p><script type="math/tex; mode=display">f(x) = sign(w·x+b)</script><p>其中，w和b为感知机模型，w∈R*叫做权值或权值向量（在多数情况下，w是一个列向量），b∈R叫做偏置。</p></blockquote><p><strong><em>Tips：</em></strong><br>（1）w·x表示w和x的内积，若w=（w1，w2，w3），x=（x1，x2，x3），则</p><pre><code>$$w·x=∑wi·xi=w1·x1+w2·x2+w3·x3$$</code></pre><p>（2）sign(x)是一个符号函数，它根据x的输出1或-1</p><blockquote><p>感知机是一种线性分类模型，属于判别模型，其几何解释是线性方程<script type="math/tex">w·x+b=0</script>对应于特征空间的一个超平面S（二维情况下，该线性方程退化为一条直线），其中，w是超平面的法向量，b是平面的截距（如果是二维情况下，b就是在坐标轴上的截距），这个超平面将特征空间分为两部分，位于两部分的点分别被分为正负两类。因此，超平面称之为分离超平面（二维情况下是分割线）。<br>    <img src="http://img.blog.csdn.net/20170916103921608?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHl6MTU4NDE3MjgwOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="感知机模型"></p></blockquote><p><strong>感知机学习策略</strong><br>:   这一部分主要介绍如何衡量模型中的参数的好坏。<br>感知机学习的目标是求得一个能够将训练集正实例和负实例完全正确分开的分离超平面，为在这个寻找超平面的过程中（即确定感知机模型参数w，b），就需要衡量每次得到的模型参数的好坏，那么如何衡量呢？而通常使用损失函数来度量预测值与真实值之间的差距。于是我们在求最好的模型参数就相当于求极小化的损失函数。<br>再想想感知机的学习目标（将正实例和负实例完全正确分开），如果把分错了类型的点（误分类点）的多少作为衡量呢？效果会怎么样？这样的话，如果模型参数差，那么分错的点就很多，如果模型参数好的话，分错的点就很少，貌似有点道理。但是把误分类点的总数作为损失函数的话，此时的损失函数不是参数w，b的连续可导函数，所以不能直接使用误分类点总数。有没有办法将误分类总数转化为其他的衡量方式呢？于是想到将误分类点到超平面的S的总距离作为损失函数，而就是感知机的学习策略。</p><p>首先，输入空间R*中任意一点x0到超平面S的距离可以表示成：</p><script type="math/tex; mode=display">d= \dfrac{1}{||w||} |w·x0+b|$$**Tips:**（1）对这个公式的理解可以来源于中学的点到平面的距离公式，$$d = \dfrac{|Ax0+By0+Cz0+D|}{\sqrt{A^2+B^2+C^2 }} $$公式中的平面方程为Ax+By+Cz+D=0，点P的坐标(x0,y0,z0)，d为点P到平面的距离。（2）||w||是w的l2范数，即：$$||w||={\sqrt{w1^2+w2^2+...+wn^2}}$$这个形式看着像传统点到平面公式的分母，实际上，它就是距离公式中的分母，在传统的公式中，分母是各个变量前面系数的平方和开根号，而现在对于超平面：w·x+b=0，变量前面的系数是w，且w是个向量，这就是公式中||w||的由来。同理，公式中的b相当于传统公式中的D。 接下来我们来想想如何打开绝对值得问题，对于误分类的数据（xi,yi)来说,$$-yi(w·xi+b)>0$$总是成立的，因为当w·xi+b>0时，yi=-1；当w·xi+b<0时，yi=1。故误分类点到超平面的距离是$$-\dfrac{1}{||w||}·yi· （w·xi+b）</script><p> 假设误分类点的集合为M，则所有误分类点到超平面S的总距离为<script type="math/tex">-\dfrac{1}{||w||}∑yi· （w·xi+b）</script>其中，xi∈M。若不考虑$\dfrac{1}{||w||}$，就得到来感知机学习的损失函数：<script type="math/tex">L(w,b)=-∑yi· （w·xi+b）</script><br>其中，xi∈M。</p><p><strong>感知机学习算法</strong><br>:   这一部分主要介绍如何找到最好的参数，主要有原始形式的算法和对偶形式的算法。<br><strong>原始形式</strong><br><img src="http://img.blog.csdn.net/20170916151514384?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHl6MTU4NDE3MjgwOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><strong>对偶形式</strong>(未完待续)</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 统计学习方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【自然语言处理入门】01：利用jieba对数据集进行分词，并统计词频</title>
      <link href="/2017/12/13/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9101%EF%BC%9A%E5%88%A9%E7%94%A8jieba%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E8%AF%8D%EF%BC%8C%E5%B9%B6%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91/"/>
      <url>/2017/12/13/%E3%80%90%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%9101%EF%BC%9A%E5%88%A9%E7%94%A8jieba%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E8%AF%8D%EF%BC%8C%E5%B9%B6%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91/</url>
      
        <content type="html"><![CDATA[<h3 id="一、基本要求"><a href="#一、基本要求" class="headerlink" title="一、基本要求"></a>一、基本要求</h3><p>使用jieba对垃圾短信数据集进行分词，然后统计其中的单词出现的个数，找到出现频次最高的top100个词。<br><a id="more"></a></p><h3 id="二、完整代码"><a href="#二、完整代码" class="headerlink" title="二、完整代码"></a>二、完整代码</h3><pre class=" language-lang-python"><code class="language-lang-python"># -*- coding: UTF-8 -*-from collections import Counterimport jieba.analyseimport reimport time#分词模板def cut_word(datapath):    with open(datapath, 'r',encoding='utf-8') as fr:        string=fr.read()        print(type(string))        #对文件中的非法字符进行过滤        data=re.sub(r"[\s+\.\!\/_,$%^*(【】：\]\[\-:;+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[0-9]+","",string)        word_list= jieba.cut(data)        print(word_list)        return word_list#词频统计模块def statistic_top_word(word_list,top=100):    #统计每个单词出现的次数，别将结果转化为键值对（即字典）    result= dict(Counter(word_list))    print(result)    #sorted对可迭代对象进行排序    #items()方法将字典的元素转化为了元组，而这里key参数对应的lambda表达式的意思则是选取元组中的第二个元素作为比较参数    #排序厚的结果是一个列表，列表中的每个元素是一个将原字典中的键值对转化为的元祖    sortlist=sorted(result.items(),key=lambda item:item[1],reverse=True)    resultlist=[]    for i in range(0,top):        resultlist.append(sortlist[i])    return resultlist#主函数def main():    #设置数据集地址    datapath='F:\\python3\\nlp\\data\\spam.txt'    #对文本进行分词    word_list=cut_word(datapath)    #统计文本中的词频    statistic_result=statistic_top_word(word_list,100)    #输出统计结果    print(statistic_result)if __name__ == "__main__":    main()</code></pre><h3 id="三、相关知识点"><a href="#三、相关知识点" class="headerlink" title="三、相关知识点"></a>三、相关知识点</h3><ul><li>1、jieba分词：三种模式，详见<a href="http://www.jianshu.com/p/c434be968dee" target="_blank" rel="noopener">相关介绍</a></li><li>2、对字典进行排序：字典可以实现对键和值分别排序。详见<a href="http://blog.csdn.net/tangtanghao511/article/details/47810729" target="_blank" rel="noopener">原文链接</a></li><li>3、python 过滤中文、英文标点特殊符号：在进行分词前，主要是利用正则表达式对欲分词文本进行过滤，利用re.sub（）函数对“非法”字符进行空字符替换。详见<a href="http://blog.csdn.net/mach_learn/article/details/41744487" target="_blank" rel="noopener">原文链接</a></li></ul><h3 id="四、相关参考"><a href="#四、相关参考" class="headerlink" title="四、相关参考"></a>四、相关参考</h3><ul><li>1、<a href="http://www.linuxyw.com/810.html" target="_blank" rel="noopener">python数据分析：jieba模块对数据进行切词并统计出现每个词的次数</a></li><li>2、<a href="http://blog.csdn.net/tangtanghao511/article/details/47810729" target="_blank" rel="noopener">python的sorted函数对字典按key排序和按value排序</a></li><li>3、<a href="http://blog.csdn.net/mach_learn/article/details/41744487" target="_blank" rel="noopener">python 过滤中文、英文标点特殊符号</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 木豆课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>visdom无法正常使用、只有蓝屏</title>
      <link href="/0201/12/09/visdom%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8%E3%80%81%E5%8F%AA%E6%9C%89%E8%93%9D%E5%B1%8F/"/>
      <url>/0201/12/09/visdom%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8%E3%80%81%E5%8F%AA%E6%9C%89%E8%93%9D%E5%B1%8F/</url>
      
        <content type="html"><![CDATA[<p>如果出现蓝屏情况，说明服务是能够正常启动的，只是前端页面显示有问题，这些前端页面一般要翻墙才能下载，所以只要下载这些文件放进去就可以了。<a href="https://download.csdn.net/download/xyz1584172808/10862649" target="_blank" rel="noopener">点我下载文件</a><br>正常运行结果：<br><img src="https://img-blog.csdnimg.cn/20181219213055377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="运行结果"></p><p>下载后直接替换static文件即可，static目录下的文件情况如下：<br><img src="https://img-blog.csdnimg.cn/20181219212703621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="static目录下的文件情况"></p><p>css目录下的文件情况如下：<br><img src="https://img-blog.csdnimg.cn/20181219212804378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="css目录下的文件情况"><br>fonts目录下文件情况如下：<br><img src="https://img-blog.csdnimg.cn/20181219212902376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="fonts目录下文件情况"><br>js目录下的文件情况如下：<br><img src="https://img-blog.csdnimg.cn/20181219212952530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5ejE1ODQxNzI4MDg=,size_16,color_FFFFFF,t_70" alt="js目录下的文件情况"></p><h3 id="点我下载文件"><a href="#点我下载文件" class="headerlink" title="点我下载文件"></a><a href="https://download.csdn.net/download/xyz1584172808/10862649" target="_blank" rel="noopener">点我下载文件</a></h3>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 实验记录 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    

    
    <title>tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子） | 一只NLPer渣渣的被虐日记</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content="笔记,Tensorflow" />
    
    <meta name="description" content="注：由于tensorflow版本的不同，这个函数所在的模块可能不同，如：tf.nn.seq2seq.sequence_loss_by_example和tf.contrib.legacy_seq2seq.sequence_loss_by_example  在正式进入sequence_loss_by_example（）函数的计算过程之前，需要先复习下两个基本的知识点，softmax的计算和交叉熵的计">
<meta name="keywords" content="笔记,Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）">
<meta property="og:url" content="http://yoursite.com/2018/10/15/tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）/index.html">
<meta property="og:site_name" content="一只NLPer渣渣的被虐日记">
<meta property="og:description" content="注：由于tensorflow版本的不同，这个函数所在的模块可能不同，如：tf.nn.seq2seq.sequence_loss_by_example和tf.contrib.legacy_seq2seq.sequence_loss_by_example  在正式进入sequence_loss_by_example（）函数的计算过程之前，需要先复习下两个基本的知识点，softmax的计算和交叉熵的计">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/1.png">
<meta property="og:updated_time" content="2018-10-15T12:30:37.798Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）">
<meta name="twitter:description" content="注：由于tensorflow版本的不同，这个函数所在的模块可能不同，如：tf.nn.seq2seq.sequence_loss_by_example和tf.contrib.legacy_seq2seq.sequence_loss_by_example  在正式进入sequence_loss_by_example（）函数的计算过程之前，需要先复习下两个基本的知识点，softmax的计算和交叉熵的计">
<meta name="twitter:image" content="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/1.png">
    

    

    
        <link rel="icon" href="/css/image/icon.png" />
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    


</head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">好好学习，天天被虐</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C/">C/C++</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C/leetcode/">leetcode</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Experiment/">Experiment</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Experiment/C/">C++</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Experiment/PHP/">PHP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Experiment/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Experiment/Scrapy/">Scrapy</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/nlp/">nlp</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Material/">Material</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Material/DL/">DL</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Material/ML/">ML</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/Classification/">Classification</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/MDCourse/">MDCourse</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/NLTK/">NLTK</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/Text-similarity/">Text similarity</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NOTES/">NOTES</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NOTES/Chaotic/">Chaotic</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NOTES/Keras/">Keras</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NOTES/Papers/">Papers</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NOTES/Statistical/">Statistical</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NOTES/Tensorflow/">Tensorflow</a></li></ul></li></ul>
                                    
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>
        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NOTES/">NOTES</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/NOTES/Tensorflow/">Tensorflow</a>
    </h1>
</div>
                        <div class="main-body-content">
                            <article id="post-tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2018/10/15/tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）/" class="article-date">
            <time datetime="2018-10-15T12:25:43.000Z" itemprop="datePublished">2018-10-15</time>
        </a>
    </div>

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/Tensorflow/">Tensorflow</a>, <a class="tag-link" href="/tags/笔记/">笔记</a>
    </div>

				<span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span>
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <blockquote>
<p>注：由于tensorflow版本的不同，这个函数所在的模块可能不同，如：tf.nn.seq2seq.sequence_loss_by_example和tf.contrib.legacy_seq2seq.sequence_loss_by_example</p>
</blockquote>
<p>在正式进入sequence_loss_by_example（）函数的计算过程之前，需要先复习下两个基本的知识点，softmax的计算和交叉熵的计算。</p>
<h3 id="1-softmax的计算过程"><a href="#1-softmax的计算过程" class="headerlink" title="1  softmax的计算过程"></a>1  softmax的计算过程</h3><p>可以直接网上已经写好的博客：<a href="https://blog.csdn.net/red_stone1/article/details/80687921" target="_blank" rel="noopener">三分钟带你对 Softmax 划重点</a>，这篇文章中有举具体的例子，最好自己动手算一下，不自己动手计算，往往看了就忘了。</p>
<h3 id="2-交叉熵的计算过程"><a href="#2-交叉熵的计算过程" class="headerlink" title="2  交叉熵的计算过程"></a>2  交叉熵的计算过程</h3><p>交叉熵网上的文章也很多，<a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="noopener">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉</a>这篇文章讲得非常详细，还举了各种例子。</p>
<p>以上复习了softmax和交叉熵的计算过程，为啥要使用softmax和交叉熵，可以自行网上搜搜。接下来就进入sequence_loss_by_example（）函数的计算过程。</p>
<h3 id="3-sequence-loss-by-example（）函数的计算过程（以TF的ptb构建语言模型例子为例）"><a href="#3-sequence-loss-by-example（）函数的计算过程（以TF的ptb构建语言模型例子为例）" class="headerlink" title="3  sequence_loss_by_example（）函数的计算过程（以TF的ptb构建语言模型例子为例）"></a>3  sequence_loss_by_example（）函数的计算过程（以TF的ptb构建语言模型例子为例）</h3><blockquote>
<p>注：例子中的batch_size=20，num_steps=20，为了更直观的查看各个数据的维度，我将num_steps改为了15.（因为本例是通过上一个词预测下一个词，其实num_steps改为多少并没有影响）。</p>
</blockquote>
<h4 id="（1）LSTM的输出"><a href="#（1）LSTM的输出" class="headerlink" title="（1）LSTM的输出"></a>（1）LSTM的输出</h4><p>LSTM的隐藏层的单元个数为200，因此，LSTM每一步的输出数据的维度为（batch_size,hidden_size）。有因为LSTM展开的时间步数为num_steps，于是通过<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs.append(cell_output)</span><br></pre></td></tr></table></figure></p>
<p>将每一时刻的输出都收集起来，这样，最后的outputs是一个list，其样式为：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/1.png" alt="LSTM的全部输出"><br>图中黄色的部分表示同一个序列在LSTM不同时刻的输出。<br>紧接着对outputs进行拼接和reshape，其过程如下图：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/2.png" alt="拼接输出"><br>将每一时刻的输出在第1维上拼接（上图），这样每一行就完整的表示了一个序列。reshape后的结构如下图：<br><img src="https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/3.png" alt="outputs的reshape结果"><br>其中每一种颜色表示一个序列，同一种颜色中的各个块表示这个序列的不同时刻。<br>以上就是LSTM的输出，并对其适当变形。接下来通过一个全连接层，将每一时刻的输出映射成字典大小。</p>
<h4 id="（2）通过全连接层"><a href="#（2）通过全连接层" class="headerlink" title="（2）通过全连接层"></a>（2）通过全连接层</h4><p>这部分就是常见的y=wx+b的构造形式，通过以下代码实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">softmax_w = tf.get_variable( &quot;softmax_w&quot;, [size, vocab_size], dtype=tf.float32)</span><br><span class="line">softmax_b = tf.get_variable(&quot;softmax_b&quot;, [vocab_size], dtype=tf.float32)</span><br><span class="line"> # 网络的最后输出(相当于最后添加了一个全连接层)</span><br><span class="line">logits = tf.matmul(output, softmax_w) + softmax_b   # logits shape:batch_size*num_step,vocab_size</span><br></pre></td></tr></table></figure></p>
<p>通过全连接层后，得到logits，其维度为（batch_size<em>num_step,vocab_size），在本例中就是300</em>10000（本例的词汇表大小是10000）。</p>
<h4 id="（3）执行tf-contrib-legacy-seq2seq-sequence-loss-by-example函数"><a href="#（3）执行tf-contrib-legacy-seq2seq-sequence-loss-by-example函数" class="headerlink" title="（3）执行tf.contrib.legacy_seq2seq.sequence_loss_by_example函数"></a>（3）执行tf.contrib.legacy_seq2seq.sequence_loss_by_example函数</h4><p>关于这个函数的定义，解释之类的，可以参考<a href="https://blog.csdn.net/appleml/article/details/54017873" target="_blank" rel="noopener">这个解释</a>，小例子可以参考<a href="https://blog.csdn.net/UESTC_C2_403/article/details/72792889" target="_blank" rel="noopener">这个</a>。前一篇博客的解释看得有些稀里糊涂的，后面就找了后面那个例子来跑跑，但是这两个都没有讲清楚内部是怎么计算的，后面我又参考了tensorflow的<a href="https://blog.csdn.net/u012436149/article/details/52874718" target="_blank" rel="noopener">损失函数源代码</a>，找到这个函数，可以看到这个函数在内部调用的是sparse_softmax_cross_entropy_with_logits()函数，好嘛，接下来就一步一步的来看整个计算过程。<br><strong>首先</strong>来看输入的数据： logits和targets</p>
<ul>
<li>logits数据的格式在前面已经介绍了，为（300<em>10000）的矩阵，300为num_steps</em>batch_size得到，10000为词汇表大小，这个数据表示的意思是：每一行表示一个时刻（对应一个预测的单词），每num_steps行对应一个序列，一共有batch_size个num_steps行（因为这是一个batch大小的数据）。</li>
<li>输出targets的维度可以看到其形状为（batch_size,num_steps）（即20<em>15），表示的意思是，一个batch中有20条数据，而每一条数据有15个时间步，一个时间步对应一个单词。为了让预测的单词的顺序和targets中真实单词的顺序对应上，于是将targets的维度变成了（300，）（即：20</em>15），这样一个元素对应logits中的一行，每15个数据就表示一个序列。</li>
</ul>
<p><strong>接下来</strong>，就将上面的输入以及权重w（通常设置为1）传入sequence_loss_by_example函数。下面是这个函数的实现代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def sequence_loss_by_example(logits, targets, weights,</span><br><span class="line">                             average_across_timesteps=True,</span><br><span class="line">                             softmax_loss_function=None, name=None):</span><br><span class="line">#logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</span><br><span class="line">#targets: List of 1D batch-sized int32 Tensors of the same length as logits.</span><br><span class="line">#weights: List of 1D batch-sized float-Tensors of the same length as logits.</span><br><span class="line">#return:log_pers 形状是 [batch_size].</span><br><span class="line">   for logit, target, weight in zip(logits, targets, weights):</span><br><span class="line">      if softmax_loss_function is None:</span><br><span class="line">        # TODO(irving,ebrevdo): This reshape is needed because</span><br><span class="line">        # sequence_loss_by_example is called with scalars sometimes, which</span><br><span class="line">        # violates our general scalar strictness policy.</span><br><span class="line">        target = array_ops.reshape(target, [-1])</span><br><span class="line">        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">            logit, target)</span><br><span class="line">      else:</span><br><span class="line">        crossent = softmax_loss_function(logit, target)</span><br><span class="line">      log_perp_list.append(crossent * weight)</span><br><span class="line">    log_perps = math_ops.add_n(log_perp_list)</span><br><span class="line">    if average_across_timesteps:</span><br><span class="line">      total_size = math_ops.add_n(weights) </span><br><span class="line">      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.</span><br><span class="line">      log_perps /= total_size</span><br><span class="line">  return log_perps</span><br></pre></td></tr></table></figure>
<p>可以看到函数内部主要是调用sparse_softmax_cross_entropy_with_logits函数，然后再加权平均后返回。<br>那么这个sparse_softmax_cross_entropy_with_logits函数是怎么计算的呢？这里有个<a href="https://blog.csdn.net/u012193416/article/details/77918732" target="_blank" rel="noopener">小例子</a>，可以看到他是将softmax和cross_entropy放在一起计算，这里就涉及到文章开头所复习得softmax和交叉熵的计算过程了。<br><strong>于是</strong>，我们知道了，针对于一个logits元素和一个targets元素，比如这个例子中取logits[0]，其维度为（10000，），targets[0]，它就是单独的一个整型的数，表示单词在词汇表中的id号。<strong>先计算logits[0]中各个元素的相对概率（即计算softmax），然后利用交叉熵公式计算预测值和真实值之间的交叉熵。</strong></p>
<blockquote>
<p>这个函数直接使用标签数据的，而不是采用one-hot编码形式（另一个函数softmax_cross_entropy_with_logits必须是one-hot形式的数据，具体见<a href="https://blog.csdn.net/zchang81/article/details/70225220" target="_blank" rel="noopener">softmax_cross_entropy_with_logits函数详解</a>）</p>
</blockquote>
<p>为了验证这个计算过程，我将这个例子中的logits[0]，targets[0]以及通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(    # 这个函数先求解softmax，再求解交叉熵</span><br><span class="line">        [logits],</span><br><span class="line">        [tf.reshape(input_.targets, [-1])],</span><br><span class="line">        [tf.ones([batch_size * num_steps], dtype=tf.float32)])</span><br></pre></td></tr></table></figure>
<p>得到的loss（这里取loss[0]）取出，单独通过tf.nn.softmax以及交叉熵公式来依存计算这个过程。<br>首先来看下通过例子源代码取出的数据情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">logits shape:</span><br><span class="line">(10000,)</span><br><span class="line">logits value:</span><br><span class="line">[ 7.8470936   8.238499    8.979608   ... -0.73421586 -0.913356</span><br><span class="line"> -0.7552418 ]</span><br><span class="line">target value:</span><br><span class="line">9971</span><br><span class="line">loss value:</span><br><span class="line">11.060587</span><br></pre></td></tr></table></figure>
<p>自己计算的代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># 将单词id转化为one-hot模式</span><br><span class="line">target=list([0]*10000)</span><br><span class="line">target[9971]=1</span><br><span class="line">target=np.array(target)</span><br><span class="line"># 转化为张量</span><br><span class="line">logits=tf.convert_to_tensor(r[&apos;logits&apos;],dtype=float)</span><br><span class="line">y_=tf.convert_to_tensor(target,dtype=float)</span><br><span class="line"></span><br><span class="line"># 法1，分开计算----------------------------------------------------</span><br><span class="line"># 计算softmax</span><br><span class="line">y=tf.nn.softmax(logits)</span><br><span class="line"># 计算交叉熵</span><br><span class="line">cross_entropy=-tf.reduce_sum(y_*tf.log(y))</span><br><span class="line"># 法2，调用函数计算-------------------------------------------------</span><br><span class="line"># 调用函数计算</span><br><span class="line">cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    softmax=sess.run(y)</span><br><span class="line">    c_e=sess.run(cross_entropy)</span><br><span class="line">    c_e2=sess.run(cross_entropy2)</span><br><span class="line">    print(&quot;softmax:\n&quot;,softmax)</span><br><span class="line">    print(&quot;cross_entropy:\n&quot;,c_e)</span><br><span class="line">    print(&quot;function:\n&quot;,c_e2)</span><br></pre></td></tr></table></figure>
<p>运行结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">softmax:</span><br><span class="line"> [5.8356632e-02 8.6312823e-02 1.8110682e-01 ... 1.0946491e-05 9.1511438e-06</span><br><span class="line"> 1.0718737e-05]</span><br><span class="line">cross_entropy:</span><br><span class="line"> 11.060587</span><br><span class="line">function:</span><br><span class="line"> 11.060587</span><br></pre></td></tr></table></figure>
<p>可以看到，不管是分开计算还是调用函数计算，其结果和例子源代码中得到的结果相同。这就是sparse_softmax_cross_entropy_with_logits函数的计算过程，tf.contrib.legacy_seq2seq.sequence_loss_by_example函数的计算过程就是在其内部的每个时间步中调用sparse_softmax_cross_entropy_with_logits函数即可。</p>
<h3 id="4-小结"><a href="#4-小结" class="headerlink" title="4  小结"></a>4  小结</h3><p>本文通过tensorflow官方提供的基于LSTM的语言模型ptb_word_lm.py例子中的部分代码，对tf.contrib.legacy_seq2seq.sequence_loss_by_example函数的计算过程进行了简单的介绍，这其中的理论知识可以查看前文中链接到的哪些博文，这里只是纯介绍计算过程，因为这也是我最近遇到的问题。我也不知道有没有错，如果文中有写错或者理解错误的地方，请大家及时联系我纠正，谢谢！</p>
<h3 id="参考文献（时间仓促，就没单独整理了）"><a href="#参考文献（时间仓促，就没单独整理了）" class="headerlink" title="参考文献（时间仓促，就没单独整理了）"></a>参考文献（时间仓促，就没单独整理了）</h3><p>【1】前文提到的所有链接<br>【2】tensorflow官方文档以及官方代码</p>

        </div>
        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
</article>

    <section id="comments">
    
        
    <div id="SOHUCS" sid="http://yoursite.com/2018/10/15/tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）/"></div>


    
    </section>


                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>关注我 :</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/xiongzongyang" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2018/10/22/【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for RC/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">下一篇</strong>
        <p class="article-nav-title">
        
            【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2018/10/13/【论文笔记03】ReasoNet_ Learning to Stop Reading in Machine Comprehension/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">上一篇</strong>
        <p class="article-nav-title">【论文笔记03】ReasoNet-Learning to Stop Reading in Machine Comprehension</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">最新文章</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/10/22/【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for RC/" class="thumbnail">
    
    
        <span style="background-image:url(https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/TriviaQA/temp.jpg)" alt="【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NOTES/">NOTES</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/NOTES/Papers/">Papers</a></p>
                            <p class="item-title"><a href="/2018/10/22/【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for RC/" class="title">【论文笔记04】TriviaQA_A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></p>
                            <p class="item-date"><time datetime="2018-10-22T07:45:05.000Z" itemprop="datePublished">2018-10-22</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/10/15/tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）/" class="thumbnail">
    
    
        <span style="background-image:url(https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/ptb/1.png)" alt="tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NOTES/">NOTES</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/NOTES/Tensorflow/">Tensorflow</a></p>
                            <p class="item-title"><a href="/2018/10/15/tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）/" class="title">tensorflow中sequence_loss_by_example()函数的计算过程（结合TF的ptb构建语言模型例子）</a></p>
                            <p class="item-date"><time datetime="2018-10-15T12:25:43.000Z" itemprop="datePublished">2018-10-15</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/10/13/【论文笔记03】ReasoNet_ Learning to Stop Reading in Machine Comprehension/" class="thumbnail">
    
    
        <span style="background-image:url(https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/reasonet/swdt.png)" alt="【论文笔记03】ReasoNet-Learning to Stop Reading in Machine Comprehension" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NOTES/">NOTES</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/NOTES/Papers/">Papers</a></p>
                            <p class="item-title"><a href="/2018/10/13/【论文笔记03】ReasoNet_ Learning to Stop Reading in Machine Comprehension/" class="title">【论文笔记03】ReasoNet-Learning to Stop Reading in Machine Comprehension</a></p>
                            <p class="item-date"><time datetime="2018-10-13T12:29:43.000Z" itemprop="datePublished">2018-10-13</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/10/13/【论文笔记02】Text Understanding with the Attention Sum Reader Network/" class="thumbnail">
    
    
        <span style="background-image:url(https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/asreader/swdt.png)" alt="【论文笔记02】Text Understanding with the Attention Sum Reader Network" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NOTES/">NOTES</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/NOTES/Papers/">Papers</a></p>
                            <p class="item-title"><a href="/2018/10/13/【论文笔记02】Text Understanding with the Attention Sum Reader Network/" class="title">【论文笔记02】Text Understanding with the Attention Sum Reader Network</a></p>
                            <p class="item-date"><time datetime="2018-10-13T01:42:10.000Z" itemprop="datePublished">2018-10-13</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/10/13/【论文笔记01】Phrase-Based & Neural Unsupervised Machine Translation/" class="thumbnail">
    
    
        <span style="background-image:url(https://raw.githubusercontent.com/xiongzongyang/hexo_photo/master/pnumt/p1.png)" alt="【论文笔记01】Phrase-Based &amp; Neural Unsupervised Machine Translation" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NOTES/">NOTES</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/NOTES/Papers/">Papers</a></p>
                            <p class="item-title"><a href="/2018/10/13/【论文笔记01】Phrase-Based & Neural Unsupervised Machine Translation/" class="title">【论文笔记01】Phrase-Based &amp; Neural Unsupervised Machine Translation</a></p>
                            <p class="item-date"><time datetime="2018-10-13T01:42:10.000Z" itemprop="datePublished">2018-10-13</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">8</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">标签</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kaggle/">Kaggle</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keras/">Keras</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLTK基础教程/">NLTK基础教程</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PHP/">PHP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tensorflow/">Tensorflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/">leetcode</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scrapy/">scrapy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分类指标/">分类指标</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/实验教程/">实验教程</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/文本分类/">文本分类</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/文本相似度/">文本相似度</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器翻译/">机器翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/混沌神经网络/">混沌神经网络</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/笔记/">笔记</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/自然语言处理/">自然语言处理</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/论文笔记/">论文笔记</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/资料/">资料</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/阅读理解/">阅读理解</a><span class="tag-list-count">4</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/Kaggle/" style="font-size: 14px;">Kaggle</a> <a href="/tags/Keras/" style="font-size: 16px;">Keras</a> <a href="/tags/NLTK基础教程/" style="font-size: 14px;">NLTK基础教程</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/leetcode/" style="font-size: 10px;">leetcode</a> <a href="/tags/python/" style="font-size: 16px;">python</a> <a href="/tags/scrapy/" style="font-size: 12px;">scrapy</a> <a href="/tags/分类指标/" style="font-size: 10px;">分类指标</a> <a href="/tags/实验教程/" style="font-size: 18px;">实验教程</a> <a href="/tags/文本分类/" style="font-size: 16px;">文本分类</a> <a href="/tags/文本相似度/" style="font-size: 10px;">文本相似度</a> <a href="/tags/机器学习/" style="font-size: 10px;">机器学习</a> <a href="/tags/机器翻译/" style="font-size: 10px;">机器翻译</a> <a href="/tags/深度学习/" style="font-size: 16px;">深度学习</a> <a href="/tags/混沌神经网络/" style="font-size: 10px;">混沌神经网络</a> <a href="/tags/笔记/" style="font-size: 14px;">笔记</a> <a href="/tags/自然语言处理/" style="font-size: 20px;">自然语言处理</a> <a href="/tags/论文笔记/" style="font-size: 16px;">论文笔记</a> <a href="/tags/资料/" style="font-size: 12px;">资料</a> <a href="/tags/阅读理解/" style="font-size: 16px;">阅读理解</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">链接</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="https://blog.csdn.net/xyz1584172808">CSDN</a>
                    </li>
                
                    <li>
                        <a href="http://study.163.com/course/courseMain.htm?courseId=1005638005">Online Course</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2018 IrvingBei</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
				<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
				<span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			</div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script id="cy_cmt_num" src="https://changyan.sohu.com/upload/plugins/plugins.list.count.js?clientId=cytocN1SG"></script>
    <script type="text/javascript">
        (function() {
            var appid = 'cytocN1SG';
            var conf = '6a5712782fd0d61668873ebb581285ce';
            var width = window.innerWidth || document.documentElement.clientWidth;
            if (width < 960) {
                window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>');
            } else {
                var loadJs = function(d, a) {
                    var c = document.getElementsByTagName("head")[0] || document.head || document.documentElement;
                    var b = document.createElement("script");
                    b.setAttribute("type", "text/javascript");
                    b.setAttribute("charset", "UTF-8");
                    b.setAttribute("src", d);
                    if (typeof a === "function") {
                        if (window.attachEvent) {
                            b.onreadystatechange = function() {
                                var e = b.readyState;
                                if (e === "loaded" || e === "complete") {
                                    b.onreadystatechange = null;
                                    a()
                                }
                            }
                        } else {
                            b.onload = a
                        }
                    }
                    c.appendChild(b)
                };
                loadJs("https://changyan.sohu.com/upload/changyan.js", function() {
                    window.changyan.api.config({
                        appid: appid,
                        conf: conf
                    })
                });
            }
        })();
    </script>





    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>
